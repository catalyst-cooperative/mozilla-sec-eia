{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66d3be20-260b-4e9e-b39c-9ad016c57c53",
   "metadata": {},
   "source": [
    "# Experiment with LayoutLM Model\n",
    "\n",
    "Notebooks and Documentation:\n",
    "* [LayoutLMv3 model doc](https://huggingface.co/docs/transformers/model_doc/layoutlmv3)\n",
    "* [Inference for LayoutLMv2 with labels](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Inference_with_LayoutLMv2ForTokenClassification.ipynb) and [Inference for LayoutLMv2 with no labels](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb), also has examples of displaying images with bounding boxes inline\n",
    "* [Original LayoutLM paper](https://arxiv.org/abs/1912.13318)\n",
    "\n",
    "Blog Posts:\n",
    "* Fine-tuning Transformer Model For Invoice Recognition: [v1](https://towardsdatascience.com/fine-tuning-transformer-model-for-invoice-recognition-1e55869336d4), [v2](https://towardsdatascience.com/fine-tuning-layoutlm-v2-for-invoice-recognition-91bf2546b19e), [v3](https://towardsdatascience.com/fine-tuning-layoutlm-v3-for-invoice-processing-e64f8d2c87cf)\n",
    "* [Extracting entities from structured documents: a practical guide](https://medium.com/@ravisatvik.192/unleashing-the-power-of-layoutlm-extracting-entities-from-structured-documents-made-easy-5d82c6290ec7)\n",
    "* [How to train LayoutLM on a custom dataset](https://medium.com/@matt.noe/tutorial-how-to-train-layoutlm-on-a-custom-dataset-with-hugging-face-cda58c96571c)\n",
    "\n",
    "Datasets:\n",
    "* DocBank: DocBank is a large-scale dataset for document layout analysis, understanding, and retrieval. It contains labeled tables and other elements in documents. \n",
    "* FUNSD: The [FUNSD dataset](https://huggingface.co/datasets/nielsr/funsd-layoutlmv3) contains labeled annotations for various elements in forms and tables in documents. While it primarily focuses on forms, it may contain some relevant data for table extraction.\n",
    "* SciIE: The SciIE dataset focuses on scientific documents and contains annotations for entities, relations, and events. \n",
    "* CORD-19: This dataset contains a large collection of scholarly articles related to COVID-19. While it may not have explicit annotations for tables, it contains a wide range of document types, including tables, figures, and text.\n",
    "\n",
    "Annotation:\n",
    "* [UBIAI](https://ubiai.tools/) - basic free level allows unlimited annotation, accepts HTML\n",
    "* [UBIAI tutorial video](https://www.youtube.com/watch?v=r1aoFj974FU&ab_channel=KarndeepSingh)\n",
    "* [Label Studio](https://labelstud.io/guide/get_started.html#Quick-start), open source, accepts html\n",
    "\n",
    "Notes:\n",
    "* From the LayoutLMv2 [usage guide](https://huggingface.co/docs/transformers/model_doc/layoutlmv2#usage-layoutlmv2processor): LayoutLMv2Processor uses PyTesseract, a Python wrapper around Googleâ€™s Tesseract OCR engine, under the hood. Note that you can still use your own OCR engine of choice, and provide the words and normalized boxes yourself. This requires initializing LayoutLMv2ImageProcessor with apply_ocr set to False.\n",
    "* See Use case 3: token classification (training), apply_ocr=False\n",
    "* Need to make a decision between using PyTorch to load training dataset or using Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c569588-cd8e-4729-b011-9e0fd110f789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096b2a7-88bb-47b0-beda-34897ec853f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# had to run brew install tesseract as well, not sure how to put into environment/package deps\n",
    "!pip install -q pytesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6c10c-f701-4d47-9ee5-306729c297be",
   "metadata": {},
   "source": [
    "# Try LayoutLMv3 Inference with FUNSD Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89a274-f462-48f6-bbd1-fb7e4e2b4841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# this dataset uses the new Image feature :)\n",
    "dataset = load_dataset(\"nielsr/funsd-layoutlmv3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a977d60-fa00-48dd-858a-935d02cf0422",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141010d-5fcd-4c58-867c-ac80e1204949",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352d8e0-d998-414f-89e3-a4b8d89a83c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {v: k for v, k in enumerate(labels)}\n",
    "label2id = {k: v for v, k in enumerate(labels)}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff315da7-776d-486d-9880-73c8aed2deb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = dataset[\"test\"][1]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d936b6d-904b-467d-ab02-453dd699a012",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = example[\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0058da17-76a1-44aa-ba58-fa94e06ddedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "# processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base-uncased\")\n",
    "# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\n",
    "# based on the checkpoint we provide from the hub\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=True)\n",
    "\n",
    "encoding = processor(image, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fca07e-51e4-4de1-86d1-6a1af94ccc06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for k,v in encoding.items():\n",
    "    encoding[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f9c54e-f461-41e2-924f-c6434d2f2960",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "\n",
    "# load the fine-tuned model from the hub\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"nielsr/layoutlmv3-finetuned-funsd\")\n",
    "model.to(device)\n",
    "\n",
    "# forward pass\n",
    "outputs = model(**encoding)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f14ac51-b3ba-4ae6-9f63-6bc46a7284b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnormalize_box(bbox, width, height):\n",
    "     return [\n",
    "         width * (bbox[0] / 1000),\n",
    "         height * (bbox[1] / 1000),\n",
    "         width * (bbox[2] / 1000),\n",
    "         height * (bbox[3] / 1000),\n",
    "     ]\n",
    "\n",
    "predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "token_boxes = encoding.bbox.squeeze().tolist()\n",
    "\n",
    "width, height = image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61321577-3b9a-4d33-81b3-75265dd7274e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0\n",
    "\n",
    "true_predictions = [id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]]\n",
    "true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(token_boxes) if not is_subword[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc19fb78-78ea-4905-8671-ff705ee6911f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(true_predictions)\n",
    "print(true_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b31bb5-64b3-4507-817e-61af0a1ceb65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "def iob_to_label(label):\n",
    "    label = label[2:]\n",
    "    if not label:\n",
    "        return \"other\"\n",
    "    return label\n",
    "\n",
    "label2color = {\"question\":\"blue\", \"answer\":\"green\", \"header\":\"orange\", \"other\":\"violet\"}\n",
    "\n",
    "for prediction, box in zip(true_predictions, true_boxes):\n",
    "    predicted_label = iob_to_label(prediction).lower()\n",
    "    draw.rectangle(box, outline=label2color[predicted_label])\n",
    "    draw.text((box[0]+10, box[1]-10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb62c9d-af33-4ed9-a2e0-cfcc889dc739",
   "metadata": {},
   "source": [
    "# Inference with Ex. 21 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94895b8f-8202-4a90-8329-67041bf3ffc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"trans_lux_corp_table.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad614947-bb3c-4e59-b892-774aba99130e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding = processor(image, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8822337-83a1-446d-9c10-ce8128ab1b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k,v in encoding.items():\n",
    "    encoding[k] = v.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321c9e4-b40a-4c68-9856-0954a506b0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outputs = model(**encoding)\n",
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a8dd4-a57d-4191-9207-bb1c0ac0ce09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = outputs.logits.argmax(-1).squeeze().tolist()\n",
    "token_boxes = encoding.bbox.squeeze().tolist()\n",
    "\n",
    "width, height = image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db532f82-c4c6-49ca-9f67-72d7ae7d4a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "is_subword = np.array(offset_mapping.squeeze().tolist())[:,0] != 0\n",
    "\n",
    "true_predictions = [id2label[pred] for idx, pred in enumerate(predictions) if not is_subword[idx]]\n",
    "true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(token_boxes) if not is_subword[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e53c8f-c3ee-4751-bd01-051a64a06d07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "def iob_to_label(label):\n",
    "    label = label[2:]\n",
    "    if not label:\n",
    "        return \"other\"\n",
    "    return label\n",
    "\n",
    "label2color = {\"question\":\"blue\", \"answer\":\"green\", \"header\":\"orange\", \"other\":\"violet\"}\n",
    "\n",
    "for prediction, box in zip(true_predictions, true_boxes):\n",
    "    predicted_label = iob_to_label(prediction).lower()\n",
    "    draw.rectangle(box, outline=label2color[predicted_label])\n",
    "    draw.text((box[0]+10, box[1]-10), text=predicted_label, fill=label2color[predicted_label], font=font)\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decdb33b-3779-41c1-972c-7545bc42fb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mozilla-sec-eia",
   "language": "python",
   "name": "mozilla-sec-eia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
