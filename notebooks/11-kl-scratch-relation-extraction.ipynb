{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09a24ad-46b7-4c6a-972d-a179d5e94e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e655e2-2934-4d83-aeb0-fb968f430bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, LayoutLMv3ForTokenClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import mode\n",
    " \n",
    "from mozilla_sec_eia.ex_21.inference import (\n",
    "    clean_extracted_df,\n",
    "    create_inference_dataset,\n",
    "    perform_inference\n",
    ")\n",
    "from mozilla_sec_eia.utils.layoutlm import (\n",
    "    iob_to_label,\n",
    "    draw_boxes_on_img,\n",
    "    unnormalize_box\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4acebd-95c6-4cc3-9890-4b2ce3682258",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extract into a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a142a-0a2f-44fa-ad8d-ba829f74566a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "has_labels=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5832f16-e3b1-44c1-8761-56d1eb5b461f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-Subsidiary', 'I-Subsidiary', 'B-Loc', 'I-Loc', 'B-Own_Per', 'I-Own_Per']\n",
    "id2label = {k: v for k,v in enumerate(label_list)}\n",
    "label2id = {v: k for k,v in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1d00ac-79d2-42ba-b0ff-7d550fcab0f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load_model function loads a model with mlflow\n",
    "model_path = Path(\"../models/layoutlm_v1_50_labeled_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd88503-5323-4372-9076-decd4ac65a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LayoutLMv3ForTokenClassification.from_pretrained(model_path,\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055292c8-3f1d-49f3-97ce-39316b3476d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\", apply_ocr=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262d11b-07df-4349-9ef5-dd31b4893a29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_dir = Path(\"../sec10k_filings/pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf3a4a9-52de-4a08-8db0-edf206e8b62f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only necessary if using data with labels\n",
    "labeled_json_dir = Path(\"../sec10k_filings/labeled_jsons_v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983dbd41-9c39-4efe-86f5-f63add59b472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = create_inference_dataset(\n",
    "        pdfs_dir=pdf_dir, labeled_json_dir=labeled_json_dir, has_labels=has_labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938fb2c-c888-4e61-8185-46b77c6f1e3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# only use 3 examples\n",
    "dataset_index = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e5208-7d5c-4dc6-aac7-0cca090cba41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check but I think this is mainly slow because it's checking to make sure PDFs and JSONs are cached\n",
    "logit_list, pred_list, output_dfs = perform_inference(pdf_dir,\n",
    "                                          model,\n",
    "                                          processor,\n",
    "                                          dataset_index,\n",
    "                                          labeled_json_dir,\n",
    "                                          has_labels\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ebcea6-5cd7-4a55-8945-0b3f7a428240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_stride = 128\n",
    "\n",
    "def convert_ner_tags_to_id(ner_tags):\n",
    "    return [int(label2id[ner_tag]) for ner_tag in ner_tags]\n",
    "\n",
    "def visual_inputs_with_labels():\n",
    "    for i in range(len(pred_list)):\n",
    "        predictions = pred_list[i]\n",
    "        example = dataset[i]\n",
    "        image = example[\"image\"]\n",
    "        words = example[\"tokens\"]\n",
    "        boxes = example[\"bboxes\"]\n",
    "        ner_tags = convert_ner_tags_to_id(example[\"ner_tags\"])\n",
    "        encoding = processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            word_labels=ner_tags,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True,\n",
    "            max_length=512,  # this is the maximum max_length\n",
    "            stride=doc_stride,\n",
    "        )\n",
    "        yield predictions, encoding, images\n",
    "\n",
    "def visual_inputs():\n",
    "    for i in range(len(pred_list)):\n",
    "        predictions = pred_list[i]\n",
    "        example = dataset[i]\n",
    "        image = example[\"image\"]\n",
    "        words = example[\"tokens\"]\n",
    "        boxes = example[\"bboxes\"]\n",
    "        encoding = processor(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "            return_overflowing_tokens=True,\n",
    "            max_length=512,\n",
    "            stride=doc_stride\n",
    "        )\n",
    "        yield predictions, encoding, image\n",
    "\n",
    "if has_labels:\n",
    "    gen = visual_inputs_with_labels()\n",
    "else:\n",
    "    gen = visual_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f5e6d-ba00-4b94-9133-1d93de1a5d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: also add visualizaton for wrong predictions\n",
    "predictions, encoding, image = next(gen)\n",
    "width, height = image.size\n",
    "token_boxes = encoding.bbox.flatten(start_dim=0, end_dim=1).tolist()\n",
    "boxes = [unnormalize_box(box, width, height) for box in token_boxes]\n",
    "if has_labels:\n",
    "    labels = encoding.labels.flatten(start_dim=0, end_dim=1).tolist()\n",
    "    predictions = torch.tensor(predictions).view(-1).tolist()\n",
    "    true_predictions = [model.config.id2label[pred] for pred, label in zip(predictions, labels) if label != - 100]\n",
    "    true_labels = [model.config.id2label[label] for prediction, label in zip(predictions, labels) if label != -100]\n",
    "    true_boxes = [unnormalize_box(box, width, height) for box, label in zip(token_boxes, labels) if label != -100]\n",
    "    draw_boxes_on_img(true_predictions, true_boxes, image, width, height)\n",
    "else:\n",
    "    predictions = torch.tensor(predictions).view(-1).tolist()\n",
    "    true_predictions = [model.config.id2label[pred] for pred in predictions]\n",
    "    draw_boxes_on_img(true_predictions, boxes, image, width, height)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e104dd-b961-4ac5-8612-f58ddca25900",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding[\"pixel_values\"] = torch.stack(encoding[\"pixel_values\"])\n",
    "# Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "# its corresponding example. This key gives us just that.\n",
    "sample_mapping = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "# The offset mappings will give us a map from token to character position in the original context. This will\n",
    "# help us compute the start_positions and end_positions.\n",
    "offset_mapping = encoding.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4ec14-ae65-4e91-8e9a-b9658a8d1986",
   "metadata": {},
   "source": [
    "# Get mode predictions:\n",
    "- [x] finish vectorizing below cell to get mode predictions\n",
    "- [ ] add in context to break ties\n",
    "- [x] flatten out boxes and preds\n",
    "- [x] check out whether to filter out subwords\n",
    "\n",
    "See below code snippet to handle out of span docs later and this issue: https://github.com/huggingface/transformers/issues/19190\n",
    "\n",
    "See this notebook for snippet on subwords:\n",
    "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/True_inference_with_LayoutLMv2ForTokenClassification_%2B_Gradio_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd73026-7a98-4cbb-a43f-56562406bd42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_by_order(target_array):\n",
    "    label_priority = [\"I-Subsidiary\", \"I-Loc\", \"I-Own_Per\", \"B-Subsidiary\", \"B-Loc\", \"B-Own_Per\", \"O\"]\n",
    "    id_priority = [label2id[label] for label in label_priority]\n",
    "    # Create a priority map from the order array\n",
    "    priority_map = {val: idx for idx, val in enumerate(id_priority)}\n",
    "\n",
    "    # Sort the target array based on the priority map\n",
    "    sorted_array = sorted(target_array, key=lambda x: priority_map.get(x, float('inf')))\n",
    "    return sorted_array\n",
    "\n",
    "def get_flattened_mode_predictions(token_boxes_tensor, predictions_tensor):\n",
    "    # Flatten the tensors\n",
    "    flat_token_boxes = token_boxes_tensor.view(-1, 4)\n",
    "    flat_predictions = predictions_tensor.view(-1)\n",
    "    \n",
    "    # Filter out invalid boxes\n",
    "    # valid_mask = (flat_token_boxes != 0).all(dim=1) & (flat_token_boxes != torch.tensor([0, 0, 0, 0])).all(dim=1)\n",
    "    # valid_boxes = flat_token_boxes[valid_mask]\n",
    "    # valid_predictions = flat_predictions[valid_mask]\n",
    "    # valid_boxes = valid_boxes.numpy()\n",
    "    # valid_predictions = valid_predictions.numpy()\n",
    "    valid_boxes = flat_token_boxes.numpy()\n",
    "    valid_predictions = flat_predictions.numpy()\n",
    "    \n",
    "    # Find unique boxes and indices \n",
    "    # with inverse_indices, you can go from unique_boxes to the original array\n",
    "    # it gives the indices from unique_boxes that are needed to reconstruct the array\n",
    "    unique_boxes, inverse_indices = np.unique(valid_boxes, axis=0, return_inverse=True)\n",
    "    \n",
    "    # Compute the mode for each unique bounding box\n",
    "    # Use advanced indexing to group predictions by unique bounding box\n",
    "    # for each unique box in valid_boxes, create a list with all predictions for that box\n",
    "    # get the indices in predictions where the corresponding index in boxes is\n",
    "    # want to be able to go from value in unique_boxes to indices in original array\n",
    "    unique_box_predictions = [valid_predictions[np.where(inverse_indices == i)[0]] for i in range(len(unique_boxes))]\n",
    "    pred_counts = [np.bincount(arr) for arr in unique_box_predictions]\n",
    "    # Compute the mode of predictions for each group\n",
    "    # modes = np.array([mode(pred_arr)[0] for pred_arr in unique_box_predictions])\n",
    "    modes = np.array([sort_by_order(np.where(arr == np.max(arr))[0])[0] for arr in pred_counts])\n",
    "    flattened_modes = modes[inverse_indices]\n",
    "    return flattened_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1411e-02bb-4dec-a7ed-78b818648634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_boxes = torch.tensor([[[0, 0, 0, 0], [0, 1, 1, 1], [1, 2, 3, 4], [2, 3, 4, 5]], [[1, 2, 3, 4], [2, 3, 4, 5], [2, 3, 4, 6], [0, 0, 0, 0]]])\n",
    "test_labels = [\"O\", \"B-Subsidiary\", \"I-Subsidiary\", \"B-Loc\", \"B-Subsidiary\", \"B-Loc\", \"I-Loc\", \"O\"]\n",
    "test_preds = torch.tensor([label2id[label] for label in test_labels]).unsqueeze(dim=1)\n",
    "expected = [\"O\", \"B-Subsidiary\", \"I-Subsidiary\", \"B-Loc\", \"I-Subsidiary\", \"B-Loc\", \"I-Loc\", \"O\"]\n",
    "test_boxes.shape, test_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e44d3-15c3-4e74-9e24-38d91b7b8397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93263ed7-d3dd-48de-bb2d-df78d4cde57b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4847c8-c33b-4d9c-b671-3add76d5a2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modes = get_flattened_mode_predictions(test_boxes, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefdf518-d65f-48f8-89a4-e2cacdb495b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ad2e7-de7f-46a9-b16c-5476b3dea77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "([label2id[label] for label in expected] == modes).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc9c8db-5b09-4252-9d04-e08a9b3747ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_predictions.shape, token_boxes_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d23171-abf2-4ed3-8b67-3d014a2778f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_subword = np.array(offset_mapping.flatten(start_dim=0, end_dim=1).tolist())[:,0] != 0\n",
    "\n",
    "true_predictions = [id2label[pred] for idx, pred in enumerate(mode_predictions) if not is_subword[idx]]\n",
    "true_boxes = [unnormalize_box(box, width, height) for idx, box in enumerate(token_boxes_tensor) if not is_subword[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0332c6-e1f3-4b98-895e-6cdbbfb31a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_predictions), len(true_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae228c-eac9-4551-a6c3-9b8e82b4c2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in encoding[\"input_ids\"][:2]:\n",
    "    print(processor.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693b8723-47cd-4853-8470-3c734ddfe2ba",
   "metadata": {},
   "source": [
    "# Return to Looking at Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128efc2-5299-4669-b26a-327737209996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make sure this lines up with the generator index\n",
    "example = dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464995c5-d2e2-4525-bd92-f015d56f2a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "example[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77d670-2f88-4ea7-82ad-c82ba4b70c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predicted_labels = [model.config.id2label[pred] for pred in predictions]\n",
    "token_boxes_tensor = encoding.bbox.flatten(start_dim=0, end_dim=1)\n",
    "predictions_tensor = torch.tensor(predictions)\n",
    "mode_predictions = get_flattened_mode_predictions(token_boxes_tensor, predictions_tensor) \n",
    "token_boxes = encoding.bbox.flatten(start_dim=0, end_dim=1).tolist()\n",
    "predicted_labels = [model.config.id2label[pred] for pred in mode_predictions]\n",
    "# words = [processor.decode(token) for token in encoding[\"input_ids\"].flatten()]\n",
    "simple_preds = [_iob_to_label(pred).lower() for pred in predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b8f3b-cdc3-453f-98ca-cd92ee568f4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(predicted_labels), len(token_boxes), len(simple_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b463c7-af32-4966-8c52-2d7153f4703f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_cols = [\"top_left_x\", \"top_left_y\", \"bottom_right_x\", \"bottom_right_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823e4c2-6c0c-49d6-a663-dd555c54d7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=token_boxes, columns=bbox_cols)\n",
    "# df.loc[:, \"word\"] = words\n",
    "df.loc[:, \"iob_pred\"] = predicted_labels\n",
    "df.loc[:, \"pred\"] = simple_preds\n",
    "\n",
    "invalid_mask = ((df[\"top_left_x\"] == 0) & (df[\"top_left_y\"] == 0) & (df[\"bottom_right_x\"] == 0) & (df[\"bottom_right_y\"] == 0))\n",
    "df = df[~invalid_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f318a-6181-4caa-a3bb-8b3e6d410766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: it seems more correct to use tokens, not words\n",
    "# BUT if the B and I labels are finicky and can't be fixed, merging on full words\n",
    "# and then dropping duplicates serves as a fix\n",
    "\n",
    "# we want to get actual words on the dataframe, not just subwords that correspond to tokens\n",
    "words_df = pd.DataFrame(data=example[\"bboxes\"], columns=bbox_cols)\n",
    "words_df.loc[:, \"word\"] = example[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3accc-7ed5-4053-9bc7-c9622cbd6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3defab17-4829-4b5a-ba53-95c9a2201cbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.merge(words_df, how=\"left\", on=bbox_cols).drop_duplicates(subset=bbox_cols + [\"pred\", \"word\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075313f-0bea-463b-a559-0d09165b9177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities_df = df.sort_values(by=[\"top_left_y\", \"top_left_x\"])\n",
    "entities_df = entities_df[entities_df[\"pred\"] != \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e0f29-b52a-49af-a362-16987ab5d1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac07195-d1b3-4ba7-84a0-176800af2541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "entities_df[\"group\"] = (entities_df['iob_pred'].str.startswith('B-')).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf8df0-5472-46ef-9a90-0994de3b6f71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if using tokens not full words, don't join with white space\n",
    "# grouped_df = entities_df.groupby([\"group\", \"pred\"])[\"word\"].apply(\"\".join).reset_index()[[\"pred\", \"word\"]]\n",
    "grouped_df = entities_df.groupby([\"group\", \"pred\"])[\"word\"].apply(\" \".join).reset_index()[[\"pred\", \"word\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184c1ae0-d3c0-43d2-91ff-57733d352dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e4ba28-b2e7-4706-904a-987d6ff09ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df[\"row\"] = (grouped_df['pred'].str.startswith('subsidiary')).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8041f3-5442-4feb-8806-7ad8e5c73d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e898eb66-0a4c-417b-b364-6eb73aff4c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df = grouped_df.pivot_table(index='row', columns='pred', values='word', aggfunc=lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdef47-05fe-4d2b-87ef-2dc856bc1b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1a0b90-f8ed-4ca6-9b26-a6e41371268f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_extracted_df(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dd23e-437d-4406-9980-4457e514c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97f84730-6fa8-4d37-a52e-70bc9e915c18",
   "metadata": {},
   "source": [
    "Move from left to right and top to bottom.\n",
    "Assign row numbers to each labeled entity.\n",
    "Each row should have at least one sub.\n",
    "Each time you hit a new sub you start a new row.\n",
    "\n",
    "Pseudo 1\n",
    "\n",
    "row_num = 0\n",
    "data = [] and filled with NAs\n",
    "for word, label in words moving from left to right and top to bottom:\n",
    "  if label is subsidiary:\n",
    "    data[row][0] = word\n",
    "    row += 1\n",
    "  elif label is location:\n",
    "    data[row][1] = word\n",
    "  elif label is own_per:\n",
    "    data[row][2] = word\n",
    "\n",
    "create a dataframe from this\n",
    "\n",
    "Pseudo 2\n",
    "# filter out \"other\" predictions if they're not already filtered\n",
    "subs = df[df.pred == \"subsidiary\"]\n",
    "subs = subs.reset_index(name=\"join_id\")\n",
    "subs = subs.reset_index(name=\"row_num\")\n",
    "df = df.merge(subs[[\"join_id\", \"row_num\"]], how=\"left\", left_index=True, right_on=\"join_id\")\n",
    "# now we have a dataframe with row numbers for subsidiaries\n",
    "# forward fill row number\n",
    "# groupby row number and drop duplicates on preds (only want one own per, loc per row)\n",
    "# pivot table to get row to be index and preds to be column headers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ea4d6-0073-4f27-812f-bf8470200a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the pages in the doc, len(token_boxes) is n pages\n",
    "box_token_dict = {}\n",
    "for i in range(0, len(token_boxes)):\n",
    "    # skip first 128 tokens except in the first window\n",
    "    initial_j = 0 if i == 0 else (doc_stride + 1)\n",
    "    for j in range(initial_j, len(token_boxes[i])):\n",
    "        unnormal_box = unnormalize_box(token_boxes[i][j], width, height)\n",
    "        if (np.asarray(token_boxes[i][j]).shape != (4,)):\n",
    "            continue\n",
    "        elif (token_boxes[i][j] == [0, 0, 0, 0] or token_boxes[i][j] == 0):\n",
    "            continue\n",
    "        else:\n",
    "            bbox = tuple(unnormal_box)  # Convert the list to a tuple\n",
    "            token = processor.tokenizer.decode(encoding[\"input_ids\"][i][j])\n",
    "            if bbox not in box_token_dict:\n",
    "                box_token_dict[bbox] = [token]\n",
    "            else:\n",
    "                box_token_dict[bbox].append(token)\n",
    "    box_token_dict = {bbox: [\"\".join(words)] for bbox, words in box_token_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed4b3f7-9fe6-412f-a71e-b553db4953d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make faster, vectorize\n",
    "# maybe turn token_boxes and predictions into tensors so this can be vectorized\n",
    "box_prediction_dict = {}\n",
    "for i in range(0, len(token_boxes)):\n",
    "    for j in range(0, len(token_boxes[i])):\n",
    "        if (np.asarray(token_boxes[i][j]).shape != (4,)):\n",
    "            continue\n",
    "        elif (token_boxes[i][j] == [0, 0, 0, 0] or token_boxes[i][j] == 0):\n",
    "            continue\n",
    "        else:\n",
    "            bbox = tuple(token_boxes[i][j])  # Convert the list to a tuple\n",
    "            prediction = predictions[i][j]\n",
    "            if bbox not in box_prediction_dict:\n",
    "                box_prediction_dict[bbox] = [prediction]\n",
    "            else:\n",
    "                box_prediction_dict[bbox].append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b96f6-b321-4edd-abe2-30e44a862772",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Begin Old Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca92ad40-f3cd-4fa0-9f64-88650e570deb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(words), len(boxes), len(ner_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0d64a-ebab-4906-9d9e-e243f1e23a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# true boxes is the unnormalized version of boxes\n",
    "len(true_predictions), len(true_labels), len(true_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c0f6d-dd08-4200-918b-ee4ff836dbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_predictions[100], true_labels[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d653a-ecc0-41ce-b559-4e1555197f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(labels), len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f732323b-9f7a-4e86-89cd-a587dde525c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_labels = []\n",
    "for label in true_predictions:\n",
    "    simple_labels.append(_iob_to_label(label).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5379ab-3ce5-4c37-962b-d274a116b143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# can also just generate this dataframe from the PDF text extraction functions\n",
    "# and then merge on the label\n",
    "doc_df = pd.DataFrame(data=boxes, columns=[\"top_left_x\", \"top_left_y\", \"bottom_right_x\", \"bottom_right_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e959b8c2-1342-4ec9-b695-c908fb04f576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_df.loc[:, \"word\"] = words\n",
    "doc_df.loc[:, \"label\"] = simple_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fab7a0-32b5-4ad9-8ae0-b474082a4eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfdbca-77cd-4756-ae23-640007730c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# probably need some tolerance here\n",
    "rows_df = doc_df.sort_values(by=\"top_left_x\").groupby([\"top_left_y\", \"label\"])[\"word\"].apply(\" \".join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17058dc0-d766-4335-aa8e-1e5afad5f8f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68da5998-fd80-4dce-92e8-018536e099ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_df = rows_df[rows_df.label != \"other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad30f66-8eb5-4df3-9d1d-76138f721467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_df = rows_df.pivot_table(index='top_left_y', columns='label', values='word', aggfunc=lambda x: ' '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2780bb-d2aa-4e01-b8b9-f28b70bdfeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_df = extracted_df.drop(columns=[\"top_left_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e3a8a5-8394-4151-bd2d-cdf90e0d6a68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308081b-f51c-494c-a2c2-56c743e1d0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_df[\"own_per\"] = extracted_df['own_per'].str.replace('[^\\w.]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a528e5-a610-4078-8d1b-37a6eebd75d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extracted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17403bbb-4741-4c28-ba6b-bb3f46859b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e109baae-52ee-43e2-9ad9-cce92cd36d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mozilla-sec-eia",
   "language": "python",
   "name": "mozilla-sec-eia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
