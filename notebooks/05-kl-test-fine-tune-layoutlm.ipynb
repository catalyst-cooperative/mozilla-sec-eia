{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277dd674-b2a5-49d7-b182-758bb6bb179a",
   "metadata": {},
   "source": [
    "# Create training data\n",
    "\n",
    "Schema:\n",
    "* id: (int) unique ID for each doc\n",
    "* tokens: (list) words or groups of words \n",
    "* bboxes: (list) bounding box for each token (x1, y1, x2, y2)\n",
    "* ner_tags: (int) the entity tag corresponding to each token\n",
    "* image: PIL Image\n",
    "\n",
    "See [FUNSD dataset](https://huggingface.co/datasets/nielsr/funsd-layoutlmv3) as an example\n",
    "\n",
    "Tutorials and notebooks:\n",
    "* [Fine-tuning on custom dataset tutorial](https://medium.com/@matt.noe/tutorial-how-to-train-layoutlm-on-a-custom-dataset-with-hugging-face-cda58c96571c)\n",
    "* [Fine-tuning LayoutLMv3 notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb#scrollTo=cqcq7rzlVDOE)\n",
    "* [Fine-tuning LayoutLMv2 notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv2/FUNSD/Fine_tuning_LayoutLMv2ForTokenClassification_on_FUNSD_using_HuggingFace_Trainer.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1965dd48-1d86-4456-afa6-66ca5ae4a51b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91015277-ac2f-485e-b866-8de301e7ea60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Optional, Union\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, ClassLabel, Features, Sequence, Value, Array2D, Array3D, load_metric\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "from mozilla_sec_eia.utils import GCSArchive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae380a14-2690-4e89-ae55-f053ebd415ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PDF text extraction utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69287355-f6be-4892-977e-3a3e27eb76f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copied from well gas project wellgas/features/extract_text.py\n",
    "def extract_pdf_data_from_page(page: fitz.Page) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Parse PDF page data.\"\"\"\n",
    "    contents = _parse_page_contents(page)\n",
    "    meta = {\n",
    "        \"rotation_degrees\": [page.rotation],\n",
    "        \"origin_x_pdf_coord\": [page.rect[0]],\n",
    "        \"origin_y_pdf_coord\": [page.rect[1]],\n",
    "        \"width_pdf_coord\": [page.rect[2] - page.rect[0]],\n",
    "        \"height_pdf_coord\": [page.rect[3] - page.rect[1]],\n",
    "        \"has_images\": [not contents[\"image\"].empty],\n",
    "        \"has_text\": [not contents[\"pdf_text\"].empty],\n",
    "        \"page_num\": [page.number],\n",
    "    }\n",
    "    if not contents[\"image\"].empty:\n",
    "        img_area = (\n",
    "            contents[\"image\"]\n",
    "            .eval(\n",
    "                \"((bottom_right_x_pdf - top_left_x_pdf)\"\n",
    "                \" * (bottom_right_y_pdf - top_left_y_pdf))\"\n",
    "            )\n",
    "            .sum()\n",
    "        )\n",
    "    else:\n",
    "        img_area = 0\n",
    "    total_area = meta[\"width_pdf_coord\"][0] * meta[\"height_pdf_coord\"][0]\n",
    "\n",
    "    meta[\"image_area_frac\"] = [np.float32(img_area / total_area)]\n",
    "    meta_df = pd.DataFrame(meta).astype(\n",
    "        {\n",
    "            \"rotation_degrees\": np.int16,\n",
    "            \"origin_x_pdf_coord\": np.float32,\n",
    "            \"origin_y_pdf_coord\": np.float32,\n",
    "            \"width_pdf_coord\": np.float32,\n",
    "            \"height_pdf_coord\": np.float32,\n",
    "            \"has_images\": \"boolean\",\n",
    "            \"has_text\": \"boolean\",\n",
    "            \"page_num\": np.int16,\n",
    "            \"image_area_frac\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    meta = dict(page=meta_df)\n",
    "    for df in contents.values():  # add ID fields\n",
    "        if not df.empty:\n",
    "            df[\"page_num\"] = np.int16(page.number)\n",
    "    return contents | meta\n",
    "\n",
    "\n",
    "def _parse_page_contents(page: fitz.Page) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Parse page contents using fitz.TextPage.\"\"\"\n",
    "    flags = fitz.TEXTFLAGS_DICT\n",
    "    # try getting only words\n",
    "    textpage = page.get_textpage(flags=flags)\n",
    "    content = textpage.extractDICT()\n",
    "    words = textpage.extractWORDS()\n",
    "    images = []\n",
    "    text = []\n",
    "    for block in content[\"blocks\"]:\n",
    "        if block[\"type\"] == 0:\n",
    "            # skip over text, we'll parse it by word blocks\n",
    "            continue\n",
    "        elif block[\"type\"] == 1:\n",
    "            images.append(_parse_image_block(block))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown block type: {block['type']}\")\n",
    "    for word_block in words:\n",
    "        parsed = _parse_word_block(word_block)\n",
    "        if not parsed.empty:\n",
    "            text.append(parsed)\n",
    "    if text:\n",
    "        text = pd.concat(text, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        text = pd.DataFrame()\n",
    "    if images:\n",
    "        images = pd.concat(\n",
    "            (pd.DataFrame(image) for image in images), axis=0, ignore_index=True\n",
    "        )\n",
    "    else:\n",
    "        images = pd.DataFrame()\n",
    "        \n",
    "    return dict(pdf_text=text, image=images)\n",
    "\n",
    "\n",
    "def _parse_image_block(img_block: dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Parse an image block from a fitz.TextPage.extractDICT() output.\"\"\"\n",
    "    top_left_x_pdf, top_left_y_pdf, bottom_right_x_pdf, bottom_right_y_pdf = img_block[\n",
    "        \"bbox\"\n",
    "    ]\n",
    "    dpi = min(\n",
    "        img_block[\"xres\"], img_block[\"yres\"]\n",
    "    )  # should be equal; min() just in case\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"img_num\": [img_block[\"number\"]],\n",
    "            \"dpi\": [dpi],\n",
    "            \"top_left_x_pdf\": [top_left_x_pdf],\n",
    "            \"top_left_y_pdf\": [top_left_y_pdf],\n",
    "            \"bottom_right_x_pdf\": [bottom_right_x_pdf],\n",
    "            \"bottom_right_y_pdf\": [bottom_right_y_pdf],\n",
    "        }\n",
    "    ).astype(\n",
    "        {\n",
    "            \"img_num\": np.int16,\n",
    "            \"dpi\": np.int16,\n",
    "            \"top_left_x_pdf\": np.float32,\n",
    "            \"top_left_y_pdf\": np.float32,\n",
    "            \"bottom_right_x_pdf\": np.float32,\n",
    "            \"bottom_right_y_pdf\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def _parse_word_block(word_block: tuple) -> pd.DataFrame:\n",
    "    \"\"\"Parse a word block from a fitz.TextPage.extractWORDS() output.\"\"\"\n",
    "    out = {\n",
    "        \"top_left_x_pdf\": [word_block[0]],\n",
    "        \"top_left_y_pdf\": [word_block[1]],\n",
    "        \"bottom_right_x_pdf\": [word_block[2]],\n",
    "        \"bottom_right_y_pdf\": [word_block[3]],\n",
    "        \"text\": [word_block[4]],\n",
    "        \"block_num\": [word_block[5]],\n",
    "        \"line_num\": [word_block[6]],\n",
    "        \"word_num\": [word_block[7]]\n",
    "    }\n",
    "    out = pd.DataFrame(out).astype(\n",
    "        {\n",
    "            \"block_num\": np.int16,\n",
    "            \"line_num\": np.int16,\n",
    "            \"word_num\": np.int16,\n",
    "            \"text\": \"string\",\n",
    "            \"top_left_x_pdf\": np.float32,\n",
    "            \"top_left_y_pdf\": np.float32,\n",
    "            \"bottom_right_x_pdf\": np.float32,\n",
    "            \"bottom_right_y_pdf\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def _frac_normal_ascii(text: Union[str, bytes]) -> float:\n",
    "    \"\"\"Fraction of characters that are normal ASCII characters.\"\"\"\n",
    "    # normal characters, from space to tilde, plus whitespace\n",
    "    # see https://www.asciitable.com/\n",
    "    sum_ = 0\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "    for char in text:\n",
    "        if (32 <= ord(char) <= 126) or char in \"\\t\\n\":\n",
    "            sum_ += 1\n",
    "    return sum_ / len(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb1911-6368-4393-a6f7-a66a7d8f5c55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def _render_page(\n",
    "    pg: fitz.Page, dpi=150, clip: Optional[fitz.Rect] = None\n",
    ") -> Image.Image:\n",
    "    \"\"\"Render a page of a PDF as a PIL.Image object.\n",
    "\n",
    "    Args:\n",
    "        pg (fitz.Page): a page of a PDF\n",
    "        dpi (int, optional): image resolution in pixels per inch. Defaults to 150.\n",
    "        clip (Optional[fitz.Rect], optional): Optionally render only a subset of the\n",
    "            page. Defined in PDF coordinates. Defaults to None, which renders the\n",
    "            full page.\n",
    "\n",
    "    Returns:\n",
    "        Image.Image: PDF page rendered as a PIL.Image object\n",
    "    \"\"\"\n",
    "    # 300 dpi is what tesseract recommends. PaddleOCR seems to do fine with half that.\n",
    "    render: fitz.Pixmap = pg.get_pixmap(dpi=dpi, clip=clip)  # type: ignore\n",
    "    img = _pil_img_from_pixmap(render)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _pil_img_from_pixmap(pix: fitz.Pixmap) -> Image.Image:\n",
    "    \"\"\"Convert pyMuPDF Pixmap object to PIL.Image object.\n",
    "\n",
    "    For some reason pyMuPDF (aka fitz) lets you save images using PIL, but does not\n",
    "    have any function to convert to PIL objects. Clearly they do this conversion\n",
    "    internally; they should just expose it. Instead, I had to copy it out from their\n",
    "    source code.\n",
    "\n",
    "    Args:\n",
    "        pix (fitz.Pixmap): a rendered Pixmap\n",
    "\n",
    "    Returns:\n",
    "        Image: a PIL.Image object\n",
    "    \"\"\"\n",
    "    # pyMuPDF source code on GitHub is all in SWIG (some kind of C to python code\n",
    "    # generator) and is unreadable to me. So you have to inspect your local .py files.\n",
    "    # Adapted from the Pixmap.pil_save method in python3.9/site-packages/fitz/fitz.py\n",
    "    # I just replaced instances of \"self\" with \"pix\"\n",
    "    cspace = pix.colorspace\n",
    "    if cspace is None:\n",
    "        mode = \"L\"\n",
    "    elif cspace.n == 1:\n",
    "        mode = \"L\" if pix.alpha == 0 else \"LA\"\n",
    "    elif cspace.n == 3:\n",
    "        mode = \"RGB\" if pix.alpha == 0 else \"RGBA\"\n",
    "    else:\n",
    "        mode = \"CMYK\"\n",
    "\n",
    "    img = Image.frombytes(mode, (pix.width, pix.height), pix.samples)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ba492-7913-47bc-81cb-2e1179a8e1d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PDF_POINTS_PER_INCH = 72  # I believe this is standard for all PDFs\n",
    "\n",
    "def pil_to_cv2(image: Image.Image) -> np.ndarray:  # noqa: C901\n",
    "    \"\"\"Convert a PIL Image to an OpenCV image (numpy array).\"\"\"\n",
    "    # copied from https://gist.github.com/panzi/1ceac1cb30bb6b3450aa5227c02eedd3\n",
    "    # This covers the common modes, is not exhaustive.\n",
    "    mode = image.mode\n",
    "    new_image: np.ndarray\n",
    "    if mode == \"1\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image *= 255\n",
    "    elif mode == \"L\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "    elif mode == \"LA\" or mode == \"La\":\n",
    "        new_image = np.array(image.convert(\"RGBA\"), dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    elif mode == \"RGB\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "    elif mode == \"RGBA\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    elif mode == \"LAB\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_LAB2BGR)\n",
    "    elif mode == \"HSV\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_HSV2BGR)\n",
    "    elif mode == \"YCbCr\":\n",
    "        # XXX: not sure if YCbCr == YCrCb\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_YCrCb2BGR)\n",
    "    elif mode == \"P\" or mode == \"CMYK\":\n",
    "        new_image = np.array(image.convert(\"RGB\"), dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "    elif mode == \"PA\" or mode == \"Pa\":\n",
    "        new_image = np.array(image.convert(\"RGBA\"), dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    else:\n",
    "        raise ValueError(f\"unhandled image color mode: {mode}\")\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def cv2_to_pil(img: np.ndarray) -> Image.Image:\n",
    "    \"\"\"Create PIL Image from numpy pixel array.\"\"\"\n",
    "    if len(img.shape) == 2:  # single channel, AKA grayscale\n",
    "        return Image.fromarray(img)\n",
    "    else:  # only handle BGR for now\n",
    "        return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "def display_img_array(img: np.ndarray, figsize=(5, 5), **kwargs):\n",
    "    \"\"\"Plot image array for jupyter sessions.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(img.shape) == 2:  # grayscale\n",
    "        return plt.imshow(img, cmap=\"gray\", vmin=0, vmax=255, **kwargs)\n",
    "    else:\n",
    "        return plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), **kwargs)\n",
    "\n",
    "\n",
    "def overlay_bboxes(\n",
    "    img: np.ndarray, bboxes: np.ndarray, color=(255, 0, 0)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Overlay bounding boxes of shape N x 4 (x0, y0, x1, y1) on an image.\"\"\"\n",
    "    img = img.copy()\n",
    "    for box in np.round(bboxes, 0).astype(np.int32):  # float to int just in case:\n",
    "        x0, y0, x1, y1 = box\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), color=color, thickness=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def pdf_coords_to_pixel_coords(coords: np.ndarray, dpi: int) -> np.ndarray:\n",
    "    \"\"\"Convert PDF coordinates to pixel coordinates.\"\"\"\n",
    "    # For arbitrary PDFs you would need to subtract the origin in PDF coordinates,\n",
    "    # but since you create these PDFs, you know the origin is (0, 0).\n",
    "    out = coords * dpi / PDF_POINTS_PER_INCH\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b76769-dd06-439e-b6c5-f8961b5ec5b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Create PDFs of Ex. 21's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b5b70-83b2-460f-92c5-f004acc0a3f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archive = GCSArchive()\n",
    "md = archive.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3091ad-cd5f-49e1-a229-907781ebf1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tucson_md = md[md[\"Company Name\"].str.contains(\"TUCSON\")].iloc[[0]]\n",
    "tucson_filing = archive.get_filings(tucson_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e653f3-4be6-4132-a756-ef728da76606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"tucson_electric.pdf\", \"wb\") as file:\n",
    "    tucson_filing[0].ex_21.save_as_pdf(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45448d-5ddc-4d47-801f-6c71e5431731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get some random PDFs\n",
    "sample = md[md.exhibit_21_version.notnull()].sample(3)\n",
    "filings = archive.get_filings(sample)\n",
    "for i in range(len(filings)):\n",
    "    with open(f\"test_{i}.pdf\", \"wb\") as file:\n",
    "        filings[i].ex_21.save_as_pdf(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2595a71e-09ff-4b40-ae6a-dfc1e6e24d97",
   "metadata": {},
   "source": [
    "# Demo with one doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ea213-9084-4c6e-b59e-76dda20b9ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = Path(\"./wisconsin_electric.pdf\")\n",
    "assert src_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2db08d-fe70-47a6-b539-1cb20c60c7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from file\n",
    "doc = fitz.Document(str(src_path))\n",
    "doc.is_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4bee7-ee3a-4752-83b0-ca098135e66f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bytes\n",
    "_bytes = src_path.read_bytes()\n",
    "from io import BytesIO\n",
    "doc = fitz.open(stream=BytesIO(_bytes), filetype=\"pdf\")\n",
    "doc.is_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08536a6-04db-401f-a725-d54ef3723576",
   "metadata": {},
   "source": [
    "### Extract Text Bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d30555-63aa-4e8d-9aab-bf8d887ac5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg = doc[0]\n",
    "extracted = extract_pdf_data_from_page(pg)\n",
    "extracted.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b2066-62d5-4240-b927-c0bc720a6852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = extracted['pdf_text']\n",
    "img_info = extracted['image']\n",
    "pg_meta = extracted['page']\n",
    "txt.shape, img_info.shape, pg_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5044efd-fd14-42ab-8f2b-23f2a9d50b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46310943-177b-4bf5-86ad-29fb156ff5ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Label the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066e666-af80-4df1-9682-9d96a10413cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do we actually need the headers?\n",
    "id_to_label_full = {\n",
    "    0: \"O\",\n",
    "    1: \"B-Header_Subsidiary\",\n",
    "    2: \"I-Header_Subsidiary\",\n",
    "    3: \"B-Body_Subsidiary\",\n",
    "    4: \"I-Body_Subsidiary\",\n",
    "    5: \"B-Header_Loc\",\n",
    "    6: \"I-Header_Loc\",\n",
    "    7: \"B-Body_Loc\",\n",
    "    8: \"I-Body_Loc\",\n",
    "    9: \"B-Header_Own_Per\",\n",
    "    10: \"I-Header_Own_Per\",\n",
    "    11: \"B-Body_Own_Per\",\n",
    "    12: \"I-Body_Own_Per\"\n",
    "}\n",
    "id_to_label_small = {\n",
    "    0: \"O\",\n",
    "    1: \"B-Subsidiary\",\n",
    "    2: \"I-Subsidiary\",\n",
    "    3: \"B-Loc\",\n",
    "    4: \"I-Loc\",\n",
    "    5: \"B-Own_Per\",\n",
    "    6: \"I-Own_Per\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f16fa8-857e-40de-9d1f-e04cbaf42d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_col = \"ner_tag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b98246f-47b6-47c3-b64f-9dc4c494ddfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt.loc[:, label_col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6ae61-e87d-4e63-8037-faccd3de70d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_img = _render_page(pg, dpi=50)  # small dpi for notebook display\n",
    "pg_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9a9df-9aaa-4ad2-b466-4393dc32afbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pg_img = _render_page(pg)  # full image for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d338fa-2bd1-4474-ab18-f26e645c7671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_filename = \"wisconsin_electric.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7d5e2-83b3-4e8b-aa6c-5c218006ba70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pg_img.save(image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041f9cb-81af-4b05-bd48-0dee4873b80c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_to_label_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01950c4-f101-4994-80b8-c0c64490c813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt.iloc[70:90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9979cd-9461-4f39-9196-31a104ad5ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_to_indices = {\n",
    "    1: [62, 68, 75],\n",
    "    2: [63, 64, 69, 70, 71, 76],\n",
    "    3: [66, 73, 77],\n",
    "    5: [67, 74, 78]\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc013dca-73ca-4456-b1b3-a8d1cc309cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label, indices in label_to_indices.items():\n",
    "    txt.loc[indices, label_col] = label\n",
    "txt[label_col] = txt[label_col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b7b2a-15fa-4630-8f09-7e095fec33dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_to_color = {\n",
    "    0: (128, 128, 128),\n",
    "    1: (0, 128, 0),\n",
    "    2: (0, 255, 0),\n",
    "    3: (255, 0, 255),\n",
    "    4: (128, 0, 128),\n",
    "    5: (128, 128, 0),\n",
    "    6: (255, 0, 0)\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa0275-058a-4034-aeee-82da0d6d7b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix to overlay boxes on pdf and color code it by label\n",
    "coord_cols = ['top_left_x_pdf', 'top_left_y_pdf', 'bottom_right_x_pdf', 'bottom_right_y_pdf']\n",
    "dpi = 70\n",
    "display_img = np.array(_render_page(pg, dpi=dpi))\n",
    "for tag in id_to_label_small.keys():\n",
    "    subset = txt[txt[label_col] == tag]\n",
    "    bboxes = pdf_coords_to_pixel_coords(subset[coord_cols].values, dpi=dpi)\n",
    "    display_img = overlay_bboxes(display_img, bboxes, color=id_to_color[tag])\n",
    "display_img_array(display_img, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fd6add-af11-43d8-bbb8-92d97b319a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt.loc[:, \"id\"] = 0\n",
    "output_df = pd.DataFrame()\n",
    "output_df = pd.concat([output_df, txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dda34e4-54f5-4bb2-9e17-3df7e739f638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7a5001-7ec3-4b67-ac17-29bb39b80405",
   "metadata": {},
   "source": [
    "# Put into demo doc into JSON Format for Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702675e5-5713-4c30-90ec-2d1d6bc5d934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pil_to_cv2(full_pg_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539ed8c4-5aa4-4113-b26d-8d968de4e5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt.iloc[62]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0966fea-8bd4-4720-b4f3-f231d866e05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_norm = 100/pg_meta.width_pdf_coord.iloc[0]\n",
    "y_norm = 100/pg_meta.height_pdf_coord.iloc[0]\n",
    "x_norm, y_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95336e74-783b-4017-94e8-0180ee56d992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotation_json = {\n",
    "   \"data\": {\n",
    "      \"ocr\": \"gs://labeled-ex21-filings/wisconsin_electric.png\" # how to fill with an f string?\n",
    "   },\n",
    "   \"annotations\": [],\n",
    "   \"predictions\": [\n",
    "      {\n",
    "         \"model_version\": \"v1.0\",\n",
    "         \"result\": [\n",
    "            {\n",
    "              \"original_width\": 1241,\n",
    "              \"original_height\": 1754,\n",
    "              \"image_rotation\": 0,\n",
    "              \"value\": {\n",
    "                \"x\": 31.34646 * x_norm,\n",
    "                \"y\": 349.576477 * y_norm,\n",
    "                \"width\": 20.000002 * x_norm,\n",
    "                \"height\": 13.73999 * y_norm,\n",
    "                \"rotation\": 0\n",
    "              },\n",
    "                \"id\": \"bb1\",\n",
    "                \"from_name\": \"bbox\",\n",
    "                \"to_name\": \"image\",\n",
    "                \"type\": \"rectangle\",\n",
    "                \"origin\": \"manual\"\n",
    "            },\n",
    "            {\n",
    "               \"original_width\": 1241,\n",
    "               \"original_height\": 1754,\n",
    "               \"image_rotation\": 0,\n",
    "               \"value\": {\n",
    "                  \"x\": 31.34646 * x_norm,\n",
    "                  \"y\": 349.576477 * y_norm,\n",
    "                  \"width\": 20.000002 * x_norm,\n",
    "                  \"height\": 13.73999 * y_norm,\n",
    "                  \"rotation\": 0,\n",
    "                  \"text\": [\n",
    "                     \"ATC\"\n",
    "                  ]\n",
    "               },\n",
    "                \"id\": \"bb1\",\n",
    "                \"from_name\": \"transcription\",\n",
    "                \"to_name\": \"image\",\n",
    "                \"type\": \"textarea\",\n",
    "                \"origin\": \"manual\"\n",
    "            },\n",
    "         ],\n",
    "      }\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc79d1d-0b53-4cad-abac-780ffad6021c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('wisconsin_electric.json', 'w') as fp:\n",
    "    json.dump(annotation_json, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d6a78-2f2a-4c4b-b0c2-be5113527dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Label Another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b95efd5-c242-4f0c-8693-6e0c612cb8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = Path(\"./test_0.pdf\")\n",
    "assert src_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5de9a-9af6-44d1-a860-7e399513064e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from file\n",
    "doc = fitz.Document(str(src_path))\n",
    "doc.is_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22614c0e-7864-4a83-a686-b041ee6edc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bytes\n",
    "_bytes = src_path.read_bytes()\n",
    "from io import BytesIO\n",
    "doc = fitz.open(stream=BytesIO(_bytes), filetype=\"pdf\")\n",
    "doc.is_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e543c-0b0d-4639-bc53-a69f540afe6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg2 = doc[0]\n",
    "extracted = extract_pdf_data_from_page(pg2)\n",
    "extracted.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38c025-35c1-4668-9e9d-49e50e14a03e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt2 = extracted['pdf_text']\n",
    "img_info = extracted['image']\n",
    "pg_meta = extracted['page']\n",
    "txt2.shape, img_info.shape, pg_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da3639c-645a-497b-b440-cd9ae074f3c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt2.loc[:, label_col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546cd55-e46e-41c6-9cb6-9f1de165b6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg_img2 = _render_page(pg2, dpi=50)  # small dpi for notebook display\n",
    "pg_img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656f3af1-c71d-43bc-9d8a-58a0854b1bda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pg_img2 = _render_page(pg2)  # full image for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca81796-8196-48ec-a726-d0d7ae1c29d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_to_label_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1ae966-90d4-4b4d-87f4-d5e9960e1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8001f2e4-a1d6-4a98-bacc-c72473a6b77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_to_indices = {\n",
    "    1: [12, 17, 23, 29, 33],\n",
    "    2: [13, 14, 18, 19, 20, 24, 25, 26, 30, 31, 34],\n",
    "    3: [15, 22, 28, 32, 35],\n",
    "    4: [16],\n",
    "    5: [21, 27]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1db51-667c-4d6a-931e-cd6d177fcbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for label, indices in label_to_indices.items():\n",
    "    txt2.loc[indices, label_col] = label\n",
    "txt2[label_col] = txt2[label_col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b85703-14ec-4a4b-9460-55ad8ff32870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix to overlay boxes on pdf and color code it by label\n",
    "coord_cols = ['top_left_x_pdf', 'top_left_y_pdf', 'bottom_right_x_pdf', 'bottom_right_y_pdf']\n",
    "dpi = 70\n",
    "display_img = np.array(_render_page(pg2, dpi=dpi))\n",
    "for tag in id_to_label_small.keys():\n",
    "    subset = txt2[txt2[label_col] == tag]\n",
    "    bboxes = pdf_coords_to_pixel_coords(subset[coord_cols].values, dpi=dpi)\n",
    "    display_img = overlay_bboxes(display_img, bboxes, color=id_to_color[tag])\n",
    "display_img_array(display_img, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55689c78-b955-461d-8cae-a46225a868c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt2.loc[:, \"id\"] = 1\n",
    "output_df = pd.concat([output_df, txt2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e43ca-d388-4353-834c-7ccc654ccd2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Combine into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff7b11c-dfb2-4699-89e5-a6602d19355d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for now make image a filename\n",
    "output_df = pd.DataFrame(columns=[\"tokens\", \"bboxes\", \"ner_tags\", \"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2522f7b-7e20-4c85-991a-d4d2896b9ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3115358-d056-4990-aa34-92aeaef99903",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt.loc[:, \"id\"] = doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef93bad-7ef7-4322-b167-eeee45c62185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df[\"tokens\"] = txt.groupby(\"id\")[\"text\"].apply(list)\n",
    "output_df[\"ner_tags\"] = txt.groupby(\"id\")[\"ner_tag\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d0920a-905f-4821-9368-4c046d850333",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.loc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1d609-8ddc-4abb-b17c-c36614c7581e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_cols = [\"top_left_x_pdf\", \"top_left_y_pdf\", \"bottom_right_x_pdf\", \"bottom_right_y_pdf\"]\n",
    "matrices = {}\n",
    "for _, group in txt.groupby('id'):\n",
    "    bbox = group[bbox_cols].values\n",
    "    matrix = np.reshape(bbox, (len(group), len(bbox_cols)))\n",
    "    matrices[group.iloc[0]['id']] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5685fc-713a-4483-bc5a-16893269a161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this isn't so nice, what does the model want as input?\n",
    "for i, arr in matrices.items():\n",
    "    output_df.loc[i, \"bboxes\"] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6849a8-1770-47f9-a25c-9d2f86cb2d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# format bboxes to have x1, y1, x2, y2\n",
    "txt[\"bboxes\"] = txt[[\"top_left_x_pdf\", \"top_left_y_pdf\", \"bottom_right_x_pdf\", \"bottom_right_y_pdf\"]].astype(str).agg(', '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9c6af-4719-4663-bb87-4897c12cc1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"bboxes\"] = txt.groupby(\"id\")[\"bboxes\"].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb55c8bd-436a-444f-a9c1-d51d2a1ae20a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac178ea-7047-47b0-998b-14ceb6343086",
   "metadata": {},
   "source": [
    "# Try vendoring DocAI\n",
    "https://medium.com/@matt.noe/tutorial-how-to-train-layoutlm-on-a-custom-dataset-with-hugging-face-cda58c96571c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0431220-8549-469c-9955-855743282942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_cols = [\"top_left_x_pdf\", \"top_left_y_pdf\", \"bottom_right_x_pdf\", \"bottom_right_y_pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4a7fa5-879d-4448-a585-6a1e8dd2418a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert dataframe/dictionary into NER format\n",
    "# document_annotation_to_ner https://github.com/butlerlabs/docai/blob/main/docai/annotations/ner_utils.py\n",
    "# complete dataset is a list of dicts, with one dict for each doc\n",
    "n_docs = 2\n",
    "ner_annotations = []\n",
    "images = [full_pg_img, full_pg_img2]\n",
    "for i in range(n_docs):\n",
    "        annotation = {\n",
    "            \"id\": i, \n",
    "            \"tokens\": output_df.groupby(\"id\")[\"text\"].apply(list).loc[i], \n",
    "            \"ner_tags\": [id_to_label_small[n] for n in output_df.groupby(\"id\")[\"ner_tag\"].apply(list).loc[i]],\n",
    "            \"bboxes\": output_df.loc[output_df[\"id\"] == i, :][bbox_cols].values.tolist(),\n",
    "            \"image\": images[i]\n",
    "        }\n",
    "        ner_annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73d7014-add1-4732-ab0e-4560af56daac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize NER notation for LayoutLM\n",
    "# https://github.com/butlerlabs/docai/blob/main/docai/annotations/layoutlm_utils.py\n",
    "def normalize_bounding_box(bbx):\n",
    "    normalize_bounding_box = list(map(lambda point: int(point * 1000), bbx))\n",
    "    return normalize_bounding_box\n",
    "\n",
    "def normalize_ner_annotation_for_layoutlm(annotation):\n",
    "    \"\"\"\n",
    "    Normalize the bounding boxes by 1000 to match LayoutLM expected bounding box format\n",
    "    \"\"\"\n",
    "    normalized_bbxs = list(map(normalize_bounding_box, annotation[\"bboxes\"]))\n",
    "    return {\n",
    "        \"id\": annotation[\"id\"],\n",
    "        \"tokens\": annotation[\"tokens\"],\n",
    "        # Normalize NER bounding boxes by 1000 as LayoutLM expects\n",
    "        \"bboxes\": normalized_bbxs,\n",
    "        \"ner_tags\": annotation[\"ner_tags\"],\n",
    "        \"image\": annotation[\"image\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f889f3f-1a0c-4d3c-b0ba-ef4ccf0e772b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# not sure if we need to actually normalize, Hugging Face expects bboxes between 0 and 1000\n",
    "# and they appear to already be that way\n",
    "# norm_ner_annotations = normalize_ner_annotation_for_layoutlm(ner_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505ed41c-23ee-4c5f-924f-f78166817b97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(ner_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa01c7-70f6-4ce3-bf5f-f1d4608bf208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab3068-43b4-484d-b4d5-8feb2ac18b15",
   "metadata": {},
   "source": [
    "# Fine-tune Layout LM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1d569-a08a-4dfc-95aa-0e0914bb4bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = list(id_to_label_small.values())\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e8d0d-4703-428a-ba31-85552fcef6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "column_names = dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bcfbb-4595-420c-9eb4-70845d5de9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_to_label_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee688e7-7650-46cb-be5d-b1bbbf20f964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id2label = {k: v for k,v in enumerate(label_list)}\n",
    "label2id = {v: k for k,v in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3955cb-4ca4-40b6-9dde-cc246f79d289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_label = ClassLabel(names=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b73042-17cb-4748-8fa2-9378c00c4e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update this split size when there are more than 2 docs\n",
    "dataset = dataset.train_test_split(test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63eef47-4168-402b-b7eb-5347847df004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base-uncased\")\n",
    "# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\n",
    "# based on the checkpoint we provide from the hub\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "def convert_ner_tags_to_id(ner_tags):\n",
    "    return [label2id[ner_tag] for ner_tag in ner_tags]\n",
    "\n",
    "# This function is used to put the Dataset in its final format for training LayoutLM\n",
    "def prepare_dataset(annotations):\n",
    "    images = annotations['image']\n",
    "    words = annotations['tokens']\n",
    "    boxes = annotations['bboxes']\n",
    "    # Map over labels and convert to numeric id for each ner_tag\n",
    "    ner_tags = [convert_ner_tags_to_id(ner_tags) for ner_tags in annotations['ner_tags']]\n",
    "\n",
    "    encoding = processor(images, words, boxes=boxes, word_labels=ner_tags, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937250b-62a5-472a-a68d-0799b15fdd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features for use training the model \n",
    "features = Features({\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': Sequence(feature=Value(dtype='int64')),\n",
    "})\n",
    "\n",
    "# Prepare our train & eval dataset\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "eval_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047c1635-8848-4256-8568-12425217666b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "processor.tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f161ea-46a3-4318-a843-bcd926ccf358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903ee393-6a51-4e65-81ad-628e59718cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821c0d2-3e42-4934-9fb2-38d520633bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(eval_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de27acd-7d93-4564-ae8f-132dee1fce5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b4845-60fa-435a-9646-15337a242f44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_entity_level_metrics = False\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae8eae-5684-438a-8424-0bc2a1439b29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\",\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e1fe2-3cf8-4c13-8d51-e9e760704f65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test\",\n",
    "                                  max_steps=1000,\n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  learning_rate=1e-5,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  eval_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce43aa4d-14c1-42df-b040-f8d0d626859f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f3d8e-e963-462c-8d5a-a9d10145ce8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c655590-0ce7-417f-8cf0-b8923c28833f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mozilla-sec-eia",
   "language": "python",
   "name": "mozilla-sec-eia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
