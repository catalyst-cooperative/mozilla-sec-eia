{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e1d3c2-0256-45ea-a787-14bf5c3e8376",
   "metadata": {},
   "source": [
    "# Create a JSON for a doc to import into Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ff09d-a4d8-459e-a582-ce015f046ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import cv2\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from mozilla_sec_eia.utils import GCSArchive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc000f-8b6e-4133-b192-4ec910e8fe3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archive = GCSArchive()\n",
    "md = archive.get_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44299403-e650-469f-9f62-95e3588025b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "md[\"company_name\"] = md[\"Company Name\"].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988a679-2e73-4ec8-a9bb-34828a69f634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../labeled_data_tracking.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10373d5f-925a-49e3-af27-395f8429175f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = train_df.merge(md, on=[\"Filename\", \"CIK\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353804e-8d9e-469d-8d5c-d2b54924bc2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_dir = Path(\"../sec10k_filings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425d024-4215-4a07-82f4-7025fb8c3fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_filings = archive.get_filings(train_df, cache_directory=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777be419-70a5-4c61-812f-9b66c6232dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdfs_dir = cache_dir / \"pdfs\"\n",
    "pdfs_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eabdd3-61b5-4269-95ad-c56b6adad124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create PDFs\n",
    "for sec_filing in train_filings:\n",
    "    filename = pdfs_dir / Path(sec_filing.filename.split(\".\")[0] + \".pdf\")\n",
    "    print(filename)\n",
    "    with open(filename, \"wb\") as file:\n",
    "        sec_filing.ex_21.save_as_pdf(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf41468-9d82-448c-a29e-1e710093fd5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PDF text extraction utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4a347-2c66-42e4-8bf6-6f7002e9c810",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copied from well gas project wellgas/features/extract_text.py\n",
    "def extract_pdf_data_from_page(page: fitz.Page) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Parse PDF page data.\"\"\"\n",
    "    contents = _parse_page_contents(page)\n",
    "    meta = {\n",
    "        \"rotation_degrees\": [page.rotation],\n",
    "        \"origin_x_pdf_coord\": [page.rect[0]],\n",
    "        \"origin_y_pdf_coord\": [page.rect[1]],\n",
    "        \"width_pdf_coord\": [page.rect[2] - page.rect[0]],\n",
    "        \"height_pdf_coord\": [page.rect[3] - page.rect[1]],\n",
    "        \"has_images\": [not contents[\"image\"].empty],\n",
    "        \"has_text\": [not contents[\"pdf_text\"].empty],\n",
    "        \"page_num\": [page.number],\n",
    "    }\n",
    "    if not contents[\"image\"].empty:\n",
    "        img_area = (\n",
    "            contents[\"image\"]\n",
    "            .eval(\n",
    "                \"((bottom_right_x_pdf - top_left_x_pdf)\"\n",
    "                \" * (bottom_right_y_pdf - top_left_y_pdf))\"\n",
    "            )\n",
    "            .sum()\n",
    "        )\n",
    "    else:\n",
    "        img_area = 0\n",
    "    total_area = meta[\"width_pdf_coord\"][0] * meta[\"height_pdf_coord\"][0]\n",
    "\n",
    "    meta[\"image_area_frac\"] = [np.float32(img_area / total_area)]\n",
    "    meta_df = pd.DataFrame(meta).astype(\n",
    "        {\n",
    "            \"rotation_degrees\": np.int16,\n",
    "            \"origin_x_pdf_coord\": np.float32,\n",
    "            \"origin_y_pdf_coord\": np.float32,\n",
    "            \"width_pdf_coord\": np.float32,\n",
    "            \"height_pdf_coord\": np.float32,\n",
    "            \"has_images\": \"boolean\",\n",
    "            \"has_text\": \"boolean\",\n",
    "            \"page_num\": np.int16,\n",
    "            \"image_area_frac\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    meta = dict(page=meta_df)\n",
    "    for df in contents.values():  # add ID fields\n",
    "        if not df.empty:\n",
    "            df[\"page_num\"] = np.int16(page.number)\n",
    "    return contents | meta\n",
    "\n",
    "\n",
    "def _parse_page_contents(page: fitz.Page) -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"Parse page contents using fitz.TextPage.\"\"\"\n",
    "    flags = fitz.TEXTFLAGS_DICT\n",
    "    # try getting only words\n",
    "    textpage = page.get_textpage(flags=flags)\n",
    "    content = textpage.extractDICT()\n",
    "    words = textpage.extractWORDS()\n",
    "    images = []\n",
    "    text = []\n",
    "    for block in content[\"blocks\"]:\n",
    "        if block[\"type\"] == 0:\n",
    "            # skip over text, we'll parse it by word blocks\n",
    "            continue\n",
    "        elif block[\"type\"] == 1:\n",
    "            images.append(_parse_image_block(block))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown block type: {block['type']}\")\n",
    "    for word_block in words:\n",
    "        parsed = _parse_word_block(word_block)\n",
    "        if not parsed.empty:\n",
    "            text.append(parsed)\n",
    "    if text:\n",
    "        text = pd.concat(text, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        text = pd.DataFrame()\n",
    "    if images:\n",
    "        images = pd.concat(\n",
    "            (pd.DataFrame(image) for image in images), axis=0, ignore_index=True\n",
    "        )\n",
    "    else:\n",
    "        images = pd.DataFrame()\n",
    "        \n",
    "    return dict(pdf_text=text, image=images)\n",
    "\n",
    "\n",
    "def _parse_image_block(img_block: dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Parse an image block from a fitz.TextPage.extractDICT() output.\"\"\"\n",
    "    top_left_x_pdf, top_left_y_pdf, bottom_right_x_pdf, bottom_right_y_pdf = img_block[\n",
    "        \"bbox\"\n",
    "    ]\n",
    "    dpi = min(\n",
    "        img_block[\"xres\"], img_block[\"yres\"]\n",
    "    )  # should be equal; min() just in case\n",
    "    out = pd.DataFrame(\n",
    "        {\n",
    "            \"img_num\": [img_block[\"number\"]],\n",
    "            \"dpi\": [dpi],\n",
    "            \"top_left_x_pdf\": [top_left_x_pdf],\n",
    "            \"top_left_y_pdf\": [top_left_y_pdf],\n",
    "            \"bottom_right_x_pdf\": [bottom_right_x_pdf],\n",
    "            \"bottom_right_y_pdf\": [bottom_right_y_pdf],\n",
    "        }\n",
    "    ).astype(\n",
    "        {\n",
    "            \"img_num\": np.int16,\n",
    "            \"dpi\": np.int16,\n",
    "            \"top_left_x_pdf\": np.float32,\n",
    "            \"top_left_y_pdf\": np.float32,\n",
    "            \"bottom_right_x_pdf\": np.float32,\n",
    "            \"bottom_right_y_pdf\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def _parse_word_block(word_block: tuple) -> pd.DataFrame:\n",
    "    \"\"\"Parse a word block from a fitz.TextPage.extractWORDS() output.\"\"\"\n",
    "    out = {\n",
    "        \"top_left_x_pdf\": [word_block[0]],\n",
    "        \"top_left_y_pdf\": [word_block[1]],\n",
    "        \"bottom_right_x_pdf\": [word_block[2]],\n",
    "        \"bottom_right_y_pdf\": [word_block[3]],\n",
    "        \"text\": [word_block[4]],\n",
    "        \"block_num\": [word_block[5]],\n",
    "        \"line_num\": [word_block[6]],\n",
    "        \"word_num\": [word_block[7]]\n",
    "    }\n",
    "    out = pd.DataFrame(out).astype(\n",
    "        {\n",
    "            \"block_num\": np.int16,\n",
    "            \"line_num\": np.int16,\n",
    "            \"word_num\": np.int16,\n",
    "            \"text\": \"string\",\n",
    "            \"top_left_x_pdf\": np.float32,\n",
    "            \"top_left_y_pdf\": np.float32,\n",
    "            \"bottom_right_x_pdf\": np.float32,\n",
    "            \"bottom_right_y_pdf\": np.float32,\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "def _frac_normal_ascii(text: Union[str, bytes]) -> float:\n",
    "    \"\"\"Fraction of characters that are normal ASCII characters.\"\"\"\n",
    "    # normal characters, from space to tilde, plus whitespace\n",
    "    # see https://www.asciitable.com/\n",
    "    sum_ = 0\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "    for char in text:\n",
    "        if (32 <= ord(char) <= 126) or char in \"\\t\\n\":\n",
    "            sum_ += 1\n",
    "    return sum_ / len(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f45f1-16c4-4181-ba2c-aeec07eacdaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def _render_page(\n",
    "    pg: fitz.Page, dpi=150, clip: Optional[fitz.Rect] = None\n",
    ") -> Image.Image:\n",
    "    \"\"\"Render a page of a PDF as a PIL.Image object.\n",
    "\n",
    "    Args:\n",
    "        pg (fitz.Page): a page of a PDF\n",
    "        dpi (int, optional): image resolution in pixels per inch. Defaults to 150.\n",
    "        clip (Optional[fitz.Rect], optional): Optionally render only a subset of the\n",
    "            page. Defined in PDF coordinates. Defaults to None, which renders the\n",
    "            full page.\n",
    "\n",
    "    Returns:\n",
    "        Image.Image: PDF page rendered as a PIL.Image object\n",
    "    \"\"\"\n",
    "    # 300 dpi is what tesseract recommends. PaddleOCR seems to do fine with half that.\n",
    "    render: fitz.Pixmap = pg.get_pixmap(dpi=dpi, clip=clip)  # type: ignore\n",
    "    img = _pil_img_from_pixmap(render)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _pil_img_from_pixmap(pix: fitz.Pixmap) -> Image.Image:\n",
    "    \"\"\"Convert pyMuPDF Pixmap object to PIL.Image object.\n",
    "\n",
    "    For some reason pyMuPDF (aka fitz) lets you save images using PIL, but does not\n",
    "    have any function to convert to PIL objects. Clearly they do this conversion\n",
    "    internally; they should just expose it. Instead, I had to copy it out from their\n",
    "    source code.\n",
    "\n",
    "    Args:\n",
    "        pix (fitz.Pixmap): a rendered Pixmap\n",
    "\n",
    "    Returns:\n",
    "        Image: a PIL.Image object\n",
    "    \"\"\"\n",
    "    # pyMuPDF source code on GitHub is all in SWIG (some kind of C to python code\n",
    "    # generator) and is unreadable to me. So you have to inspect your local .py files.\n",
    "    # Adapted from the Pixmap.pil_save method in python3.9/site-packages/fitz/fitz.py\n",
    "    # I just replaced instances of \"self\" with \"pix\"\n",
    "    cspace = pix.colorspace\n",
    "    if cspace is None:\n",
    "        mode = \"L\"\n",
    "    elif cspace.n == 1:\n",
    "        mode = \"L\" if pix.alpha == 0 else \"LA\"\n",
    "    elif cspace.n == 3:\n",
    "        mode = \"RGB\" if pix.alpha == 0 else \"RGBA\"\n",
    "    else:\n",
    "        mode = \"CMYK\"\n",
    "\n",
    "    img = Image.frombytes(mode, (pix.width, pix.height), pix.samples)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce4c4f-ac84-4a85-ade4-6268f840bbd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PDF_POINTS_PER_INCH = 72  # I believe this is standard for all PDFs\n",
    "\n",
    "def pil_to_cv2(image: Image.Image) -> np.ndarray:  # noqa: C901\n",
    "    \"\"\"Convert a PIL Image to an OpenCV image (numpy array).\"\"\"\n",
    "    # copied from https://gist.github.com/panzi/1ceac1cb30bb6b3450aa5227c02eedd3\n",
    "    # This covers the common modes, is not exhaustive.\n",
    "    mode = image.mode\n",
    "    new_image: np.ndarray\n",
    "    if mode == \"1\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image *= 255\n",
    "    elif mode == \"L\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "    elif mode == \"LA\" or mode == \"La\":\n",
    "        new_image = np.array(image.convert(\"RGBA\"), dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    elif mode == \"RGB\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "    elif mode == \"RGBA\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    elif mode == \"LAB\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_LAB2BGR)\n",
    "    elif mode == \"HSV\":\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_HSV2BGR)\n",
    "    elif mode == \"YCbCr\":\n",
    "        # XXX: not sure if YCbCr == YCrCb\n",
    "        new_image = np.array(image, dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_YCrCb2BGR)\n",
    "    elif mode == \"P\" or mode == \"CMYK\":\n",
    "        new_image = np.array(image.convert(\"RGB\"), dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR)\n",
    "    elif mode == \"PA\" or mode == \"Pa\":\n",
    "        new_image = np.array(image.convert(\"RGBA\"), dtype=np.uint8)\n",
    "        new_image = cv2.cvtColor(new_image, cv2.COLOR_RGBA2BGRA)\n",
    "    else:\n",
    "        raise ValueError(f\"unhandled image color mode: {mode}\")\n",
    "\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def cv2_to_pil(img: np.ndarray) -> Image.Image:\n",
    "    \"\"\"Create PIL Image from numpy pixel array.\"\"\"\n",
    "    if len(img.shape) == 2:  # single channel, AKA grayscale\n",
    "        return Image.fromarray(img)\n",
    "    else:  # only handle BGR for now\n",
    "        return Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "def display_img_array(img: np.ndarray, figsize=(5, 5), **kwargs):\n",
    "    \"\"\"Plot image array for jupyter sessions.\"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    if len(img.shape) == 2:  # grayscale\n",
    "        return plt.imshow(img, cmap=\"gray\", vmin=0, vmax=255, **kwargs)\n",
    "    else:\n",
    "        return plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), **kwargs)\n",
    "\n",
    "\n",
    "def overlay_bboxes(\n",
    "    img: np.ndarray, bboxes: np.ndarray, color=(255, 0, 0)\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Overlay bounding boxes of shape N x 4 (x0, y0, x1, y1) on an image.\"\"\"\n",
    "    img = img.copy()\n",
    "    for box in np.round(bboxes, 0).astype(np.int32):  # float to int just in case:\n",
    "        x0, y0, x1, y1 = box\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), color=color, thickness=1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def pdf_coords_to_pixel_coords(coords: np.ndarray, dpi: int) -> np.ndarray:\n",
    "    \"\"\"Convert PDF coordinates to pixel coordinates.\"\"\"\n",
    "    # For arbitrary PDFs you would need to subtract the origin in PDF coordinates,\n",
    "    # but since you create these PDFs, you know the origin is (0, 0).\n",
    "    out = coords * dpi / PDF_POINTS_PER_INCH\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2acb1e5-6a53-4b8a-a78d-2cb86a19c4ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bbox_dicts(bbox: pd.Series, ind) -> List[Dict]:\n",
    "    x = bbox[\"top_left_x_pdf\"] * x_norm\n",
    "    y = bbox[\"top_left_y_pdf\"] * y_norm\n",
    "    width = (bbox[\"bottom_right_x_pdf\"] - bbox[\"top_left_x_pdf\"]) * x_norm\n",
    "    height = (bbox[\"bottom_right_y_pdf\"] - bbox[\"top_left_y_pdf\"]) * y_norm\n",
    "    word = bbox[\"text\"]\n",
    "    bbox_id = f\"bbox_{ind}\"\n",
    "    box_dict = {\n",
    "        \"original_width\": original_width,\n",
    "        \"original_height\": original_height,\n",
    "        \"image_rotation\": 0,\n",
    "        \"value\": {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"rotation\": 0\n",
    "        },\n",
    "        \"id\": bbox_id,\n",
    "        \"from_name\": \"bbox\",\n",
    "        \"to_name\": \"image\",\n",
    "        \"type\": \"rectangle\",\n",
    "        \"origin\": \"manual\"\n",
    "    }\n",
    "    word_dict = {\n",
    "        \"original_width\": original_width,\n",
    "        \"original_height\": original_height,\n",
    "        \"image_rotation\": 0,\n",
    "        \"value\": {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"width\": width,\n",
    "            \"height\": height,\n",
    "            \"rotation\": 0,\n",
    "            \"text\": [word]\n",
    "        },\n",
    "        \"id\": bbox_id,\n",
    "        \"from_name\": \"transcription\",\n",
    "        \"to_name\": \"image\",\n",
    "        \"type\": \"textarea\",\n",
    "        \"origin\": \"manual\"\n",
    "    }\n",
    "    return [box_dict, word_dict]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ba0a3-75f3-4553-843e-e15bf8aa7e9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Read in one doc and create a JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ac471-be16-4aba-8352-b17e1c5e22e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UPDATE THIS\n",
    "pdf_filename = \"wisconsin_electric.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc33fcc-41aa-479e-9593-230e1121ea69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_path = Path(pdf_filename)\n",
    "assert src_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a2238-a388-4537-bea9-6d8b5b3a83ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from file\n",
    "doc = fitz.Document(str(src_path))\n",
    "doc.is_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a758c-dead-417f-b293-d47f164bc91b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bytes\n",
    "_bytes = src_path.read_bytes()\n",
    "from io import BytesIO\n",
    "doc = fitz.open(stream=BytesIO(_bytes), filetype=\"pdf\")\n",
    "doc.is_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff145a-7f9f-4ad6-b608-3fe222fd2f0c",
   "metadata": {},
   "source": [
    "### Extract Text Bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80121c43-3d1b-4d0d-ac31-335a9adb9680",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pg = doc[0]\n",
    "extracted = extract_pdf_data_from_page(pg)\n",
    "extracted.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7b775-df37-4005-b884-c4b918fb5cae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt = extracted['pdf_text']\n",
    "img_info = extracted['image']\n",
    "pg_meta = extracted['page']\n",
    "txt.shape, img_info.shape, pg_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82258cb0-81f6-4386-a30a-226ba1dcdaea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15720340-aa63-4832-8993-e2e128d4c939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pg_img = _render_page(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38a2b9-1e00-4073-90d2-1dea3ef8df31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_filename = \"wisconsin_electric.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dda993-8962-46cb-abc1-a97d7cd73fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_pg_img.save(image_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f437add-fa00-401a-b19f-98d8e8ee5907",
   "metadata": {},
   "source": [
    "## Define page variables and JSON dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ab8090-7933-4375-8a7f-26ba12d25290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_width = pil_to_cv2(full_pg_img).shape[1]\n",
    "original_height = pil_to_cv2(full_pg_img).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebc2992-d806-40df-94ae-ee212e9320fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_norm = 100/pg_meta.width_pdf_coord.iloc[0]\n",
    "y_norm = 100/pg_meta.height_pdf_coord.iloc[0]\n",
    "x_norm, y_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7c12e-4c87-4347-aadc-81f878db6773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotation_json = {\n",
    "    \"data\": {\n",
    "        \"ocr\": f\"gs://labeled-ex21-filings/{image_filename}\"  # how do we get the image name?\n",
    "    },\n",
    "    \"annotations\": [],\n",
    "    \"predictions\": [{\"model_version\": \"v1.0\", \"result\": []}],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f811e16-d444-4235-b5c1-90c5c335e32d",
   "metadata": {},
   "source": [
    "## Create a bounding box result entry for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0317ca-288a-4e65-b322-198b7b3ae6f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "# change to using an apply?\n",
    "for i, row in txt.iterrows():\n",
    "    result += get_bbox_dicts(row, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448e8b0-4ff8-47ce-b96b-6d0012c2595b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotation_json[\"predictions\"][0][\"result\"] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ce01c-e3c6-45c6-bd40-2b478472c916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_filename = \"wisconsin_electric_full.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a561cb1-fb88-4062-b85a-71bc4f022796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(json_filename, 'w') as fp:\n",
    "    json.dump(annotation_json, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f834cd6-360c-4485-891c-6d7238e0b35f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Create JSONs and images for entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a0d33-7020-44c0-a4e9-d935577576a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_dir = cache_dir / \"images\"\n",
    "image_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764f66d-bfa2-4558-927d-7fd22ffe3923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "json_dir = cache_dir / \"jsons\"\n",
    "json_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be2936a-1718-499f-b812-ed0bb88a5517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for pdf_filename in os.listdir(pdfs_dir):\n",
    "    if pdf_filename.split(\".\")[-1] != \"pdf\":\n",
    "        continue\n",
    "    print(f\"Creating JSON for {pdf_filename}\")\n",
    "    src_path = pdfs_dir / pdf_filename\n",
    "    assert src_path.exists()\n",
    "    # from file\n",
    "    doc = fitz.Document(str(src_path))\n",
    "    assert doc.is_pdf\n",
    "    pg = doc[0]\n",
    "    extracted = extract_pdf_data_from_page(pg)\n",
    "    txt = extracted['pdf_text']\n",
    "    img_info = extracted['image']\n",
    "    pg_meta = extracted['page']\n",
    "    # render an image of the page and save\n",
    "    # what happens when there are multiple pages?\n",
    "    # might need to use util function\n",
    "    full_pg_img = _render_page(pg)\n",
    "    image_filename = pdf_filename.split(\".\")[0] + \".png\"\n",
    "    full_pg_img.save(image_dir / image_filename)\n",
    "    # fill in some basic variables\n",
    "    original_width = pil_to_cv2(full_pg_img).shape[1]\n",
    "    original_height = pil_to_cv2(full_pg_img).shape[0]\n",
    "    x_norm = 100/pg_meta.width_pdf_coord.iloc[0]\n",
    "    y_norm = 100/pg_meta.height_pdf_coord.iloc[0]\n",
    "    # base annotation JSON template\n",
    "    filename_no_ext = pdf_filename.split(\".\")[0]\n",
    "    annotation_json = {\n",
    "        \"id\": f\"{filename_no_ext}\",\n",
    "        \"data\": {\n",
    "        \"ocr\": f\"gs://labeled-ex21-filings/unlabeled/{image_filename}\" \n",
    "        },\n",
    "        \"annotations\": [],\n",
    "        \"predictions\": [{\"model_version\": \"v1.0\", \"result\": []}],\n",
    "    }\n",
    "    result = []\n",
    "    # change to using an apply?\n",
    "    for i, row in txt.iterrows():\n",
    "        result += get_bbox_dicts(row, i)\n",
    "\n",
    "    annotation_json[\"predictions\"][0][\"result\"] = result\n",
    "    json_filename = json_dir / Path(filename_no_ext + \".json\")\n",
    "    with open(json_filename, 'w') as fp:\n",
    "        json.dump(annotation_json, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdcc7d2-4374-4f3d-b791-c4cb13e2ec16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Format LS output JSON into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3cc1c1-263e-445f-9bef-5030fe116940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_json_dir = Path(\"../sec10k_filings/labeled_jsons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64391694-f56e-4af0-89b2-56f23b2f8d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_cik_in_training_data(labeled_json_filename):\n",
    "    # for now CIK is stored as an int\n",
    "    cik = int(labeled_json_filename.split(\"/\")[-1].split(\"-\")[0])\n",
    "    return cik in train_df.CIK.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd21e2dd-2e7a-4a2c-90e1-b5b5f7f3a70b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_df = pd.DataFrame()\n",
    "image_dict = {}\n",
    "for json_filename in os.listdir(labeled_json_dir):\n",
    "    json_file_path = labeled_json_dir / json_filename\n",
    "    with open(json_file_path, 'r') as j:\n",
    "        doc_dict = json.loads(j.read())\n",
    "        filename = doc_dict[\"task\"][\"data\"][\"ocr\"].split(\"/\")[-1].split(\".\")[0]\n",
    "        print(filename)\n",
    "        if not is_cik_in_training_data(filename):\n",
    "            continue\n",
    "        pdf_filename = filename + \".pdf\"\n",
    "        src_path = pdfs_dir / pdf_filename\n",
    "        assert src_path.exists()\n",
    "        # from file\n",
    "        doc = fitz.Document(str(src_path))\n",
    "        assert doc.is_pdf\n",
    "        pg = doc[0]\n",
    "        extracted = extract_pdf_data_from_page(pg)\n",
    "        txt = extracted['pdf_text']\n",
    "        img_info = extracted['image']\n",
    "        pg_meta = extracted['page']\n",
    "        full_pg_img = _render_page(pg)\n",
    "        # normalize bboxes between 0 and 1000 for Hugging Face\n",
    "        txt[\"top_left_x_pdf\"] = txt[\"top_left_x_pdf\"]/pg_meta.width_pdf_coord.iloc[0]*1000\n",
    "        txt[\"top_left_y_pdf\"] = txt[\"top_left_y_pdf\"]/pg_meta.height_pdf_coord.iloc[0]*1000\n",
    "        txt[\"bottom_right_x_pdf\"] = txt[\"bottom_right_x_pdf\"]/pg_meta.width_pdf_coord.iloc[0]*1000\n",
    "        txt[\"bottom_right_y_pdf\"] = txt[\"bottom_right_y_pdf\"]/pg_meta.height_pdf_coord.iloc[0]*1000\n",
    "        pg_meta.height_pdf_coord.iloc[0]\n",
    "        doc_df = pd.DataFrame()\n",
    "        for item in doc_dict[\"result\"]:\n",
    "            value = item[\"value\"]\n",
    "            # sometimes Label Studio will fill in an empty list as a label\n",
    "            # when there is really no label\n",
    "            # do this without dict comprehension?\n",
    "            if (\"labels\" in value) and value[\"labels\"] == []:\n",
    "                value = {k: v for k, v in value.items() if k != \"labels\"}\n",
    "            ind = int(item[\"id\"].split(\"_\")[-1])\n",
    "            doc_df = pd.concat([doc_df, pd.DataFrame(value, index=[ind])])\n",
    "        doc_df = doc_df.groupby(level=0).first()\n",
    "        txt.loc[:, \"id\"] = filename\n",
    "        output_df = pd.concat([txt, doc_df[[\"labels\"]]], axis=1)\n",
    "        labeled_df = pd.concat([labeled_df, output_df])\n",
    "        image_dict[filename] = full_pg_img\n",
    "labeled_df[\"labels\"] = labeled_df[\"labels\"].fillna(\"O\")\n",
    "labeled_df = labeled_df.rename(columns={\"labels\": \"ner_tag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81d506-d56b-414c-a8be-730c82ba3340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reorganize columns in labeled_df\n",
    "non_id_columns = [col for col in labeled_df.columns if col != \"id\"]\n",
    "labeled_df = labeled_df.loc[:, [\"id\"] + non_id_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe74c4e-6b4d-453c-b63b-9985bdb2e9ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sanity check the bboxes\n",
    "labeled_df.top_left_x_pdf.max(), labeled_df.top_left_y_pdf.max(), labeled_df.bottom_right_x_pdf.max(), labeled_df.bottom_right_y_pdf.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319214bc-c833-45f7-9e39-67ea683caebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "labeled_df.top_left_x_pdf.min(), labeled_df.top_left_y_pdf.min(), labeled_df.bottom_right_x_pdf.min(), labeled_df.bottom_right_y_pdf.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bd347-aa13-41b0-911a-ab347a4893eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.to_parquet(\"labeled_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0fc58c-5d9b-4206-aa6e-659ffccc908b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_dict(labeled_df):\n",
    "    image_dict = {}\n",
    "    for filename in labeled_df[\"id\"].unique():\n",
    "        continue\n",
    "        # read in image from cached images as PIL and save with key as id\n",
    "    return image_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569b6b6-d2f0-4ce7-860e-0bfb796c9d09",
   "metadata": {},
   "source": [
    "# Fine-tune LayoutLM on the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2b879-0629-440a-8cc0-b9409947fa31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, ClassLabel, Features, Sequence, Value, Array2D, Array3D, load_metric\n",
    "import torch\n",
    "from transformers import AutoProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d4e76-e29a-4502-b333-d66d06554463",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbox_cols = [\"top_left_x_pdf\", \"top_left_y_pdf\", \"bottom_right_x_pdf\", \"bottom_right_y_pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9817ea6-d75f-4ec6-af6d-71dabe150e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert dataframe/dictionary into NER format\n",
    "# document_annotation_to_ner https://github.com/butlerlabs/docai/blob/main/docai/annotations/ner_utils.py\n",
    "# complete dataset is a list of dicts, with one dict for each doc\n",
    "doc_filenames = labeled_df[\"id\"].unique()\n",
    "ner_annotations = []\n",
    "for filename in doc_filenames:\n",
    "    annotation = {\n",
    "        \"id\": filename, \n",
    "        \"tokens\": labeled_df.groupby(\"id\")[\"text\"].apply(list).loc[filename], \n",
    "        \"ner_tags\": labeled_df.groupby(\"id\")[\"ner_tag\"].apply(list).loc[filename],\n",
    "        \"bboxes\": labeled_df.loc[labeled_df[\"id\"] == filename, :][bbox_cols].values.tolist(),\n",
    "        \"image\": image_dict[filename]\n",
    "    }\n",
    "    ner_annotations.append(annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a76c6d-a033-4d7d-a1c8-68bded0b9baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(ner_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16104fd3-503c-499b-be2b-3c3963379bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use the DocAI normalizer or is everything okay as is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438202be-eb2a-494f-aeeb-c7b6b2f45d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(ner_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f864908-ff00-4f94-a13f-13713a2f8bf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8f9f7-3868-4d58-b6d6-4c5b4085f7dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_list = ['O', 'B-Subsidiary', 'I-Subsidiary', 'B-Loc', 'I-Loc', 'B-Own_Per']\n",
    "id2label = {k: v for k,v in enumerate(label_list)}\n",
    "label2id = {v: k for k,v in enumerate(label_list)}\n",
    "column_names = dataset.column_names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086d0af3-7e80-41f4-aa5d-0044dca74b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_label = ClassLabel(names=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe09cd-f9cc-4ed1-9be7-83f807b7835d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f76edb-6cdd-4838-b739-0b6c02956e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# processor = LayoutLMv3Processor.from_pretrained(\"microsoft/layoutlmv3-base-uncased\")\n",
    "# we'll use the Auto API here - it will load LayoutLMv3Processor behind the scenes,\n",
    "# based on the checkpoint we provide from the hub\n",
    "processor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\n",
    "\n",
    "def convert_ner_tags_to_id(ner_tags):\n",
    "    return [int(label2id[ner_tag]) for ner_tag in ner_tags]\n",
    "\n",
    "# This function is used to put the Dataset in its final format for training LayoutLM\n",
    "def prepare_dataset(annotations):\n",
    "    images = annotations['image']\n",
    "    words = annotations['tokens']\n",
    "    boxes = annotations['bboxes']\n",
    "    # Map over labels and convert to numeric id for each ner_tag\n",
    "    ner_tags = [convert_ner_tags_to_id(ner_tags) for ner_tags in annotations['ner_tags']]\n",
    "\n",
    "    encoding = processor(images, words, boxes=boxes, word_labels=ner_tags, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db8ac2d-7c04-4ae1-bfae-823b8951755c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define features for use training the model \n",
    "features = Features({\n",
    "    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "    'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "    'attention_mask': Sequence(Value(dtype='int64')),\n",
    "    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "    'labels': Sequence(feature=Value(dtype='int64')),\n",
    "})\n",
    "\n",
    "# Prepare our train & eval dataset\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")\n",
    "\n",
    "eval_dataset = dataset[\"test\"].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d07547-1cb1-4f74-9f73-a14e6df4934e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "processor.tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74e3fe-85fe-49d4-9672-cfdfb598cd59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6871304-cdd3-4128-a0a6-fe06858b8a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = train_dataset[0]\n",
    "for k,v in example.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a99f2de-f856-477c-a14f-91f76e96698f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor.tokenizer.decode(eval_dataset[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6bad4-c9b6-45a9-807e-1dd3f0e4da73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf2071-cf55-4fe2-b80d-bc78562409ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375d454-57dd-42d3-a955-f7a963770448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "return_entity_level_metrics = False\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    else:\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e572e-5b3a-4473-9351-2e4aacd70d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import LayoutLMv3ForTokenClassification\n",
    "\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\"microsoft/layoutlmv3-base\",\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe8e3c-95b0-47fa-b460-fc3f7d83fe2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test\",\n",
    "                                  max_steps=1000,\n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  learning_rate=1e-5,\n",
    "                                  evaluation_strategy=\"steps\",\n",
    "                                  eval_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d60229-7471-4b47-8d73-af6c5edc0138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb6b0e-fb2c-4c15-9741-3d9580cbb7c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490fd8d3-6191-4bf4-ae6d-fd68aa04f834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"../models/layoutlm_v1_50_labeled_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99965729-fcfa-40a2-a131-20077cef0f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use model = LayoutLMv3ForTokenClassification.from_pretrained({path}) to load\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c8f0c-5766-4f8a-b8df-78d1517090ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e460fee-a78d-4399-ad3c-49e70c72fde8",
   "metadata": {},
   "source": [
    "# Perform Inference on a Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527392a-639b-45be-8e6d-86b7e39a5c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def unnormalize_box(bbox, width, height):\n",
    "     return [\n",
    "         width * (bbox[0] / 1000),\n",
    "         height * (bbox[1] / 1000),\n",
    "         width * (bbox[2] / 1000),\n",
    "         height * (bbox[3] / 1000),\n",
    "     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe51a5-92c2-4b56-b614-f76f44b99423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Some simple utilities for drawing bboxes on images of Driver's Licenses\n",
    "'''\n",
    "from PIL import ImageDraw, ImageFont\n",
    "\n",
    "font = ImageFont.load_default()\n",
    "\n",
    "def iob_to_label(label):\n",
    "    label = label[2:]\n",
    "    if not label:\n",
    "        return 'other'\n",
    "    return label\n",
    "\n",
    "def draw_boxes_on_img(\n",
    "    preds_or_labels, \n",
    "    boxes,\n",
    "    draw,\n",
    "    image, \n",
    "    unnormalize = False\n",
    "):\n",
    "    label_color_lookup = {\n",
    "        \"subsidiary\": \"green\",\n",
    "        \"loc\": \"red\",\n",
    "        \"own_per\": \"orange\",\n",
    "    }\n",
    "    for pred_or_label, box in zip(preds_or_labels, boxes):\n",
    "        label = iob_to_label(pred_or_label).lower()\n",
    "        if label == 'other':\n",
    "            continue\n",
    "        else:\n",
    "            if unnormalize:\n",
    "                box = unnormalize_box(box, width, height)\n",
    "            color = label_color_lookup[label]\n",
    "            draw.rectangle(box, outline=color)\n",
    "            draw.text((box[0] + 10, box[1] - 10), text=label, fill=color, font=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d4e6a-2c21-42be-ac76-758564cb856d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = LayoutLMv3ForTokenClassification.from_pretrained(model_path,\n",
    "                                                         id2label=id2label,\n",
    "                                                         label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8337acf-f8b7-4639-90fa-e7e7ed10b552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example = dataset[\"test\"][0]\n",
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b2b47-176a-4e97-a962-a611355a0bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = example[\"image\"]\n",
    "words = example[\"tokens\"]\n",
    "boxes = example[\"bboxes\"]\n",
    "ner_tags = convert_ner_tags_to_id(example[\"ner_tags\"])\n",
    "\n",
    "encoding = processor(image, words, boxes=boxes, word_labels=ner_tags, return_tensors=\"pt\")\n",
    "for k,v in encoding.items():\n",
    "    print(k,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc05c0-b412-4939-ba62-7a28ed45b078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding[\"input_ids\"] = encoding[\"input_ids\"].to(torch.int64)\n",
    "encoding[\"attention_mask\"] = encoding[\"attention_mask\"].to(torch.int64)\n",
    "encoding[\"labels\"] = encoding[\"labels\"].to(torch.int64)\n",
    "encoding[\"bbox\"] = encoding[\"bbox\"].to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34a0f3-5098-459c-a455-511e536a0a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    encoding.to(\"cuda\")\n",
    "    model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846af874-c0fe-4d6e-8b8d-2f33ecf31ff7",
   "metadata": {},
   "source": [
    "Next, we do a forward pass. We use torch.no_grad() as we don't require gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a59e94-929f-40e2-8bc1-c44df7d885d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9dad5-1e82-48ca-b09e-38341361d19f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "predictions = logits.argmax(-1).squeeze().tolist()\n",
    "labels = encoding.labels.squeeze().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e497129-90a6-4b03-831b-923e405c0091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "token_boxes = encoding.bbox.squeeze().tolist()\n",
    "width, height = image.size\n",
    "\n",
    "true_predictions = [model.config.id2label[pred] for pred, label in zip(predictions, labels) if label != - 100]\n",
    "true_labels = [model.config.id2label[label] for prediction, label in zip(predictions, labels) if label != -100]\n",
    "true_boxes = [unnormalize_box(box, width, height) for box, label in zip(token_boxes, labels) if label != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39f1820-2f54-447b-9a13-b5a60afb58a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Draw predictions\n",
    "'''\n",
    "image = example[\"image\"]\n",
    "image = image.convert(\"RGB\")\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "draw_boxes_on_img(true_predictions, true_boxes, draw, image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710f226-9fe2-4d18-b791-33363db983fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mozilla-sec-eia",
   "language": "python",
   "name": "mozilla-sec-eia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
