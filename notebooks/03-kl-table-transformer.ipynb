{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3287a75-9b18-4a09-bc1f-e1d8257ee1cd",
   "metadata": {},
   "source": [
    "# Experiment with Table Transformer Model\n",
    "\n",
    "Walking through Table Transformer tutorial:\n",
    "- [x] get metadata and filter for a file with an Ex. 21\n",
    "- [x] convert to image\n",
    "- [x] apply DetrFeaturePreprocessor to get encoding\n",
    "- [x] load model and pass encoding into model\n",
    "- [x] visualize the results, has the model found the table?\n",
    "- [x] use \"microsoft/table-transformer-structure-recognition\" to actually extract the table\n",
    "- [x] try to get the extracted tables into a dataframe\n",
    "- [ ] try with the [facebook table extractor model](https://huggingface.co/facebook/rag-token-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e2441-4dfb-413b-9295-ff014ca3ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure transformers and torch are installed\n",
    "!python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d424d5e-8fdf-4ca4-aba5-e094f7ca9a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install html2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351faf7-3c79-4a48-87a0-49e787691819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from html2image import Html2Image\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import DetrImageProcessor, TableTransformerForObjectDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd18a838-40cb-44fa-8029-7c4584c4c484",
   "metadata": {},
   "source": [
    "Convert HTML file to image\n",
    "\n",
    "* this only takes a screenshot of the first page, so it doesn't really work. For now, just take an example screenshot of an Ex. 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75bda6-b41b-4c81-ac05-a9d5db19ca06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path to your HTML file\n",
    "html_file = \"trans_lux_corp.html\"\n",
    "\n",
    "hti = Html2Image()\n",
    "# Convert HTML to image\n",
    "image_paths = hti.screenshot(\n",
    "    html_file=html_file,\n",
    "    save_as=\"trans_lux_corp_test.png\"\n",
    ")\n",
    "image = Image.open(image_paths[0]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15f74be-b734-4b16-a842-71c703640c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"isle_of_capri.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ec45d-f126-4eb0-a1e5-de6962e53816",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = Image.open(filename).convert(\"RGB\")\n",
    "width, height = image.size\n",
    "image.resize((int(width*0.5), int(height*0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efa5126-6b74-4140-9823-e946f7fb94e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_extractor = DetrImageProcessor()\n",
    "encoding = feature_extractor(image, return_tensors=\"pt\")\n",
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6be07c-abdc-4bfd-bd60-6eca93018ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(encoding[\"pixel_values\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a4027e-27f8-44d2-93e8-b7588b47d224",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009104b-5ade-4f87-b980-454b70f74e0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00c7f7-9d88-42ef-9a1f-c1f44c305a8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183b36b3-57ae-4acc-9c4b-112ebe905472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "def plot_results(id2label, pil_img, scores, labels, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for score, label, (xmin, ymin, xmax, ymax),c  in zip(scores.tolist(), labels.tolist(), boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        text = f\"{id2label[label]}: {score:0.2f}\"\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor=\"yellow\", alpha=0.5))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bd9c0-5a1f-4e10-b701-4bfb02b3d285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rescale bounding boxes\n",
    "width, height = image.size\n",
    "results = feature_extractor.post_process_object_detection(outputs, threshold=0.7, target_sizes=[(height, width)])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf5ebfe-f1e4-44c3-81b9-82583118941d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_results(model.config.id2label, image, results[\"scores\"], results[\"labels\"], results[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c20bf4c-533c-420f-8d39-e833b4aad9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update id2label to include \"no object\"\n",
    "id2label = model.config.id2label\n",
    "id2label[len(model.config.id2label)] = \"no object\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fd9d8-745d-4d9e-8650-e2d0c47ba8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if class_label != \"no object\":\n",
    "            objects.append({\"label\": class_label, \"score\": float(score),\n",
    "                            \"bbox\": [float(elem) for elem in bbox]})\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4033eb63-a8ce-47dd-9450-488c90e97d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objects = outputs_to_objects(outputs, image.size, id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb61b35-3218-4547-8dd6-13f5da916b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6050a154-51fd-43c5-b352-3d6afe655ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objects_to_crops(img, tokens, objects, class_thresholds, padding=10):\n",
    "    \"\"\"Process the bounding boxes produced by the table detection model into\n",
    "    cropped table images and cropped tokens.\n",
    "    \"\"\"\n",
    "    table_crops = []\n",
    "    for obj in objects:\n",
    "        if obj[\"score\"] < class_thresholds[obj[\"label\"]]:\n",
    "            continue\n",
    "\n",
    "        cropped_table = {}\n",
    "\n",
    "        bbox = obj[\"bbox\"]\n",
    "        bbox = [bbox[0]-padding, bbox[1]-padding, bbox[2]+padding, bbox[3]+padding]\n",
    "\n",
    "        cropped_img = img.crop(bbox)\n",
    "\n",
    "        table_tokens = [token for token in tokens if iob(token[\"bbox\"], bbox) >= 0.5]\n",
    "        for token in table_tokens:\n",
    "            token[\"bbox\"] = [token[\"bbox\"][0]-bbox[0],\n",
    "                             token[\"bbox\"][1]-bbox[1],\n",
    "                             token[\"bbox\"][2]-bbox[0],\n",
    "                             token[\"bbox\"][3]-bbox[1]]\n",
    "\n",
    "        # If table is predicted to be rotated, rotate cropped image and tokens/words:\n",
    "        if obj[\"label\"] == \"table rotated\":\n",
    "            cropped_img = cropped_img.rotate(270, expand=True)\n",
    "            for token in table_tokens:\n",
    "                bbox = token[\"bbox\"]\n",
    "                bbox = [cropped_img.size[0]-bbox[3]-1,\n",
    "                        bbox[0],\n",
    "                        cropped_img.size[0]-bbox[1]-1,\n",
    "                        bbox[2]]\n",
    "                token[\"bbox\"] = bbox\n",
    "\n",
    "        cropped_table[\"image\"] = cropped_img\n",
    "        cropped_table[\"tokens\"] = table_tokens\n",
    "\n",
    "        table_crops.append(cropped_table)\n",
    "\n",
    "    return table_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7d230-182d-4bf9-8845-b112ba6dea1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "detection_class_thresholds = {\n",
    "    \"table\": 0.5,\n",
    "    \"table rotated\": 0.5,\n",
    "    \"no object\": 10\n",
    "}\n",
    "crop_padding = 50\n",
    "\n",
    "tables_crops = objects_to_crops(image, tokens, objects, detection_class_thresholds, padding=crop_padding)\n",
    "cropped_table = tables_crops[0][\"image\"].convert(\"RGB\")\n",
    "cropped_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd097d7-502c-4902-862a-fdf1a3ac7614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cropped_table.save(\"isle_of_capri_table.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6d01da-5ad7-447b-8863-b829dc48556d",
   "metadata": {},
   "source": [
    "# Table Structure Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6149cd19-5786-4e9a-9073-441f96573055",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_model = TableTransformerForObjectDetection.from_pretrained(\"microsoft/table-transformer-structure-recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f7c06-33cb-4dcc-92ad-7ccfdf092846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try structure recognition on uncropped image\n",
    "with torch.no_grad():\n",
    "    outputs = structure_model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d4fc5-5f19-4cea-860d-aeba20935f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_sizes = [image.size[::-1]]\n",
    "results = feature_extractor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
    "plot_results(structure_model.config.id2label, image, results[\"scores\"], results[\"labels\"], results[\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98737ebe-38a8-49d5-9277-b78e1a101c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structure_model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4bde8-7505-41d3-a905-0846bc2a860a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_states = outputs.last_hidden_state\n",
    "# these are of shape (batch_size, num_queries, hidden_size)\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a859d-05f1-4467-9611-8c8f34c33a19",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Try structure recognition on cropped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680cbd6-722e-412f-b362-e72aa38ba127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cropped_encoding = feature_extractor(cropped_table, return_tensors=\"pt\")\n",
    "cropped_encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ce813-cb17-468e-994a-bafb4e108931",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = structure_model(**cropped_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a866959-1a2e-4a25-87d1-94e3eb7ed92b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_sizes = [cropped_table.size[::-1]]\n",
    "results = feature_extractor.post_process_object_detection(outputs, threshold=0.6, target_sizes=target_sizes)[0]\n",
    "plot_results(structure_model.config.id2label, cropped_table, results[\"scores\"], results[\"labels\"], results[\"boxes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7cf0d-bd5f-4d3e-bfcf-7283980fb956",
   "metadata": {},
   "source": [
    "# Extract into a dataframe\n",
    "\n",
    "* https://iamrajatroy.medium.com/document-intelligence-series-part-2-transformer-for-table-detection-extraction-80a52486fa3 might be helpful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd16f9d-8890-49e6-9747-f4cf8ff56779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f79f5-57d9-4c9b-8eb7-3b511ff2b7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import easyocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba27466-bcdc-4894-828c-1a4e320a2ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if class_label != \"no object\":\n",
    "            objects.append({\"label\": class_label, \"score\": float(score),\n",
    "                            \"bbox\": [float(elem) for elem in bbox]})\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cacb28-00e9-46b3-a17d-e6f64451a880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# update id2label to include \"no object\"\n",
    "structure_id2label = structure_model.config.id2label\n",
    "structure_id2label[len(structure_id2label)] = \"no object\"\n",
    "\n",
    "cells = outputs_to_objects(outputs, cropped_table.size, structure_id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faa2828-9388-4656-af8f-89d4f415dcd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cell_coordinates_by_row(table_data):\n",
    "    # Extract rows and columns\n",
    "    rows = [entry for entry in table_data if entry[\"label\"] == \"table row\"]\n",
    "    columns = [entry for entry in table_data if entry[\"label\"] == \"table column\"]\n",
    "\n",
    "    # Sort rows and columns by their Y and X coordinates, respectively\n",
    "    rows.sort(key=lambda x: x[\"bbox\"][1])\n",
    "    columns.sort(key=lambda x: x[\"bbox\"][0])\n",
    "\n",
    "    # Function to find cell coordinates\n",
    "    def find_cell_coordinates(row, column):\n",
    "        cell_bbox = [column[\"bbox\"][0], row[\"bbox\"][1], column[\"bbox\"][2], row[\"bbox\"][3]]\n",
    "        return cell_bbox\n",
    "\n",
    "    # Generate cell coordinates and count cells in each row\n",
    "    cell_coordinates = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_cells = []\n",
    "        for column in columns:\n",
    "            cell_bbox = find_cell_coordinates(row, column)\n",
    "            row_cells.append({\"column\": column[\"bbox\"], \"cell\": cell_bbox})\n",
    "\n",
    "        # Sort cells in the row by X coordinate\n",
    "        row_cells.sort(key=lambda x: x[\"column\"][0])\n",
    "\n",
    "        # Append row information to cell_coordinates\n",
    "        cell_coordinates.append({\"row\": row[\"bbox\"], \"cells\": row_cells, \"cell_count\": len(row_cells)})\n",
    "\n",
    "    # Sort rows from top to bottom\n",
    "    cell_coordinates.sort(key=lambda x: x[\"row\"][1])\n",
    "\n",
    "    return cell_coordinates\n",
    "\n",
    "cell_coordinates = get_cell_coordinates_by_row(cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac30c11-cab7-4dfb-a71b-17cc20c5b686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(cell_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5462bae3-85e7-48da-94ca-0d87e98d5d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(cell_coordinates[0][\"cells\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409f900-3f59-4635-8942-bce0e622e4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reader = easyocr.Reader([\"en\"]) # this needs to run only once to load the model into memory\n",
    "\n",
    "def apply_ocr(cell_coordinates):\n",
    "    # let's OCR row by row\n",
    "    data = dict()\n",
    "    max_num_columns = 0\n",
    "    for idx, row in enumerate(tqdm(cell_coordinates)):\n",
    "        row_text = []\n",
    "        for cell in row[\"cells\"]:\n",
    "            # crop cell out of image\n",
    "            cell_image = np.array(cropped_table.crop(cell[\"cell\"]))\n",
    "            # apply OCR\n",
    "            result = reader.readtext(np.array(cell_image))\n",
    "            if len(result) > 0:\n",
    "                # print([x[1] for x in list(result)])\n",
    "                text = \" \".join([x[1] for x in result])\n",
    "                row_text.append(text)\n",
    "\n",
    "        if len(row_text) > max_num_columns:\n",
    "            max_num_columns = len(row_text)\n",
    "\n",
    "        data[idx] = row_text\n",
    "\n",
    "    print(\"Max number of columns:\", max_num_columns)\n",
    "\n",
    "    # pad rows which don't have max_num_columns elements\n",
    "    # to make sure all rows have the same number of columns\n",
    "    for row, row_data in data.copy().items():\n",
    "        if len(row_data) != max_num_columns:\n",
    "            row_data = row_data + [\"\" for _ in range(max_num_columns - len(row_data))]\n",
    "        data[row] = row_data\n",
    "\n",
    "    return data\n",
    "\n",
    "data = apply_ocr(cell_coordinates)\n",
    "\n",
    "for row, row_data in data.items():\n",
    "    print(row_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddb2ef7-8135-4175-afc5-bdb3c6b884d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519cfb05-47e6-4e52-a8f0-e7360cb1b0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(1, len(data)):\n",
    "    data_rows.append(data[i])\n",
    "header = data[0]\n",
    "df = pd.DataFrame(columns=header, data=data_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d608d2bc-4e3a-41e3-a87f-bd8e3d4bb753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\"Name\", \"Jurisdiction of Incorporation\", \"Percentage Owned\"]\n",
    "data_rows = []\n",
    "for i in range(3, len(data)):\n",
    "    data_rows.append(data[i])\n",
    "df = pd.DataFrame(columns=columns, data=data_rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbe6f3e-24be-4ff2-8b25-45741420ea97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine Tuning Table Transformer Model\n",
    "\n",
    "* Goal is to fine tune the model so that it only detects and extracts Ex. 21 tables\n",
    "\n",
    "- [x] Walk through fine tuning tutorial\n",
    "- [ ] Check out PubTables and FinTabNet as suggested by below issue\n",
    "- [ ] Try fine-tuning with a proprietary dataset\n",
    "- [ ] Figure out how to create label dataframe\n",
    "- [ ] Create training data to fine tune with\n",
    "\n",
    "Notes/Ideas:\n",
    "* check the [Pad and Multimodal](https://huggingface.co/docs/transformers/en/preprocessing#multimodal) sections of the Preprocessing tutorial\n",
    "  * Seems like padding the images could be helpful for fine tuning\n",
    "  * Inputs for fine tuning could be multimodal, HTML text and image version of text\n",
    "* \"Typically 50 labeled examples suffice for fine-tuning, but the more data you have, the better.\"\n",
    "* https://github.com/microsoft/table-transformer/issues/127\n",
    "* https://github.com/NielsRogge/Transformers-Tutorials/issues/316\n",
    "* https://developer.ibm.com/exchanges/data/all/fintabnet/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027fd1b-a3be-44ac-87f5-98d460820735",
   "metadata": {},
   "source": [
    "# Facebook RAG Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06e1ed-7070-4369-b262-d2d13dcd8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RagRetriever, RagTokenForGeneration, RagTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb69928a-8565-46fb-a93a-7b5212d54115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the tokenizer and retriever\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n",
    "# Load the generator\n",
    "generator = RagTokenForGeneration.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n",
    "# Your HTML content\n",
    "html_content = \"Your HTML content here\"\n",
    "\n",
    "# Retrieve relevant tables from the HTML content\n",
    "relevant_tables = retriever.retrieve([html_content])\n",
    "\n",
    "# Extract information from tables\n",
    "for table in relevant_tables:\n",
    "    # Tokenize the table\n",
    "    input_ids = tokenizer(table, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # Generate output\n",
    "    outputs = generator.generate(input_ids)\n",
    "    # Decode the output\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39900b8-8f24-46ea-9496-d3953f461763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "from transformers import (\n",
    "    RagRetriever,\n",
    "    RagTokenForGeneration,\n",
    "    RagTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Load the pre-trained tokenizer and retriever\n",
    "tokenizer = RagTokenizer.from_pretrained(\"facebook/rag-token-base\")\n",
    "retriever = RagRetriever.from_pretrained(\"facebook/rag-token-base\")\n",
    "\n",
    "# Load your dataset\n",
    "dataset = datasets.load_dataset(\"your_dataset\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6371ea-ed29-45c9-b912-ccd15ce6d7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset compatible with Trainer API?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce64be1-98a6-4f5b-b25d-0b776709c646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mozilla-sec-eia",
   "language": "python",
   "name": "mozilla-sec-eia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
