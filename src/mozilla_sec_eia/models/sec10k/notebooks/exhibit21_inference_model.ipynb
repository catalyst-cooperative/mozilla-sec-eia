{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da8c588-2d09-464b-945f-168704c0cdac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exhibit 21 extraction and validation\n",
    "\n",
    "This notebook implements a model built on top of [layoutlmv3](https://huggingface.co/microsoft/layoutlmv3-base/tree/main) to extract data\n",
    "from Exhibit 21 attachments to SEC-10k filings. These documents contain a list of all subsidiary companies owned by a filing\n",
    "company. This notebook expects a pre-finetuned version of `layoutlm`, which has been logged to our mlflow tracking server. The notebook `layoutlm_training.ipynb` can be used to finetune and log `layoutlm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd7777-4d06-4d11-85b0-50024db9a7c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load upstream assets and configuration\n",
    "\n",
    "The following cell can be run interactively to set configuration and load upstream assets. When running the notebook in dagster, this cell will be replaced with assets from the dagster run and with run configuration that can be set in the dagster UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab86d93-8194-46ad-8f40-d472f129c10d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Config\n",
    "- `layoutlm_model_uri`: Set URI pointing to finetuned layoutlm model that has been logged to our mlflow server. The notebook `layoutlm.ipynb` can be used to finetune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dadd54-d28c-47c9-8be0-fd4eeeac64e1",
   "metadata": {},
   "source": [
    "### Upstream assets\n",
    "\n",
    "- `ex21_validation_set`: Labeled validation data describing expected inference output on validation filings\n",
    "- `ex21_validation_inference_dataset`: Parsed validation filings prepped for inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982d5778-1f5f-4fdb-bda5-1d6e166c5e54",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No dagster instance configuration file (dagster.yaml) found at /home/zach/catalyst/workspace. Defaulting to loading and storing all metadata with /home/zach/catalyst/workspace. If this is the desired behavior, create an empty dagster.yaml file in /home/zach/catalyst/workspace.\n",
      "2024-10-24 17:25:38 -0400 - dagster - DEBUG - system - Loading file from: /home/zach/catalyst/workspace/storage/ex21_validation_inference_dataset using PickledObjectFilesystemIOManager...\n",
      "No dagster instance configuration file (dagster.yaml) found at /home/zach/catalyst/workspace. Defaulting to loading and storing all metadata with /home/zach/catalyst/workspace. If this is the desired behavior, create an empty dagster.yaml file in /home/zach/catalyst/workspace.\n",
      "2024-10-24 17:25:38 -0400 - dagster - DEBUG - system - Loading file from: /home/zach/catalyst/workspace/storage/ex21_validation_set using PickledObjectFilesystemIOManager...\n"
     ]
    }
   ],
   "source": [
    "import dagstermill\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k import defs\n",
    "\n",
    "context = dagstermill.get_context(op_config={\n",
    "    \"layoutlm_model_uri\": \"runs:/c81cd8c91900476a8362c54fa3a020fc/layoutlm_extractor\",\n",
    "})\n",
    "\n",
    "ex21_validation_inference_dataset = defs.load_asset_value(\"ex21_validation_inference_dataset\")\n",
    "ex21_validation_set = defs.load_asset_value(\"ex21_validation_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b4e20-7781-43a7-b7aa-caf0690a201e",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "Use the finetuned model to perform inference and evaluate on labeled validation data. First create a Huggingface `Pipeline` which wraps layoutlm with some custom pre/post processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c8e920-d671-40c2-b5db-c43611a33897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import Pipeline, pipeline\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.common import (\n",
    "    BBOX_COLS,\n",
    "    get_flattened_mode_predictions,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.utils.layoutlm import (\n",
    "    iob_to_label,\n",
    ")\n",
    "\n",
    "\n",
    "def separate_entities_by_row(entity_df):\n",
    "    \"\"\"Separate entities that span multiple rows and should be distinct.\n",
    "\n",
    "    Sometimes LayoutLM groups multiple entities that span multiple rows\n",
    "    into one entity. This function makes an attempt to break these out\n",
    "    into multiple entities, by taking the average distance between rows\n",
    "    and separating a grouped entity if the distance between y values\n",
    "    is greater than the third quantile of y value spacing.\n",
    "    \"\"\"\n",
    "    threshold = 1.0\n",
    "    entity_df.loc[:, \"line_group\"] = entity_df.loc[:, \"top_left_y\"].transform(\n",
    "        lambda y: (y // threshold).astype(int)\n",
    "    )\n",
    "    # Get the unique y-values for each line (group) per file\n",
    "    line_positions = (\n",
    "        entity_df.groupby([\"line_group\"])[\"top_left_y\"].mean().reset_index()\n",
    "    )\n",
    "    # Calculate the difference between adjacent y-values (i.e., distance between lines)\n",
    "    line_positions.loc[:, \"y_diff\"] = line_positions.loc[:, \"top_left_y\"].diff()\n",
    "    # Filter out NaN values and take the mean of the valid distances\n",
    "    y_diffs = line_positions[\"y_diff\"].dropna()\n",
    "    avg_y_diff = y_diffs.apply(np.floor).mean()\n",
    "    # if an I labeled entity is more than avg_y_diff from it's previoius box then make it a B entity\n",
    "    entity_df.loc[:, \"prev_y\"] = entity_df.loc[:, \"top_left_y\"].shift(1)\n",
    "    entity_df.loc[:, \"prev_iob\"] = entity_df.loc[:, \"iob_pred\"].shift(1)\n",
    "\n",
    "    # If the current prediction is an I label\n",
    "    # and y distance exceeds the average y difference\n",
    "    # update to a B label and make it the start of a new entity\n",
    "    entity_df.loc[:, \"iob_pred\"] = np.where(\n",
    "        (entity_df[\"iob_pred\"].str[0] == \"I\")\n",
    "        & ((entity_df[\"top_left_y\"] - entity_df[\"prev_y\"]) >= avg_y_diff),\n",
    "        \"B\" + entity_df[\"iob_pred\"].str[1:],  # Update to 'B'\n",
    "        entity_df[\"iob_pred\"],  # Keep as is\n",
    "    )\n",
    "\n",
    "    # Drop temporary columns\n",
    "    return entity_df.drop(columns=[\"prev_y\", \"prev_iob\"])\n",
    "\n",
    "class LayoutLMInferencePipeline(Pipeline):\n",
    "    \"\"\"Pipeline for performing inference with fine-tuned LayoutLM.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize LayoutLMInferencePipeline.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, doc_dict):\n",
    "        \"\"\"Encode and tokenize model inputs.\"\"\"\n",
    "        image = doc_dict[\"image\"]\n",
    "        words = doc_dict[\"tokens\"]\n",
    "        boxes = doc_dict[\"bboxes\"]\n",
    "        encoding = self.tokenizer(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,  # this is the maximum max_length\n",
    "            stride=128,\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        model_inputs = {}\n",
    "        model_inputs[\"raw_encoding\"] = encoding.copy()\n",
    "        model_inputs[\"doc_dict\"] = doc_dict\n",
    "        model_inputs[\"offset_mapping\"] = encoding.pop(\"offset_mapping\")\n",
    "        model_inputs[\"sample_mapping\"] = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "        # TODO: do we actually need to make these into ints?\n",
    "        encoding[\"input_ids\"] = encoding[\"input_ids\"].to(torch.int64)\n",
    "        encoding[\"attention_mask\"] = encoding[\"attention_mask\"].to(torch.int64)\n",
    "        encoding[\"bbox\"] = encoding[\"bbox\"].to(torch.int64)\n",
    "        encoding[\"pixel_values\"] = torch.stack(encoding[\"pixel_values\"])\n",
    "        model_inputs[\"encoding\"] = encoding\n",
    "        return model_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # encoding is passed as a UserDict in the model_inputs dictionary\n",
    "        # turn it back into a BatchEncoding\n",
    "        encoding = BatchEncoding(model_inputs[\"encoding\"])\n",
    "        if torch.cuda.is_available():\n",
    "            encoding.to(\"cuda\")\n",
    "            self.model.to(\"cuda\")\n",
    "        # since we're doing inference, we don't need gradient computation\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoding)\n",
    "            return {\n",
    "                \"logits\": output.logits,\n",
    "                \"predictions\": output.logits.argmax(-1).squeeze().tolist(),\n",
    "                \"raw_encoding\": model_inputs[\"raw_encoding\"],\n",
    "                \"doc_dict\": model_inputs[\"doc_dict\"],\n",
    "            }\n",
    "\n",
    "    def postprocess(self, output_dict):\n",
    "        \"\"\"Return logits, model predictions, and the extracted dataframe.\"\"\"\n",
    "        output_df = self.extract_table(output_dict)\n",
    "        output_dict[\"output_df\"] = output_df\n",
    "        return output_dict\n",
    "\n",
    "    def extract_table(self, output_dict):\n",
    "        \"\"\"Extract a structured table from a set of inference predictions.\n",
    "\n",
    "        This function essentially works by stacking bounding boxes and predictions\n",
    "        into a dataframe and going from left to right and top to bottom. Then, every\n",
    "        every time a new subsidiary entity is encountered, it assigns a new group or\n",
    "        \"row\" to that subsidiary. Next, location and ownership percentage words/labeled\n",
    "        entities in between these subsidiary groups are assigned to a subsidiary row/group.\n",
    "        Finally, this is all formatted into a dataframe with an ID column from the original\n",
    "        filename and a basic cleaning function normalizes strings.\n",
    "        \"\"\"\n",
    "        # TODO: when model more mature, break this into sub functions to make it\n",
    "        # clearer what's going on\n",
    "        predictions = output_dict[\"predictions\"]\n",
    "        encoding = output_dict[\"raw_encoding\"]\n",
    "        doc_dict = output_dict[\"doc_dict\"]\n",
    "\n",
    "        token_boxes_tensor = encoding[\"bbox\"].flatten(start_dim=0, end_dim=1)\n",
    "        predictions_tensor = torch.tensor(predictions)\n",
    "        mode_predictions = get_flattened_mode_predictions(\n",
    "            token_boxes_tensor, predictions_tensor\n",
    "        )\n",
    "        token_boxes = encoding[\"bbox\"].flatten(start_dim=0, end_dim=1).tolist()\n",
    "        predicted_labels = [\n",
    "            self.model.config.id2label[pred] for pred in mode_predictions\n",
    "        ]\n",
    "        simple_preds = [iob_to_label(pred).lower() for pred in predicted_labels]\n",
    "\n",
    "        df = pd.DataFrame(data=token_boxes, columns=BBOX_COLS)\n",
    "        df.loc[:, \"iob_pred\"] = predicted_labels\n",
    "        df.loc[:, \"pred\"] = simple_preds\n",
    "        invalid_mask = (\n",
    "            (df[\"top_left_x\"] == 0)\n",
    "            & (df[\"top_left_y\"] == 0)\n",
    "            & (df[\"bottom_right_x\"] == 0)\n",
    "            & (df[\"bottom_right_y\"] == 0)\n",
    "        )\n",
    "        df = df[~invalid_mask]\n",
    "        # we want to get actual words on the dataframe, not just subwords that correspond to tokens\n",
    "        # subwords from the same word share the same bounding box coordinates\n",
    "        # so we merge the original words onto our dataframe on bbox coordinates\n",
    "        words_df = pd.DataFrame(data=doc_dict[\"bboxes\"], columns=BBOX_COLS)\n",
    "        words_df.loc[:, \"word\"] = doc_dict[\"tokens\"]\n",
    "        df = df.merge(words_df, how=\"left\", on=BBOX_COLS).drop_duplicates(\n",
    "            subset=BBOX_COLS + [\"pred\", \"word\"]\n",
    "        )\n",
    "        df = df.sort_values(by=[\"top_left_y\", \"top_left_x\"])\n",
    "        # rows that are the first occurrence in a new group (subsidiary, loc, own_per)\n",
    "        # should always have a B entity label. Manually override labels so this is true.\n",
    "        first_in_group_df = df[\n",
    "            (df[\"pred\"].ne(df[\"pred\"].shift())) & (df[\"pred\"] != \"other\")\n",
    "        ]\n",
    "        first_in_group_df.loc[:, \"iob_pred\"] = (\n",
    "            \"B\" + first_in_group_df[\"iob_pred\"].str[1:]\n",
    "        )\n",
    "        df.update(first_in_group_df)\n",
    "        # filter for just words that were labeled with non \"other\" entities\n",
    "        entities_df = df[df[\"pred\"] != \"other\"]\n",
    "        # boxes that have the same group label but are on different rows\n",
    "        # should be updated to have two different B labels\n",
    "\n",
    "        entities_df = entities_df.groupby(\"pred\").apply(separate_entities_by_row, include_groups=False)\n",
    "        entities_df = entities_df.reset_index(\"pred\").sort_index()\n",
    "        # merge B and I entities to form one entity group\n",
    "        # (i.e. \"B-Subsidiary\" and \"I-Subsidiary\" become just \"subsidiary\"), assign a group ID\n",
    "        entities_df[\"group\"] = (entities_df[\"iob_pred\"].str.startswith(\"B-\")).cumsum()\n",
    "        grouped_df = (\n",
    "            entities_df.groupby([\"group\", \"pred\"])[\"word\"]\n",
    "            .apply(\" \".join)\n",
    "            .reset_index()[[\"pred\", \"word\"]]\n",
    "        )\n",
    "        # assign a new row every time there's a new subsidiary\n",
    "        grouped_df[\"row\"] = (grouped_df[\"pred\"].str.startswith(\"subsidiary\")).cumsum()\n",
    "        output_df = grouped_df.pivot_table(\n",
    "            index=\"row\", columns=\"pred\", values=\"word\", aggfunc=lambda x: \" \".join(x)\n",
    "        ).reset_index()\n",
    "        if output_df.empty:\n",
    "            return output_df\n",
    "        output_df.loc[:, \"id\"] = doc_dict[\"id\"]\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fe887-43ca-43e2-85e3-bf5371bd165f",
   "metadata": {},
   "source": [
    "Next, wrap the `LayoutLMInferencePipeline` in an `mlflow` `pyfunc` model, which handles loading the pretrained model and managing inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d802e00-1ca4-40b3-b15b-561711a9db70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89db8250a02f4d0fab8716d8d85af528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 17:25:47 INFO mlflow.types.utils: Unsupported type hint: <class 'pandas.core.frame.DataFrame'>, skipping schema inference\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805144f1139e4101aad89f5e61632677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 17:26:21 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2024-08-29; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'catalystcoop-mozilla-sec-eia'}\n",
      "2024/10/24 17:26:21 WARNING mlflow.utils.requirements_utils: Found catalystcoop-mozilla-sec-eia version (0.1.dev353+gdf5fe0d.d20241011) contains a local version label (+gdf5fe0d.d20241011). MLflow logged a pip requirement for this package as 'catalystcoop-mozilla-sec-eia==0.1.dev353' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2024/10/24 17:26:21 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03788638e0f546b28f94235476eb8afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "\n",
    "from mozilla_sec_eia.library.mlflow import configure_mlflow\n",
    "from mozilla_sec_eia.models.sec10k.entities import (\n",
    "    Ex21CompanyOwnership,\n",
    "    Sec10kExtractionMetadata,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.ex21_validation_helpers import (\n",
    "    clean_extracted_df,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.utils.cloud import get_metadata_filename\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "configure_mlflow()\n",
    "mlflow.set_experiment(\"exhibit21_extraction_test\")\n",
    "\n",
    "# If a model was trained in this notebook, use it. Otherwise, use\n",
    "model_uri = context.op_config[\"layoutlm_model_uri\"]\n",
    "model_info = mlflow.models.get_model_info(model_uri)\n",
    "\n",
    "def _get_data(dataset):\n",
    "    yield from dataset\n",
    "\n",
    "def _fill_known_nulls(df):\n",
    "    \"\"\"Fill known nulls in location and own per column.\n",
    "\n",
    "    Fill with known values from rows with same subsidiary.\n",
    "    If an extracted Ex. 21 table looks like the following:\n",
    "\n",
    "    subsidiary   loc       own_per\n",
    "    Company A    NaN       NaN\n",
    "    Company A    Delaware  50\n",
    "\n",
    "    Then fill in the first row with location and ownership\n",
    "    percentage from the second row.\n",
    "    \"\"\"\n",
    "    if \"own_per\" in df:\n",
    "        df[\"own_per\"] = df.groupby([\"id\", \"subsidiary\"])[\"own_per\"].transform(\n",
    "            lambda group: group.ffill()\n",
    "        )\n",
    "    if \"loc\" in df:\n",
    "        df[\"loc\"] = df.groupby([\"id\", \"subsidiary\"])[\"loc\"].transform(\n",
    "            lambda group: group.ffill()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "class Ex21Extractor(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Create an mlflow pyfunc model to perform full EX21 extraction.\"\"\"\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load pretrained model.\"\"\"\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "        self.model_components = mlflow.transformers.load_model(\n",
    "            context.artifacts[\"model_components\"], return_type=\"components\"\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame, params=None):\n",
    "        \"\"\"Use pretrained model and inference pipeline to perform inference.\"\"\"\n",
    "        # Convert dataframe to pyarrow Dataset\n",
    "        model_input[\"image\"] = model_input.apply(\n",
    "            lambda row: Image.frombytes(\n",
    "                row[\"mode\"], (row[\"width\"], row[\"height\"]), row[\"image\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        dataset = Dataset.from_list(model_input.drop([\"mode\", \"width\", \"height\"], axis=1).to_dict(\"records\"))\n",
    "\n",
    "        # TODO: figure out device argument\n",
    "        pipe = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=self.model_components[\"model\"],\n",
    "            tokenizer=self.model_components[\"tokenizer\"],\n",
    "            pipeline_class=LayoutLMInferencePipeline,\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        )\n",
    "\n",
    "        logits = []\n",
    "        predictions = []\n",
    "        all_output_df = Ex21CompanyOwnership.example(size=0)\n",
    "        extraction_metadata = Sec10kExtractionMetadata.example(size=0)\n",
    "        for output_dict in pipe(_get_data(dataset)):\n",
    "            logits.append(output_dict[\"logits\"])\n",
    "            predictions.append(output_dict[\"predictions\"])\n",
    "            output_df = output_dict[\"output_df\"]\n",
    "            if not output_df.empty:\n",
    "                filename = get_metadata_filename(output_df[\"id\"].iloc[0])\n",
    "                extraction_metadata.loc[filename, [\"success\"]] = True\n",
    "            all_output_df = pd.concat([all_output_df, output_df])\n",
    "        all_output_df.columns.name = None\n",
    "        all_output_df = clean_extracted_df(all_output_df)\n",
    "        all_output_df = _fill_known_nulls(all_output_df)\n",
    "        all_output_df = all_output_df[[\"id\", \"subsidiary\", \"loc\", \"own_per\"]].drop_duplicates()\n",
    "        all_output_df = all_output_df.reset_index(drop=True)\n",
    "        outputs_dict = {\n",
    "            \"all_output_df\": all_output_df,\n",
    "            \"logits\": logits,\n",
    "            \"predictions\": predictions,\n",
    "        }\n",
    "        return extraction_metadata, outputs_dict\n",
    "\n",
    "# Save model to local temp dir with artifacts, then reload for evaluation\n",
    "with TemporaryDirectory() as tmp_dir:\n",
    "    mlflow.pyfunc.save_model(\n",
    "        path=tmp_dir,\n",
    "        python_model=Ex21Extractor(),\n",
    "        artifacts={\"model_components\": model_uri},\n",
    "    )\n",
    "    ex21_extraction_model = mlflow.pyfunc.load_model(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee84b13-6c37-4afe-8faa-003ff149aa2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation\n",
    "Now the full extraction model can be evaluated using labeled validation data and logged to `mlflow`. The `mlflow` run used to evaluate and log the inference model will be created as a nested child run to the run used to train `layoutlm`. This setup allows multiple versions/configurations of inference to be associated with a single version of `layoutlm`, creating a clean organizational structure for testing the base model and inference logic separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee550f-7b06-4091-a65e-71c6b23a5bea",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validate model\n",
    "Finally, run the full model on the validation set and log metrics to mlflow. The logged metrics/model will appear in a nested run below the training run used for the current version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb56470-8527-424c-a9e5-4135e55fde4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 17:26:22 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "/home/zach/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/modeling_utils.py:1101: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "2024/10/24 17:26:27 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run legendary-ape-465 at: https://mlflow-ned2up6sra-uc.a.run.app/#/experiments/13/runs/b8a509ef72b447b9887f9ec1c0df02a7.\n",
      "2024/10/24 17:26:27 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: https://mlflow-ned2up6sra-uc.a.run.app/#/experiments/13.\n",
      "2024/10/24 17:26:28 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/24 17:26:28 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BBOX_COLS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmozilla_sec_eia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msec10k\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mex_21\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mex21_validation_helpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     ex21_validation_metrics,\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(parent_run_id\u001b[38;5;241m=\u001b[39mmodel_info\u001b[38;5;241m.\u001b[39mrun_id, nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 8\u001b[0m     metadata, outputs_dict \u001b[38;5;241m=\u001b[39m \u001b[43mex21_extraction_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex21_validation_inference_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     extracted \u001b[38;5;241m=\u001b[39m outputs_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_output_df\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m     jaccard_df, prec_recall_df, incorrect_filenames, metrics \u001b[38;5;241m=\u001b[39m ex21_validation_metrics(extracted, ex21_validation_set)\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/mlflow/pyfunc/__init__.py:754\u001b[0m, in \u001b[0;36mPyFuncModel.predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_or_generate_prediction_context() \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_dependencies_schemas_in_prediction_context(context)\n\u001b[0;32m--> 754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/mlflow/pyfunc/__init__.py:792\u001b[0m, in \u001b[0;36mPyFuncModel._predict\u001b[0;34m(self, data, params)\u001b[0m\n\u001b[1;32m    790\u001b[0m params_arg \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params_arg \u001b[38;5;129;01mand\u001b[39;00m params_arg\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mParameter\u001b[38;5;241m.\u001b[39mVAR_KEYWORD:\n\u001b[0;32m--> 792\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_fn(data)\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/mlflow/pyfunc/model.py:637\u001b[0m, in \u001b[0;36m_PythonModelPyfuncWrapper.predict\u001b[0;34m(self, model_input, params)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    model_input: Model input data as one of dict, str, bool, bytes, float, int, str type.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    dict or string. Chunk dict fields are determined by the model implementation.\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m _log_warning_if_params_not_in_predict_signature(_logger, params)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_model\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(model_input))\n",
      "Cell \u001b[0;32mIn[3], line 86\u001b[0m, in \u001b[0;36mEx21Extractor.predict\u001b[0;34m(self, context, model_input, params)\u001b[0m\n\u001b[1;32m     84\u001b[0m all_output_df \u001b[38;5;241m=\u001b[39m Ex21CompanyOwnership\u001b[38;5;241m.\u001b[39mexample(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     85\u001b[0m extraction_metadata \u001b[38;5;241m=\u001b[39m Sec10kExtractionMetadata\u001b[38;5;241m.\u001b[39mexample(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m, in \u001b[0;36mLayoutLMInferencePipeline.postprocess\u001b[0;34m(self, output_dict)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, output_dict):\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return logits, model predictions, and the extracted dataframe.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     output_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     output_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_df\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_df\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_dict\n",
      "Cell \u001b[0;32mIn[2], line 146\u001b[0m, in \u001b[0;36mLayoutLMInferencePipeline.extract_table\u001b[0;34m(self, output_dict)\u001b[0m\n\u001b[1;32m    141\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[pred] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m mode_predictions\n\u001b[1;32m    143\u001b[0m ]\n\u001b[1;32m    144\u001b[0m simple_preds \u001b[38;5;241m=\u001b[39m [iob_to_label(pred)\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_labels]\n\u001b[0;32m--> 146\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39mtoken_boxes, columns\u001b[38;5;241m=\u001b[39m\u001b[43mBBOX_COLS\u001b[49m)\n\u001b[1;32m    147\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miob_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_labels\n\u001b[1;32m    148\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_preds\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BBOX_COLS' is not defined"
     ]
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.ex21_validation_helpers import (\n",
    "    ex21_validation_metrics,\n",
    ")\n",
    "\n",
    "with mlflow.start_run(parent_run_id=model_info.run_id, nested=True):\n",
    "    metadata, outputs_dict = ex21_extraction_model.predict(ex21_validation_inference_dataset.copy())\n",
    "    extracted = outputs_dict[\"all_output_df\"]\n",
    "\n",
    "    jaccard_df, prec_recall_df, incorrect_filenames, metrics = ex21_validation_metrics(extracted, ex21_validation_set)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"exhibit21_extractor\",\n",
    "        python_model=Ex21Extractor(),\n",
    "        artifacts={\"model_components\": model_uri},\n",
    "        signature=infer_signature(ex21_validation_inference_dataset, extracted), # NOTE: model returns a second dataframe with metadata, but mlflow only supports one in signature\n",
    "    )\n",
    "    mlflow.log_table(extracted, \"extracted_data.json\")\n",
    "    mlflow.log_table(metadata, \"extraction_metadata.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a1eef-1c23-46d2-a403-b448ba4fd9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
