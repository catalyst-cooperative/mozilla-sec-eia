{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50bd1847-bf02-4986-ad0b-5388d5c995e6",
   "metadata": {},
   "source": [
    "# Finetune Layoutlm\n",
    "\n",
    "Exhibit 21 extraction is built on top of [layoutlmv3](https://huggingface.co/microsoft/layoutlmv3-base/tree/main). This model works by labels for words as being one of three entities: `subsidiary`, `location`, and `owner percentage`. We have a separate inference portion of the extraction pipeline, which takes these predictions and assembles records in a table from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb48cef-ce11-4d87-8742-80a17b3b3f73",
   "metadata": {},
   "source": [
    "## Load configuration\n",
    "\n",
    "The following cell configures where to pull training data from. If running this notebook from dagster, you can set this value in the launchpad, and it will replace whatever is in this cell. Otherwise, set this value directly below.\n",
    "\n",
    "- `ex21_training_data`: Dataset containing labeled data produced in label-studio to train `layoutlm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e4ef683-8a2a-4e8e-a96b-95c2fb477600",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import dagstermill\n",
    "\n",
    "context = dagstermill.get_context(op_config={\n",
    "    \"ex21_training_data\": \"v0.2\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354059dd-3387-4c3d-aa4f-acebe71a952e",
   "metadata": {},
   "source": [
    "## Prepare training data\n",
    "\n",
    "The config value set above will now be used to download training data from the specified folder in GCS and transform it into a format that can be used by layoutlm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b956de9d-8b9d-49db-8fb9-ceb015f3460d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<table> is empty\n",
      "'<c> The Southwest Companies Nevada PriMerit Bank Federally chartered stock savings bank Paiute Pipeline Company Nevada Carson Water Company Nevada Southwest Gas Transmission Company Partnership between Southwest Gas Corporation and Utility Financial Corp. Utility Financial Corp. Nevada Southwest Gas Corporation of Arizona Nevada PRIMERIT BANK SUBSIDIARIES AT DECEMBER 31, 1993'\n",
      "<table> is empty\n",
      "'<c> TCA Management Company.................................................... Texas Teleservice Corporation of America........................................ Texas Texas Community Antennas, Inc............................................. Texas Texas Telecable, Inc...................................................... Texas TCA Cable of Amarillo, Inc................................................ Texas Telecable Associates, Inc................................................. Texas Delta Cablevision, Inc.................................................... Arkansas Sun Valley Cablevision, Inc............................................... Idaho VPI Communications, Inc................................................... Texas AvComm Corporation........................................................ Texas Tele-Communications of Arkansas L. P......................................'\n",
      "<table> is empty\n",
      "'<c> DOMESTIC SUBSIDIARIES International Sales &amp; Business, Inc. California KLA-Tencor Building Corporation California KLA-Tencor Disc Corporation California KLA-Tencor International Corporation California KLA-Tencor Klinnik Corporation California KLA-Tencor Management Corporation California KLA-Tencor (Thailand Branch) Corporation California VLSI Standards, Inc. California Amray, Inc. Delaware Groff Associates, Inc. California DeviceWare, Inc. California INTERNATIONAL SUBSIDIARIES'\n",
      "<table> is empty\n",
      "'<c> 1. Northeast Energy, LLC (100%-Owned) .................................................... Florida 2. Northeast Energy Associates, A Limited Partnership (99%-Owned) (a) .................... Massachusetts 3. North Jersey Energy Associates, A Limited Partnership (99%-Owned) (a) ................. New Jersey (a) Northeast Energy, LLC owns the remaining 1% interest. </c>'\n",
      "<table> is empty\n",
      "'<c> 1. ESI Tractebel Urban Renewal Corporation (100%-Owned) .................................. New Jersey </c>'\n",
      "<table> is empty\n",
      "'<c> IVANHOE ENERGY HOLDINGS INC. (Nevada) 100% IVANHOE ENERGY (USA) INC. (Nevada) 100% (indirect) IVANHOE ENERGY ROYALTY INC. (Nevada) 100% (indirect) IVANHOE ENERGY INTERNATIONAL VENTURES INC. (BVI) 100% Ivanhoe Energy Sweetwater Limited (Malta) 100% (Indirect) Ivanhoe Energy (Qatar) Inc. (BVI) 100% (Indirect) GTL Japan Corporation (Japan) 100% (Indirect) IVANHOE ENERGY'\n",
      "<table> is empty\n",
      "'<c> Airgas Canada, Inc. Canada Airgas Carbonic, Inc. DE Airgas Data, LLC DE Airgas East, Inc. DE Airgas Great Lakes, Inc. DE Airgas Gulf States, Inc. DE Airgas Intermountain, Inc. CO Airgas International, Inc. VI Airgas Mid America, Inc. DE Airgas Mid South, Inc. DE Airgas Nor Pac, Inc. DE'\n",
      "<table> is empty\n",
      "'<c> Subsidiary Name State of Formation - --------------- ------------------- American Ecology Environmental Services Corporation Texas Corporation American Ecology Holdings Corporation Delaware Corporation American Ecology Recycle Center, Inc. Delaware Corporation American Ecology Services Corporation Delaware Corporation Texas Ecologists, Inc. Texas Corporation US Ecology, Inc. California Corporation US Ecology Idaho, Inc. Delaware'\n",
      "getSize: Not a float ''\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.training import format_as_ner_annotations\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "    ex21_training_data = format_as_ner_annotations(\n",
    "        labeled_json_path=Path(temp_dir) / \"sec10k_filings\" / \"labeled_jsons\",\n",
    "        pdfs_path=Path(temp_dir) / \"sec10k_filings\" / \"pdfs\",\n",
    "        gcs_folder_name=f\"labeled{context.op_config['ex21_training_data']}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80085205-5193-40ef-9960-9dce6e5b77d8",
   "metadata": {},
   "source": [
    "## Define training metrics\n",
    "The method `compute_metrics` will be used to score the model. It computes precision, recall, f1 score, and accuracy on bounding box labels output by `layoutlm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6bb633-c73e-4e3e-b2ed-176b8322ee9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p, metric, label_list, return_entity_level_metrics=False):\n",
    "    \"\"\"Compute metrics to train and evaluate the model on.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[lab] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57fd957-7f39-4722-b73f-dd1c0686ce66",
   "metadata": {},
   "source": [
    "## Finetune Model\n",
    "The next cell will use the functions defined in the previous section to actually construct a huggingface dataset from labeled data and finetune the `layoutlm` model. Model finetuning will only be run if configured to do so, otherwise a pretrained version will be used from the `mlflow` tracking server.\n",
    "\n",
    "Model training contains several steps implemented below:\n",
    "1. Use temporary path to convert filings to PDF's and stash labels\n",
    "2. Use PDF's and labels to convert PDF's and labels to NER annotations\n",
    "3. Construct huggingface dataset from NER annotations and split into train and test sets\n",
    "4. Load pretrained model from huggingface\n",
    "5. Finetune model on training data and evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec6dc9f2-95fa-4cad-9bd1-6b335443ea62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f635902070a45df8290faa0cdb16676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5262/3216464919.py:95: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "/home/zach/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "2024/10/24 11:53:58 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "/home/zach/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/modeling_utils.py:1101: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 10:59, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.373904</td>\n",
       "      <td>0.727903</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.743363</td>\n",
       "      <td>0.883270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286050</td>\n",
       "      <td>0.757225</td>\n",
       "      <td>0.829114</td>\n",
       "      <td>0.791541</td>\n",
       "      <td>0.905660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.243627</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.891501</td>\n",
       "      <td>0.858885</td>\n",
       "      <td>0.937358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212126</td>\n",
       "      <td>0.825976</td>\n",
       "      <td>0.879747</td>\n",
       "      <td>0.852014</td>\n",
       "      <td>0.940881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.180077</td>\n",
       "      <td>0.856187</td>\n",
       "      <td>0.925859</td>\n",
       "      <td>0.889661</td>\n",
       "      <td>0.952453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.214453</td>\n",
       "      <td>0.867797</td>\n",
       "      <td>0.925859</td>\n",
       "      <td>0.895888</td>\n",
       "      <td>0.953711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.187753</td>\n",
       "      <td>0.911636</td>\n",
       "      <td>0.942134</td>\n",
       "      <td>0.926634</td>\n",
       "      <td>0.958491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.203482</td>\n",
       "      <td>0.889565</td>\n",
       "      <td>0.924955</td>\n",
       "      <td>0.906915</td>\n",
       "      <td>0.956981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.292200</td>\n",
       "      <td>0.202835</td>\n",
       "      <td>0.880208</td>\n",
       "      <td>0.916817</td>\n",
       "      <td>0.898140</td>\n",
       "      <td>0.958491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.189829</td>\n",
       "      <td>0.884083</td>\n",
       "      <td>0.924051</td>\n",
       "      <td>0.903625</td>\n",
       "      <td>0.960252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zach/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/modeling_utils.py:1101: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "2024/10/24 12:05:09 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpkgmadbr3/model, flavor: transformers). Fall back to return ['transformers==4.43.3', 'torch==2.4.1', 'torchvision==0.19.1', 'accelerate==0.34.2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model URI: gs://mlflow-artifacts-mozilla/13/55f045a0fa264bf58b95e4199cddaf93/artifacts/layoutlm_extractor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/24 12:13:32 INFO mlflow.tracking._tracking_service.client: 🏃 View run casual-lynx-575 at: https://mlflow-ned2up6sra-uc.a.run.app/#/experiments/13/runs/55f045a0fa264bf58b95e4199cddaf93.\n",
      "2024/10/24 12:13:32 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://mlflow-ned2up6sra-uc.a.run.app/#/experiments/13.\n",
      "2024/10/24 12:13:32 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/24 12:13:33 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "import dagstermill\n",
    "import mlflow\n",
    "from datasets import (\n",
    "    Array2D,\n",
    "    Array3D,\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_metric,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "from mozilla_sec_eia.library.mlflow import configure_mlflow\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.common import (\n",
    "    LABELS,\n",
    "    get_id_label_conversions,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "configure_mlflow()\n",
    "mlflow.set_experiment(\"exhibit21_extraction_test\")\n",
    "\n",
    "\n",
    "def _prepare_dataset(annotations, processor, label2id):\n",
    "    \"\"\"Put the dataset in its final format for training LayoutLM.\"\"\"\n",
    "\n",
    "    def _convert_ner_tags_to_id(ner_tags, label2id):\n",
    "        return [int(label2id[ner_tag]) for ner_tag in ner_tags]\n",
    "\n",
    "    images = annotations[\"image\"]\n",
    "    words = annotations[\"tokens\"]\n",
    "    boxes = annotations[\"bboxes\"]\n",
    "    # Map over labels and convert to numeric id for each ner_tag\n",
    "    ner_tags = [\n",
    "        _convert_ner_tags_to_id(ner_tags, label2id)\n",
    "        for ner_tags in annotations[\"ner_tags\"]\n",
    "    ]\n",
    "\n",
    "    encoding = processor(\n",
    "        images,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        word_labels=ner_tags,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return encoding\n",
    "\n",
    "id2label, label2id = get_id_label_conversions(LABELS)\n",
    "# Change temp_dir to save training data locally for inspection\n",
    "# Cache/prepare training data\n",
    "dataset = Dataset.from_list(ex21_training_data)\n",
    "\n",
    "# Load pretrained model\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\", id2label=id2label, label2id=label2id\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"microsoft/layoutlmv3-base\", apply_ocr=False\n",
    ")\n",
    "\n",
    "# Prepare our train & eval dataset\n",
    "column_names = dataset.column_names\n",
    "features = Features(\n",
    "    {\n",
    "        \"pixel_values\": Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"attention_mask\": Sequence(Value(dtype=\"int64\")),\n",
    "        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "        \"labels\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "    }\n",
    ")\n",
    "dataset = dataset.map(\n",
    "    lambda annotations: _prepare_dataset(annotations, processor, label2id),\n",
    "    batched=True,\n",
    "    remove_columns=column_names,\n",
    "    features=features,\n",
    ")\n",
    "dataset.set_format(\"torch\")\n",
    "split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset, eval_dataset = split_dataset[\"train\"], split_dataset[\"test\"]\n",
    "\n",
    "# Initialize our Trainer\n",
    "metric = load_metric(\"seqeval\")\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    learning_rate=1e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    output_dir=\"./layoutlm\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor,\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics(p, metric=metric, label_list=LABELS),\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as training_run:\n",
    "    # Train inside mlflow run. Mlflow will automatically handle logging training metrcis\n",
    "    trainer.train()\n",
    "\n",
    "    # Log finetuend model with mlflow\n",
    "    model = {\"model\": trainer.model, \"tokenizer\": trainer.tokenizer}\n",
    "    mlflow.transformers.log_model(\n",
    "        model, artifact_path=\"layoutlm_extractor\", task=\"token-classification\"\n",
    "    )\n",
    "\n",
    "    # Return output from notebook (URI of logged model)\n",
    "    dagstermill.yield_result(mlflow.get_artifact_uri(\"layoutlm_extractor\"), output_name=\"finetuned_layoutlm_uri\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
