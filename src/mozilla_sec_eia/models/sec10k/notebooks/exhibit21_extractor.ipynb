{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da8c588-2d09-464b-945f-168704c0cdac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exhibit 21 extraction\n",
    "\n",
    "This notebook implements a model built on top of [layoutlmv3](https://huggingface.co/microsoft/layoutlmv3-base/tree/main)\n",
    "from Exhibit 21 attachments to SEC-10k filings. These documents contain a list of all subsidiary companies owned by a filing\n",
    "company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aab877-9d59-4ec7-bf4b-c75e216fb1d6",
   "metadata": {},
   "source": [
    "## Load upstream assets and configuration\n",
    "The following cell can be run interactively to set configuration and load upstream assets. When running the notebook in dagster, this cell will be replaced with assets from the dagster run and dagster run configuration.\n",
    "\n",
    "### Config\n",
    "- `layoutlm_uri`: If `None` the notebook will finetune layoutlm using `ex21_training_data`. If `layoutlm_uri` points to a valid model on the mlflow tracking server, the notebook will use the pre-trained model and perform inference on the validation set, logging validation metrics to a child run nested under the mlflow run associated with the pretrained model.\n",
    "\n",
    "### Upstream assets\n",
    "We are using dagster assets to construct training/validation data outside the notebook to allow for easy caching. These datasets are fairly compute intensive to create, so this is useful when iterating on the model using the same data.\n",
    "\n",
    "NOTE: The notebook will load the most recent version of these assets, so to update the training/validation data you must rerun the dagster assets with desired configuration.\n",
    "\n",
    "- `ex21_training_data`: Dataset containing labeled data produced in label-studio to train `layoutlm`\n",
    "- `ex21_validation_set`: Labeled validation data describing expected inference output on validation filings\n",
    "- `ex21_failed_parsing_metadata`: Metadata for any validation filings that couldn't be parsed (usually empty)\n",
    "- `ex21_inference_dataset`: Parsed validation filings prepped for inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f185de-95ef-4194-9245-93f8d603d2e6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import dagstermill\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k import defs\n",
    "\n",
    "context = dagstermill.get_context(op_config={\n",
    "    \"layoutlm_training_run\": \"layoutlm-labeledv0.2\",\n",
    "})\n",
    "\n",
    "ex21_training_data = defs.load_asset_value(\"ex21_training_data\")\n",
    "\n",
    "ex21_failed_parsing_metadata = defs.load_asset_value(\"ex21_failed_parsing_metadata\")\n",
    "ex21_inference_dataset = defs.load_asset_value(\"ex21_inference_dataset\")\n",
    "ex21_validation_set = defs.load_asset_value(\"ex21_validation_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f299b2b-2358-4526-b023-f29c817316d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Layoutlmv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edcce1-ab18-40b6-9da8-ce0ea53c2f72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define training metrics\n",
    "The method `compute_metrics` will be used to score the model. It computes precision, recall, f1 score, and accuracy on bounding box labels output by `layoutlm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372b908-d9b9-4d18-a5bf-d332648b3e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mozilla_sec_eia.library import validation_helpers\n",
    "from mozilla_sec_eia.models.sec10k.utils.cloud import get_metadata_filename\n",
    "\n",
    "\n",
    "def compute_metrics(p, metric, label_list, return_entity_level_metrics=False):\n",
    "    \"\"\"Compute metrics to train and evaluate the model on.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[lab] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160263c-8f69-437c-918b-e56ad007961a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Finetune Model\n",
    "The next cell will use the functions defined in the previous section to actually construct a huggingface dataset from labeled data and finetune the `layoutlm` model. Model finetuning will only be run if configured to do so, otherwise a pretrained version will be used from the `mlflow` tracking server.\n",
    "\n",
    "Model training contains several steps implemented below:\n",
    "1. Use temporary path to convert filings to PDF's and stash labels\n",
    "2. Use PDF's and labels to convert PDF's and labels to NER annotations\n",
    "3. Construct huggingface dataset from NER annotations and split into train and test sets\n",
    "4. Load pretrained model from huggingface\n",
    "5. Finetune model on training data and evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d205b2-e6ea-4ad0-982c-22e762269119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from datasets import (\n",
    "    Array2D,\n",
    "    Array3D,\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_metric,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "from mozilla_sec_eia.library.mlflow import configure_mlflow\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.common import (\n",
    "    BBOX_COLS,\n",
    "    LABELS,\n",
    "    get_id_label_conversions,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "configure_mlflow()\n",
    "mlflow.set_experiment(\"exhibit21_extraction_test\")\n",
    "\n",
    "\n",
    "def _prepare_dataset(annotations, processor, label2id):\n",
    "    \"\"\"Put the dataset in its final format for training LayoutLM.\"\"\"\n",
    "\n",
    "    def _convert_ner_tags_to_id(ner_tags, label2id):\n",
    "        return [int(label2id[ner_tag]) for ner_tag in ner_tags]\n",
    "\n",
    "    images = annotations[\"image\"]\n",
    "    words = annotations[\"tokens\"]\n",
    "    boxes = annotations[\"bboxes\"]\n",
    "    # Map over labels and convert to numeric id for each ner_tag\n",
    "    ner_tags = [\n",
    "        _convert_ner_tags_to_id(ner_tags, label2id)\n",
    "        for ner_tags in annotations[\"ner_tags\"]\n",
    "    ]\n",
    "\n",
    "    encoding = processor(\n",
    "        images,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        word_labels=ner_tags,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return encoding\n",
    "\n",
    "if (run_name := context.op_config[\"layoutlm_training_run\"]) is not None:\n",
    "    filter_string = f\"attributes.run_name = '{run_name}'\"\n",
    "    run = mlflow.search_runs(filter_string=filter_string, output_format=\"list\")[0]\n",
    "    training_run_id = run.info.run_id\n",
    "else:\n",
    "    training_run_id = None\n",
    "\n",
    "# Only finetune if configured to do so\n",
    "if training_run_id is None:\n",
    "    id2label, label2id = get_id_label_conversions(LABELS)\n",
    "    # Change temp_dir to save training data locally for inspection\n",
    "    # Cache/prepare training data\n",
    "    dataset = Dataset.from_list(ex21_training_data)\n",
    "\n",
    "    # Load pretrained model\n",
    "    model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\", id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\", apply_ocr=False\n",
    "    )\n",
    "\n",
    "    # Prepare our train & eval dataset\n",
    "    column_names = dataset.column_names\n",
    "    features = Features(\n",
    "        {\n",
    "            \"pixel_values\": Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "            \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "            \"attention_mask\": Sequence(Value(dtype=\"int64\")),\n",
    "            \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "            \"labels\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        }\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        lambda annotations: _prepare_dataset(annotations, processor, label2id),\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        features=features,\n",
    "    )\n",
    "    dataset.set_format(\"torch\")\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset, eval_dataset = split_dataset[\"train\"], split_dataset[\"test\"]\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    metric = load_metric(\"seqeval\")\n",
    "    training_args = TrainingArguments(\n",
    "        max_steps=1000,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=1e-5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        output_dir=\"./layoutlm\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p, metric=metric, label_list=LABELS),\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run() as training_run:\n",
    "        # Train inside mlflow run. Mlflow will automatically handle logging training metrcis\n",
    "        trainer.train()\n",
    "\n",
    "        # Log finetuend model with mlflow\n",
    "        model = {\"model\": trainer.model, \"tokenizer\": trainer.tokenizer}\n",
    "        mlflow.transformers.log_model(\n",
    "            model, artifact_path=\"layoutlm_extractor\", task=\"token-classification\"\n",
    "        )\n",
    "        training_run_id = training_run.info. run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b4e20-7781-43a7-b7aa-caf0690a201e",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "Use the finetuned model to perform inference and evaluate on labeled validation data. First create a Huggingface `Pipeline` which wraps layoutlm with some custom pre/post processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8e920-d671-40c2-b5db-c43611a33897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Pipeline, pipeline\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.common import (\n",
    "    get_flattened_mode_predictions,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.utils.layoutlm import (\n",
    "    iob_to_label,\n",
    ")\n",
    "\n",
    "\n",
    "def separate_entities_by_row(df):\n",
    "    \"\"\"Separate entities that span multiple rows and should be distinct.\n",
    "\n",
    "    Sometimes LayoutLM groups multiple entities that span multiple rows\n",
    "    into one entity. This function makes an attempt to break these out\n",
    "    into multiple entities, by taking the average distance between rows\n",
    "    and separating a grouped entity if the distance between y values\n",
    "    is greater than the third quantile of y value spacing.\n",
    "    \"\"\"\n",
    "    threshold = 1.0\n",
    "    for entity in [\"subsidiary\", \"loc\", \"own_per\"]:\n",
    "        entity_df = df[df[\"pred\"] == entity]\n",
    "        entity_df[\"line_group\"] = entity_df[\"top_left_y\"].transform(\n",
    "            lambda y: (y // threshold).astype(int)\n",
    "        )\n",
    "        # Get the unique y-values for each line (group) per file\n",
    "        line_positions = (\n",
    "            entity_df.groupby([\"line_group\"])[\"top_left_y\"].mean().reset_index()\n",
    "        )\n",
    "        # Calculate the difference between adjacent y-values (i.e., distance between lines)\n",
    "        line_positions[\"y_diff\"] = line_positions[\"top_left_y\"].diff()\n",
    "        # Filter out NaN values and take the mean of the valid distances\n",
    "        y_diffs = line_positions[\"y_diff\"].dropna()\n",
    "        avg_y_diff = y_diffs.apply(np.floor).mean()\n",
    "        # if an I labeled entity is more than avg_y_diff from it's previoius box then make it a B entity\n",
    "        entity_df[\"prev_y\"] = entity_df[\"top_left_y\"].shift(1)\n",
    "        entity_df[\"prev_iob\"] = entity_df[\"iob_pred\"].shift(1)\n",
    "\n",
    "        # If the current prediction is an I label\n",
    "        # and y distance exceeds the average y difference\n",
    "        # update to a B label and make it the start of a new entity\n",
    "        entity_df[\"iob_pred\"] = np.where(\n",
    "            (entity_df[\"iob_pred\"].str[0] == \"I\")\n",
    "            & ((entity_df[\"top_left_y\"] - entity_df[\"prev_y\"]) >= avg_y_diff),\n",
    "            \"B\" + entity_df[\"iob_pred\"].str[1:],  # Update to 'B'\n",
    "            entity_df[\"iob_pred\"],  # Keep as is\n",
    "        )\n",
    "\n",
    "        # Drop temporary columns\n",
    "        entity_df = entity_df.drop(columns=[\"prev_y\", \"prev_iob\"])\n",
    "        df.update(entity_df, overwrite=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "class LayoutLMInferencePipeline(Pipeline):\n",
    "    \"\"\"Pipeline for performing inference with fine-tuned LayoutLM.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize LayoutLMInferencePipeline.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, doc_dict):\n",
    "        \"\"\"Encode and tokenize model inputs.\"\"\"\n",
    "        image = doc_dict[\"image\"]\n",
    "        words = doc_dict[\"tokens\"]\n",
    "        boxes = doc_dict[\"bboxes\"]\n",
    "        encoding = self.tokenizer(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,  # this is the maximum max_length\n",
    "            stride=128,\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        model_inputs = {}\n",
    "        model_inputs[\"raw_encoding\"] = encoding.copy()\n",
    "        model_inputs[\"doc_dict\"] = doc_dict\n",
    "        model_inputs[\"offset_mapping\"] = encoding.pop(\"offset_mapping\")\n",
    "        model_inputs[\"sample_mapping\"] = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "        # TODO: do we actually need to make these into ints?\n",
    "        encoding[\"input_ids\"] = encoding[\"input_ids\"].to(torch.int64)\n",
    "        encoding[\"attention_mask\"] = encoding[\"attention_mask\"].to(torch.int64)\n",
    "        encoding[\"bbox\"] = encoding[\"bbox\"].to(torch.int64)\n",
    "        encoding[\"pixel_values\"] = torch.stack(encoding[\"pixel_values\"])\n",
    "        model_inputs[\"encoding\"] = encoding\n",
    "        return model_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # encoding is passed as a UserDict in the model_inputs dictionary\n",
    "        # turn it back into a BatchEncoding\n",
    "        encoding = BatchEncoding(model_inputs[\"encoding\"])\n",
    "        if torch.cuda.is_available():\n",
    "            encoding.to(\"cuda\")\n",
    "            self.model.to(\"cuda\")\n",
    "        # since we're doing inference, we don't need gradient computation\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoding)\n",
    "            return {\n",
    "                \"logits\": output.logits,\n",
    "                \"predictions\": output.logits.argmax(-1).squeeze().tolist(),\n",
    "                \"raw_encoding\": model_inputs[\"raw_encoding\"],\n",
    "                \"doc_dict\": model_inputs[\"doc_dict\"],\n",
    "            }\n",
    "\n",
    "    def postprocess(self, all_outputs):\n",
    "        \"\"\"Return logits, model predictions, and the extracted dataframe.\"\"\"\n",
    "        logits = all_outputs[\"logits\"]\n",
    "        predictions = all_outputs[\"logits\"].argmax(-1).squeeze().tolist()\n",
    "        output_df = self.extract_table(all_outputs)\n",
    "        return logits, predictions, output_df\n",
    "\n",
    "    def extract_table(self, all_outputs):\n",
    "        \"\"\"Extract a structured table from a set of inference predictions.\n",
    "\n",
    "        This function essentially works by stacking bounding boxes and predictions\n",
    "        into a dataframe and going from left to right and top to bottom. Then, every\n",
    "        every time a new subsidiary entity is encountered, it assigns a new group or\n",
    "        \"row\" to that subsidiary. Next, location and ownership percentage words/labeled\n",
    "        entities in between these subsidiary groups are assigned to a subsidiary row/group.\n",
    "        Finally, this is all formatted into a dataframe with an ID column from the original\n",
    "        filename and a basic cleaning function normalizes strings.\n",
    "        \"\"\"\n",
    "        # TODO: when model more mature, break this into sub functions to make it\n",
    "        # clearer what's going on\n",
    "        predictions = all_outputs[\"predictions\"]\n",
    "        encoding = all_outputs[\"raw_encoding\"]\n",
    "        doc_dict = all_outputs[\"doc_dict\"]\n",
    "\n",
    "        token_boxes_tensor = encoding[\"bbox\"].flatten(start_dim=0, end_dim=1)\n",
    "        predictions_tensor = torch.tensor(predictions)\n",
    "        mode_predictions = get_flattened_mode_predictions(\n",
    "            token_boxes_tensor, predictions_tensor\n",
    "        )\n",
    "        token_boxes = encoding[\"bbox\"].flatten(start_dim=0, end_dim=1).tolist()\n",
    "        predicted_labels = [\n",
    "            self.model.config.id2label[pred] for pred in mode_predictions\n",
    "        ]\n",
    "        simple_preds = [iob_to_label(pred).lower() for pred in predicted_labels]\n",
    "\n",
    "        df = pd.DataFrame(data=token_boxes, columns=BBOX_COLS)\n",
    "        df.loc[:, \"iob_pred\"] = predicted_labels\n",
    "        df.loc[:, \"pred\"] = simple_preds\n",
    "        invalid_mask = (\n",
    "            (df[\"top_left_x\"] == 0)\n",
    "            & (df[\"top_left_y\"] == 0)\n",
    "            & (df[\"bottom_right_x\"] == 0)\n",
    "            & (df[\"bottom_right_y\"] == 0)\n",
    "        )\n",
    "        df = df[~invalid_mask]\n",
    "        # we want to get actual words on the dataframe, not just subwords that correspond to tokens\n",
    "        # subwords from the same word share the same bounding box coordinates\n",
    "        # so we merge the original words onto our dataframe on bbox coordinates\n",
    "        words_df = pd.DataFrame(data=doc_dict[\"bboxes\"], columns=BBOX_COLS)\n",
    "        words_df.loc[:, \"word\"] = doc_dict[\"tokens\"]\n",
    "        df = df.merge(words_df, how=\"left\", on=BBOX_COLS).drop_duplicates(\n",
    "            subset=BBOX_COLS + [\"pred\", \"word\"]\n",
    "        )\n",
    "        # rows that are the first occurrence in a new group (subsidiary, loc, own_per)\n",
    "        # should always have a B entity label. Manually override labels so this is true.\n",
    "        first_in_group_df = df[\n",
    "            (df[\"pred\"].ne(df[\"pred\"].shift())) & (df[\"pred\"] != \"other\")\n",
    "        ]\n",
    "        first_in_group_df.loc[:, \"iob_pred\"] = (\n",
    "            \"B\" + first_in_group_df[\"iob_pred\"].str[1:]\n",
    "        )\n",
    "        df.update(first_in_group_df)\n",
    "        # filter for just words that were labeled with non \"other\" entities\n",
    "        entities_df = df[df[\"pred\"] != \"other\"]\n",
    "        # boxes that have the same group label but are on different rows\n",
    "        # should be updated to have two different B labels\n",
    "        entities_df = separate_entities_by_row(entities_df)\n",
    "        # words are labeled with IOB format which stands for inside, outside, beginning\n",
    "        # merge B and I entities to form one entity group\n",
    "        # (i.e. \"B-Subsidiary\" and \"I-Subsidiary\" become just \"subsidiary\"), assign a group ID\n",
    "        entities_df[\"group\"] = (entities_df[\"iob_pred\"].str.startswith(\"B-\")).cumsum()\n",
    "        grouped_df = (\n",
    "            entities_df.groupby([\"group\", \"pred\"])[\"word\"]\n",
    "            .apply(\" \".join)\n",
    "            .reset_index()[[\"pred\", \"word\"]]\n",
    "        )\n",
    "        # assign a new row every time there's a new subsidiary\n",
    "        grouped_df[\"row\"] = (grouped_df[\"pred\"].str.startswith(\"subsidiary\")).cumsum()\n",
    "        output_df = grouped_df.pivot_table(\n",
    "            index=\"row\", columns=\"pred\", values=\"word\", aggfunc=lambda x: \" \".join(x)\n",
    "        ).reset_index()\n",
    "        if output_df.empty:\n",
    "            return output_df\n",
    "        output_df.loc[:, \"id\"] = doc_dict[\"id\"]\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fe887-43ca-43e2-85e3-bf5371bd165f",
   "metadata": {},
   "source": [
    "Next, wrap the `LayoutLMInferencePipeline` in an `mlflow` `pyfunc` model, which handles loading the pretrained model and managing inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d802e00-1ca4-40b3-b15b-561711a9db70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.entities import (\n",
    "    Ex21CompanyOwnership,\n",
    "    Sec10kExtractionMetadata,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.ex21_validation_helpers import (\n",
    "    clean_extracted_df,\n",
    ")\n",
    "\n",
    "# If a model was trained in this notebook, use it. Otherwise, use\n",
    "model_uri = f\"runs:/{training_run_id}/layoutlm_extractor\"\n",
    "model_info = mlflow.models.get_model_info(model_uri)\n",
    "\n",
    "def _get_data(dataset):\n",
    "    yield from dataset\n",
    "\n",
    "def _fill_known_nulls(df):\n",
    "    \"\"\"Fill known nulls in location and own per column.\n",
    "\n",
    "    Fill with known values from rows with same subsidiary.\n",
    "    \"\"\"\n",
    "    if \"own_per\" in df:\n",
    "        df[\"own_per\"] = df.groupby([\"id\", \"subsidiary\"])[\"own_per\"].transform(\n",
    "            lambda group: group.ffill()\n",
    "        )\n",
    "    if \"loc\" in df:\n",
    "        df[\"loc\"] = df.groupby([\"id\", \"subsidiary\"])[\"loc\"].transform(\n",
    "            lambda group: group.ffill()\n",
    "        )\n",
    "    return df\n",
    "\n",
    "class Ex21Extractor(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Create an mlflow pyfunc model to perform full EX21 extraction.\"\"\"\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load pretrained model.\"\"\"\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "        self.model_components = mlflow.transformers.load_model(\n",
    "            context.artifacts[\"model_components\"], return_type=\"components\"\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame, params=None):\n",
    "        \"\"\"Use pretrained model and inference pipeline to perform inference.\"\"\"\n",
    "        # Convert dataframe to pyarrow Dataset\n",
    "        model_input[\"image\"] = model_input.apply(\n",
    "            lambda row: Image.frombytes(\n",
    "                row[\"mode\"], (row[\"width\"], row[\"height\"]), row[\"image\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        dataset = Dataset.from_list(model_input.drop([\"mode\", \"width\", \"height\"], axis=1).to_dict(\"records\"))\n",
    "\n",
    "        # TODO: figure out device argument\n",
    "        pipe = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=self.model_components[\"model\"],\n",
    "            tokenizer=self.model_components[\"tokenizer\"],\n",
    "            pipeline_class=LayoutLMInferencePipeline,\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        )\n",
    "\n",
    "        logits = []\n",
    "        predictions = []\n",
    "        all_output_df = Ex21CompanyOwnership.example(size=0)\n",
    "        extraction_metadata = Sec10kExtractionMetadata.example(size=0)\n",
    "        for logit, pred, output_df in pipe(_get_data(dataset)):\n",
    "            logits.append(logit)\n",
    "            predictions.append(pred)\n",
    "            if not output_df.empty:\n",
    "                filename = get_metadata_filename(output_df[\"id\"].iloc[0])\n",
    "                extraction_metadata.loc[filename, [\"success\"]] = True\n",
    "            all_output_df = pd.concat([all_output_df, output_df])\n",
    "        all_output_df.columns.name = None\n",
    "        all_output_df = clean_extracted_df(all_output_df)\n",
    "        all_output_df = _fill_known_nulls(all_output_df)\n",
    "        all_output_df = all_output_df[[\"id\", \"subsidiary\", \"loc\", \"own_per\"]].drop_duplicates()\n",
    "        all_output_df = all_output_df.reset_index(drop=True)\n",
    "        return extraction_metadata, all_output_df\n",
    "\n",
    "# Save model to local temp dir with artifacts, then reload for evaluation\n",
    "with TemporaryDirectory() as tmp_dir:\n",
    "    mlflow.pyfunc.save_model(\n",
    "        path=tmp_dir,\n",
    "        python_model=Ex21Extractor(),\n",
    "        artifacts={\"model_components\": model_uri},\n",
    "    )\n",
    "    ex21_extraction_model = mlflow.pyfunc.load_model(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee84b13-6c37-4afe-8faa-003ff149aa2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation\n",
    "Now the full extraction model can be evaluated using labeled validation data and logged to `mlflow`. The `mlflow` run used to evaluate and log the inference model will be created as a nested child run to the run used to train `layoutlm`. This setup allows multiple versions/configurations of inference to be associated with a single version of `layoutlm`, creating a clean organizational structure for testing the base model and inference logic separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd74bdc-bb63-4ad2-82ec-3dfcf93a6121",
   "metadata": {},
   "source": [
    "#### Load validation data\n",
    "Next, load an inference dataset containing validation data. This dataset is formatted exactly the same as those that will feed into the `Ex21Extractor` during a production run, but contain only data from the validation set. When creating inference datasets we also produce a metadata dataframe documenting any filings that couldn't be parsed/converted to a PDF. This dataframe should be empty for the validation set, but we will still load it for consistency with production runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcc912-324a-42e9-9841-3a916c6ece6b",
   "metadata": {},
   "source": [
    "Next define method method for computing validation metrics. The metrics computed above for training are looking at bounding boxes output by `layoutlm` and pertain to one word at a time. These metrics will look at an entire table produced the inference pipeline and compare to the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bd14d-5156-4f34-9a50-e9c813b822cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "def ex21_validation_metrics(computed_df: pd.DataFrame, validation_df: pd.DataFrame):\n",
    "    \"\"\"Compute validation metrics for Ex. 21 extraction.\"\"\"\n",
    "    shared_cols = validation_df.columns.intersection(computed_df.columns)\n",
    "    validation_df = validation_df.astype(computed_df[shared_cols].dtypes)\n",
    "    # strip llc and other company name parts for the similarity comparison\n",
    "    computed_df[\"subsidiary\"] = validation_helpers.strip_down_company_names(\n",
    "        computed_df[\"subsidiary\"]\n",
    "    )\n",
    "    validation_df[\"subsidiary\"] = validation_helpers.strip_down_company_names(\n",
    "        validation_df[\"subsidiary\"]\n",
    "    )\n",
    "    n_equal = 0\n",
    "    validation_filenames = validation_df[\"id\"].unique()\n",
    "    n_files = len(validation_filenames)\n",
    "    table_metrics_dict = {}\n",
    "    jaccard_dict = {}\n",
    "    incorrect_files = []\n",
    "    # iterate through each file and check each extracted table\n",
    "    for filename in validation_filenames:\n",
    "        extracted_table_df = computed_df[computed_df[\"id\"] == filename].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        validation_table_df = validation_df[\n",
    "            validation_df[\"id\"] == filename\n",
    "        ].reset_index(drop=True)\n",
    "        # check if the tables are exactly equal\n",
    "        if extracted_table_df[[\"subsidiary\", \"loc\", \"own_per\"]].equals(\n",
    "            validation_table_df[[\"subsidiary\", \"loc\", \"own_per\"]]\n",
    "        ):\n",
    "            n_equal += 1\n",
    "        else:\n",
    "            incorrect_files.append(filename)\n",
    "        # compute jaccard sim + precision and recall for each column\n",
    "        table_metrics_dict[filename] = {}\n",
    "        jaccard_dict[filename] = {}\n",
    "        for col in [\"subsidiary\", \"loc\", \"own_per\"]:\n",
    "            extracted_table_df[col] = validation_helpers.fill_nulls_for_comparison(\n",
    "                extracted_table_df[col]\n",
    "            )\n",
    "            validation_table_df[col] = validation_helpers.fill_nulls_for_comparison(\n",
    "                validation_table_df[col]\n",
    "            )\n",
    "            table_prec_recall = validation_helpers.pandas_compute_precision_recall(\n",
    "                extracted_table_df, validation_table_df, value_col=col\n",
    "            )\n",
    "            table_metrics_dict[filename][f\"{col}_precision\"] = table_prec_recall[\n",
    "                \"precision\"\n",
    "            ]\n",
    "            table_metrics_dict[filename][f\"{col}_recall\"] = table_prec_recall[\"recall\"]\n",
    "            # get the jaccard similarity between columns\n",
    "            jaccard_dict[filename][col] = validation_helpers.jaccard_similarity(\n",
    "                computed_df=extracted_table_df,\n",
    "                validation_df=validation_table_df,\n",
    "                value_col=col,\n",
    "            )\n",
    "\n",
    "    jaccard_df = pd.DataFrame.from_dict(jaccard_dict, orient=\"index\").reset_index()\n",
    "    prec_recall_df = pd.DataFrame.from_dict(\n",
    "        table_metrics_dict, orient=\"index\"\n",
    "    ).reset_index()\n",
    "\n",
    "    return (\n",
    "        jaccard_df,\n",
    "        prec_recall_df,\n",
    "        pd.DataFrame({\"filename\": incorrect_files}),\n",
    "        {\n",
    "            \"table_accuracy\": n_equal / n_files,\n",
    "            \"avg_subsidiary_jaccard_sim\": jaccard_df[\"subsidiary\"].sum() / n_files,\n",
    "            \"avg_location_jaccard_sim\": jaccard_df[\"loc\"].sum() / n_files,\n",
    "            \"avg_own_per_jaccard_sim\": jaccard_df[\"own_per\"].sum() / n_files,\n",
    "            \"avg_subsidiary_precision\": prec_recall_df[\"subsidiary_precision\"].sum()\n",
    "            / n_files,\n",
    "            \"avg_location_precision\": prec_recall_df[\"loc_precision\"].sum() / n_files,\n",
    "            \"avg_own_per_precision\": prec_recall_df[\"own_per_precision\"].sum()\n",
    "            / n_files,\n",
    "            \"avg_subsidiary_recall\": prec_recall_df[\"subsidiary_recall\"].sum()\n",
    "            / n_files,\n",
    "            \"avg_location_recall\": prec_recall_df[\"loc_recall\"].sum() / n_files,\n",
    "            \"avg_own_per_recall\": prec_recall_df[\"own_per_recall\"].sum() / n_files,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee550f-7b06-4091-a65e-71c6b23a5bea",
   "metadata": {},
   "source": [
    "#### Validate model\n",
    "Finally, run the full model on the validation set and log metrics to mlflow. The logged metrics/model will appear in a nested run below the training run used for the current version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb56470-8527-424c-a9e5-4135e55fde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(parent_run_id=model_info.run_id, nested=True):\n",
    "    metadata, extracted = ex21_extraction_model.predict(ex21_inference_dataset.copy())\n",
    "    metadata = pd.concat([ex21_failed_parsing_metadata, metadata])\n",
    "\n",
    "    jaccard_df, prec_recall_df, incorrect_filenames, metrics = ex21_validation_metrics(extracted, ex21_validation_set)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"exhibit21_extractor\",\n",
    "        python_model=Ex21Extractor(),\n",
    "        artifacts={\"model_components\": model_uri},\n",
    "        signature=infer_signature(ex21_inference_dataset, extracted), # NOTE: model returns a second dataframe with metadata, but mlflow only supports one in signature\n",
    "    )\n",
    "    mlflow.log_table(extracted, \"extracted_data.json\")\n",
    "    mlflow.log_table(metadata, \"extraction_metadata.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
