{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da8c588-2d09-464b-945f-168704c0cdac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exhibit 21 extraction\n",
    "\n",
    "This notebook implements a model built on top of [layoutlmv3](https://huggingface.co/microsoft/layoutlmv3-base/tree/main)\n",
    "from Exhibit 21 attachments to SEC-10k filings. These documents contain a list of all subsidiary companies owned by a filing\n",
    "company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84aab877-9d59-4ec7-bf4b-c75e216fb1d6",
   "metadata": {},
   "source": [
    "## Load upstream assets and configuration\n",
    "The following cell can be run interactively to set configuration and load upstream assets. When running the notebook in dagster, this cell will be replaced with assets from the dagster run and dagster run configuration.\n",
    "\n",
    "### Config\n",
    "- `layoutlm_uri`: If `None` the notebook will finetune layoutlm using `ex21_training_data`. If `layoutlm_uri` points to a valid model on the mlflow tracking server, the notebook will use the pre-trained model and perform inference on the validation set, logging validation metrics to a child run nested under the mlflow run associated with the pretrained model.\n",
    "\n",
    "### Upstream assets\n",
    "We are using dagster assets to construct training/validation data outside the notebook to allow for easy caching. These datasets are fairly compute intensive to create, so this is useful when iterating on the model using the same data.\n",
    "\n",
    "NOTE: The notebook will load the most recent version of these assets, so to update the training/validation data you must rerun the dagster assets with desired configuration.\n",
    "\n",
    "- `ex21_training_data`: Dataset containing labeled data produced in label-studio to train `layoutlm`\n",
    "- `ex21_validation_set`: Labeled validation data describing expected inference output on validation filings\n",
    "- `ex21_failed_parsing_metadata`: Metadata for any validation filings that couldn't be parsed (usually empty)\n",
    "- `ex21_inference_dataset`: Parsed validation filings prepped for inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f185de-95ef-4194-9245-93f8d603d2e6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import dagstermill\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k import defs\n",
    "\n",
    "context = dagstermill.get_context(op_config={\n",
    "    \"layoutlm_uri\": None,\n",
    "})\n",
    "\n",
    "ex21_training_data = defs.load_asset_value(\"ex21_training_data\")\n",
    "\n",
    "ex21_failed_parsing_metadata = defs.load_asset_value(\"ex21_failed_parsing_metadata\")\n",
    "ex21_inference_dataset = defs.load_asset_value(\"ex21_inference_dataset\")\n",
    "ex21_validation_set = defs.load_asset_value(\"ex21_validation_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f299b2b-2358-4526-b023-f29c817316d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Layoutlmv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32edcce1-ab18-40b6-9da8-ce0ea53c2f72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define training metrics\n",
    "The method `compute_metrics` will be used to score the model. It computes precision, recall, f1 score, and accuracy on bounding box labels output by `layoutlm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9372b908-d9b9-4d18-a5bf-d332648b3e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from mozilla_sec_eia.library import validation_helpers\n",
    "from mozilla_sec_eia.models.sec10k.utils.cloud import get_metadata_filename\n",
    "\n",
    "\n",
    "def compute_metrics(p, metric, label_list, return_entity_level_metrics=False):\n",
    "    \"\"\"Compute metrics to train and evaluate the model on.\"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[pred] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[lab] for (pred, lab) in zip(prediction, label) if lab != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    if return_entity_level_metrics:\n",
    "        # Unpack nested dictionaries\n",
    "        final_results = {}\n",
    "        for key, value in results.items():\n",
    "            if isinstance(value, dict):\n",
    "                for n, v in value.items():\n",
    "                    final_results[f\"{key}_{n}\"] = v\n",
    "            else:\n",
    "                final_results[key] = value\n",
    "        return final_results\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160263c-8f69-437c-918b-e56ad007961a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Finetune Model\n",
    "The next cell will use the functions defined in the previous section to actually construct a huggingface dataset from labeled data and finetune the `layoutlm` model. Model finetuning will only be run if configured to do so, otherwise a pretrained version will be used from the `mlflow` tracking server.\n",
    "\n",
    "Model training contains several steps implemented below:\n",
    "1. Use temporary path to convert filings to PDF's and stash labels\n",
    "2. Use PDF's and labels to convert PDF's and labels to NER annotations\n",
    "3. Construct huggingface dataset from NER annotations and split into train and test sets\n",
    "4. Load pretrained model from huggingface\n",
    "5. Finetune model on training data and evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d205b2-e6ea-4ad0-982c-22e762269119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LayoutLMv3ForTokenClassification were not initialized from the model checkpoint at microsoft/layoutlmv3-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafaf3dc8cfe431b90802b61bfe0acc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_270331/790868001.py:94: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "/home/zach/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "2024/10/03 17:52:03 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n",
      "/home/zach/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/modeling_utils.py:1101: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/1000 00:02 < 10:34, 1.57 it/s, Epoch 0.04/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/03 17:52:09 INFO mlflow.tracking._tracking_service.client: 🏃 View run orderly-mare-33 at: https://mlflow-ned2up6sra-uc.a.run.app/#/experiments/13/runs/a94ac72df36447a489d576ea06a71a4a.\n",
      "2024/10/03 17:52:09 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://mlflow-ned2up6sra-uc.a.run.app/#/experiments/13.\n",
      "2024/10/03 17:52:09 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/03 17:52:10 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 129.75 MiB is free. Including non-PyTorch memory, this process has 2.94 GiB memory in use. Of the allocated memory 1.89 GiB is allocated by PyTorch, and 979.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 118\u001b[0m\n\u001b[1;32m    106\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    107\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    108\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: compute_metrics(p, metric\u001b[38;5;241m=\u001b[39mmetric, label_list\u001b[38;5;241m=\u001b[39mLABELS),\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run() \u001b[38;5;28;01mas\u001b[39;00m training_run:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Train inside mlflow run. Mlflow will automatically handle logging training metrcis\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# Log finetuend model with mlflow\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     model \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: trainer\u001b[38;5;241m.\u001b[39mtokenizer}\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/transformers/trainer.py:2341\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2339\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m _grad_norm\n\u001b[0;32m-> 2341\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_optimizer_step(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2345\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/accelerate/optimizer.py:172\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator_state\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mXLA:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:130\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[1;32m    129\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/adamw.py:227\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         state_steps,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/adamw.py:767\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    765\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 767\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/mozilla-sec-eia/lib/python3.11/site-packages/torch/optim/adamw.py:600\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    598\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[1;32m    603\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 3.93 GiB of which 129.75 MiB is free. Including non-PyTorch memory, this process has 2.94 GiB memory in use. Of the allocated memory 1.89 GiB is allocated by PyTorch, and 979.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from datasets import (\n",
    "    Array2D,\n",
    "    Array3D,\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    load_metric,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.data.data_collator import default_data_collator\n",
    "\n",
    "from mozilla_sec_eia.library.mlflow import configure_mlflow\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.common import (\n",
    "    BBOX_COLS,\n",
    "    LABELS,\n",
    "    get_id_label_conversions,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "configure_mlflow()\n",
    "mlflow.set_experiment(\"exhibit21_extraction_test\")\n",
    "\n",
    "\n",
    "def _prepare_dataset(annotations, processor, label2id):\n",
    "    \"\"\"Put the dataset in its final format for training LayoutLM.\"\"\"\n",
    "\n",
    "    def _convert_ner_tags_to_id(ner_tags, label2id):\n",
    "        return [int(label2id[ner_tag]) for ner_tag in ner_tags]\n",
    "\n",
    "    images = annotations[\"image\"]\n",
    "    words = annotations[\"tokens\"]\n",
    "    boxes = annotations[\"bboxes\"]\n",
    "    # Map over labels and convert to numeric id for each ner_tag\n",
    "    ner_tags = [\n",
    "        _convert_ner_tags_to_id(ner_tags, label2id)\n",
    "        for ner_tags in annotations[\"ner_tags\"]\n",
    "    ]\n",
    "\n",
    "    encoding = processor(\n",
    "        images,\n",
    "        words,\n",
    "        boxes=boxes,\n",
    "        word_labels=ner_tags,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    return encoding\n",
    "\n",
    "# Only finetune if configured to do so\n",
    "training_run_id = None\n",
    "if context.op_config[\"uri\"] is None:\n",
    "    id2label, label2id = get_id_label_conversions(LABELS)\n",
    "    # Change temp_dir to save training data locally for inspection\n",
    "    # Cache/prepare training data\n",
    "    dataset = Dataset.from_list(ex21_training_data)\n",
    "\n",
    "    # Load pretrained model\n",
    "    model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\", id2label=id2label, label2id=label2id\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        \"microsoft/layoutlmv3-base\", apply_ocr=False\n",
    "    )\n",
    "\n",
    "    # Prepare our train & eval dataset\n",
    "    column_names = dataset.column_names\n",
    "    features = Features(\n",
    "        {\n",
    "            \"pixel_values\": Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "            \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "            \"attention_mask\": Sequence(Value(dtype=\"int64\")),\n",
    "            \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "            \"labels\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        }\n",
    "    )\n",
    "    dataset = dataset.map(\n",
    "        lambda annotations: _prepare_dataset(annotations, processor, label2id),\n",
    "        batched=True,\n",
    "        remove_columns=column_names,\n",
    "        features=features,\n",
    "    )\n",
    "    dataset.set_format(\"torch\")\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2)\n",
    "    train_dataset, eval_dataset = split_dataset[\"train\"], split_dataset[\"test\"]\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    metric = load_metric(\"seqeval\")\n",
    "    training_args = TrainingArguments(\n",
    "        max_steps=1000,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        learning_rate=1e-5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        output_dir=\"./layoutlm\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor,\n",
    "        data_collator=default_data_collator,\n",
    "        compute_metrics=lambda p: compute_metrics(p, metric=metric, label_list=LABELS),\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run() as training_run:\n",
    "        # Train inside mlflow run. Mlflow will automatically handle logging training metrcis\n",
    "        trainer.train()\n",
    "\n",
    "        # Log finetuend model with mlflow\n",
    "        model = {\"model\": trainer.model, \"tokenizer\": trainer.tokenizer}\n",
    "        mlflow.transformers.log_model(\n",
    "            model, artifact_path=\"layoutlm_extractor\", task=\"token-classification\"\n",
    "        )\n",
    "        training_run_id = training_run.info. run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9b4e20-7781-43a7-b7aa-caf0690a201e",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "Use the finetuned model to perform inference and evaluate on labeled validation data. First create a Huggingface `Pipeline` which wraps layoutlm with some custom pre/post processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c8e920-d671-40c2-b5db-c43611a33897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Pipeline, pipeline\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.data.common import (\n",
    "    get_flattened_mode_predictions,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.utils.layoutlm import (\n",
    "    iob_to_label,\n",
    ")\n",
    "\n",
    "\n",
    "class LayoutLMInferencePipeline(Pipeline):\n",
    "    \"\"\"Pipeline for performing inference with fine-tuned LayoutLM.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Initialize LayoutLMInferencePipeline.\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _sanitize_parameters(self, **kwargs):\n",
    "        preprocess_kwargs = {}\n",
    "        if \"maybe_arg\" in kwargs:\n",
    "            preprocess_kwargs[\"maybe_arg\"] = kwargs[\"maybe_arg\"]\n",
    "        return preprocess_kwargs, {}, {}\n",
    "\n",
    "    def preprocess(self, doc_dict):\n",
    "        \"\"\"Encode and tokenize model inputs.\"\"\"\n",
    "        image = doc_dict[\"image\"]\n",
    "        words = doc_dict[\"tokens\"]\n",
    "        boxes = doc_dict[\"bboxes\"]\n",
    "        encoding = self.tokenizer(\n",
    "            image,\n",
    "            words,\n",
    "            boxes=boxes,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,  # this is the maximum max_length\n",
    "            stride=128,\n",
    "            return_offsets_mapping=True,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        model_inputs = {}\n",
    "        model_inputs[\"raw_encoding\"] = encoding.copy()\n",
    "        model_inputs[\"doc_dict\"] = doc_dict\n",
    "        model_inputs[\"offset_mapping\"] = encoding.pop(\"offset_mapping\")\n",
    "        model_inputs[\"sample_mapping\"] = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "        # TODO: do we actually need to make these into ints?\n",
    "        encoding[\"input_ids\"] = encoding[\"input_ids\"].to(torch.int64)\n",
    "        encoding[\"attention_mask\"] = encoding[\"attention_mask\"].to(torch.int64)\n",
    "        encoding[\"bbox\"] = encoding[\"bbox\"].to(torch.int64)\n",
    "        encoding[\"pixel_values\"] = torch.stack(encoding[\"pixel_values\"])\n",
    "        model_inputs[\"encoding\"] = encoding\n",
    "        return model_inputs\n",
    "\n",
    "    def _forward(self, model_inputs):\n",
    "        # encoding is passed as a UserDict in the model_inputs dictionary\n",
    "        # turn it back into a BatchEncoding\n",
    "        encoding = BatchEncoding(model_inputs[\"encoding\"])\n",
    "        if torch.cuda.is_available():\n",
    "            encoding.to(\"cuda\")\n",
    "            self.model.to(\"cuda\")\n",
    "        # since we're doing inference, we don't need gradient computation\n",
    "        with torch.no_grad():\n",
    "            output = self.model(**encoding)\n",
    "            return {\n",
    "                \"logits\": output.logits,\n",
    "                \"predictions\": output.logits.argmax(-1).squeeze().tolist(),\n",
    "                \"raw_encoding\": model_inputs[\"raw_encoding\"],\n",
    "                \"doc_dict\": model_inputs[\"doc_dict\"],\n",
    "            }\n",
    "\n",
    "    def postprocess(self, all_outputs):\n",
    "        \"\"\"Return logits, model predictions, and the extracted dataframe.\"\"\"\n",
    "        logits = all_outputs[\"logits\"]\n",
    "        predictions = all_outputs[\"logits\"].argmax(-1).squeeze().tolist()\n",
    "        output_df = self.extract_table(all_outputs)\n",
    "        return logits, predictions, output_df\n",
    "\n",
    "    def extract_table(self, all_outputs):\n",
    "        \"\"\"Extract a structured table from a set of inference predictions.\n",
    "\n",
    "        This function essentially works by stacking bounding boxes and predictions\n",
    "        into a dataframe and going from left to right and top to bottom. Then, every\n",
    "        every time a new subsidiary entity is encountered, it assigns a new group or\n",
    "        \"row\" to that subsidiary. Next, location and ownership percentage words/labeled\n",
    "        entities in between these subsidiary groups are assigned to a subsidiary row/group.\n",
    "        Finally, this is all formatted into a dataframe with an ID column from the original\n",
    "        filename and a basic cleaning function normalizes strings.\n",
    "        \"\"\"\n",
    "        # TODO: when model more mature, break this into sub functions to make it\n",
    "        # clearer what's going on\n",
    "        predictions = all_outputs[\"predictions\"]\n",
    "        encoding = all_outputs[\"raw_encoding\"]\n",
    "        doc_dict = all_outputs[\"doc_dict\"]\n",
    "\n",
    "        token_boxes_tensor = encoding[\"bbox\"].flatten(start_dim=0, end_dim=1)\n",
    "        predictions_tensor = torch.tensor(predictions)\n",
    "        mode_predictions = get_flattened_mode_predictions(\n",
    "            token_boxes_tensor, predictions_tensor\n",
    "        )\n",
    "        token_boxes = encoding[\"bbox\"].flatten(start_dim=0, end_dim=1).tolist()\n",
    "        predicted_labels = [\n",
    "            self.model.config.id2label[pred] for pred in mode_predictions\n",
    "        ]\n",
    "        simple_preds = [iob_to_label(pred).lower() for pred in predicted_labels]\n",
    "\n",
    "        df = pd.DataFrame(data=token_boxes, columns=BBOX_COLS)\n",
    "        df.loc[:, \"iob_pred\"] = predicted_labels\n",
    "        df.loc[:, \"pred\"] = simple_preds\n",
    "        invalid_mask = (\n",
    "            (df[\"top_left_x\"] == 0)\n",
    "            & (df[\"top_left_y\"] == 0)\n",
    "            & (df[\"bottom_right_x\"] == 0)\n",
    "            & (df[\"bottom_right_y\"] == 0)\n",
    "        )\n",
    "        df = df[~invalid_mask]\n",
    "        # we want to get actual words on the dataframe, not just subwords that correspond to tokens\n",
    "        # subwords from the same word share the same bounding box coordinates\n",
    "        # so we merge the original words onto our dataframe on bbox coordinates\n",
    "        words_df = pd.DataFrame(data=doc_dict[\"bboxes\"], columns=BBOX_COLS)\n",
    "        words_df.loc[:, \"word\"] = doc_dict[\"tokens\"]\n",
    "        df = df.merge(words_df, how=\"left\", on=BBOX_COLS).drop_duplicates(\n",
    "            subset=BBOX_COLS + [\"pred\", \"word\"]\n",
    "        )\n",
    "        # rows that are the first occurrence in a new group (subsidiary, loc, own_per)\n",
    "        # should always have a B entity label. Manually override labels so this is true.\n",
    "        first_in_group_df = df[\n",
    "            (df[\"pred\"].ne(df[\"pred\"].shift())) & (df[\"pred\"] != \"other\")\n",
    "        ]\n",
    "        first_in_group_df.loc[:, \"iob_pred\"] = (\n",
    "            \"B\" + first_in_group_df[\"iob_pred\"].str[1:]\n",
    "        )\n",
    "        df.update(first_in_group_df)\n",
    "        # filter for just words that were labeled with non \"other\" entities\n",
    "        entities_df = df.sort_values(by=[\"top_left_y\", \"top_left_x\"])\n",
    "        entities_df = entities_df[entities_df[\"pred\"] != \"other\"]\n",
    "        # words are labeled with IOB format which stands for inside, outside, beginning\n",
    "        # merge B and I entities to form one entity group\n",
    "        # (i.e. \"B-Subsidiary\" and \"I-Subsidiary\" become just \"subsidiary\"), assign a group ID\n",
    "        entities_df[\"group\"] = (entities_df[\"iob_pred\"].str.startswith(\"B-\")).cumsum()\n",
    "        grouped_df = (\n",
    "            entities_df.groupby([\"group\", \"pred\"])[\"word\"]\n",
    "            .apply(\" \".join)\n",
    "            .reset_index()[[\"pred\", \"word\"]]\n",
    "        )\n",
    "        # assign a new row every time there's a new subsidiary\n",
    "        grouped_df[\"row\"] = (grouped_df[\"pred\"].str.startswith(\"subsidiary\")).cumsum()\n",
    "        output_df = grouped_df.pivot_table(\n",
    "            index=\"row\", columns=\"pred\", values=\"word\", aggfunc=lambda x: \" \".join(x)\n",
    "        ).reset_index()\n",
    "        if output_df.empty:\n",
    "            return output_df\n",
    "        output_df.loc[:, \"id\"] = doc_dict[\"id\"]\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9fe887-43ca-43e2-85e3-bf5371bd165f",
   "metadata": {},
   "source": [
    "Next, wrap the `LayoutLMInferencePipeline` in an `mlflow` `pyfunc` model, which handles loading the pretrained model and managing inputs/outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d802e00-1ca4-40b3-b15b-561711a9db70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "from mozilla_sec_eia.models.sec10k.entities import (\n",
    "    Ex21CompanyOwnership,\n",
    "    Sec10kExtractionMetadata,\n",
    ")\n",
    "from mozilla_sec_eia.models.sec10k.ex_21.ex21_validation_helpers import (\n",
    "    clean_extracted_df,\n",
    ")\n",
    "\n",
    "# If a model was trained in this notebook, use it. Otherwise, use\n",
    "if training_run_id is not None:\n",
    "    model_uri = f\"runs:/{training_run_id}/layoutlm_extractor\"\n",
    "else:\n",
    "    model_uri = context.op_config[\"uri\"]\n",
    "\n",
    "model_info = mlflow.models.get_model_info(model_uri)\n",
    "\n",
    "def _get_data(dataset):\n",
    "    yield from dataset\n",
    "\n",
    "class Ex21Extractor(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Create an mlflow pyfunc model to perform full EX21 extraction.\"\"\"\n",
    "    def load_context(self, context):\n",
    "        \"\"\"Load pretrained model.\"\"\"\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "        self.model_components = mlflow.transformers.load_model(\n",
    "            context.artifacts[\"model_components\"], return_type=\"components\"\n",
    "        )\n",
    "\n",
    "    def predict(self, context, model_input: pd.DataFrame, params=None):\n",
    "        \"\"\"Use pretrained model and inference pipeline to perform inference.\"\"\"\n",
    "        # Convert dataframe to pyarrow Dataset\n",
    "        model_input[\"image\"] = model_input.apply(\n",
    "            lambda row: Image.frombytes(\n",
    "                row[\"mode\"], (row[\"width\"], row[\"height\"]), row[\"image\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "        dataset = Dataset.from_list(model_input.drop([\"mode\", \"width\", \"height\"], axis=1).to_dict(\"records\"))\n",
    "\n",
    "        # TODO: figure out device argument\n",
    "        pipe = pipeline(\n",
    "            \"token-classification\",\n",
    "            model=self.model_components[\"model\"],\n",
    "            tokenizer=self.model_components[\"tokenizer\"],\n",
    "            pipeline_class=LayoutLMInferencePipeline,\n",
    "        )\n",
    "\n",
    "        logits = []\n",
    "        predictions = []\n",
    "        all_output_df = Ex21CompanyOwnership.example(size=0)\n",
    "        extraction_metadata = Sec10kExtractionMetadata.example(size=0)\n",
    "        for logit, pred, output_df in pipe(_get_data(dataset)):\n",
    "            logits.append(logit)\n",
    "            predictions.append(pred)\n",
    "            if not output_df.empty:\n",
    "                filename = get_metadata_filename(output_df[\"id\"].iloc[0])\n",
    "                extraction_metadata.loc[filename, [\"success\"]] = True\n",
    "            all_output_df = pd.concat([all_output_df, output_df])\n",
    "        all_output_df.columns.name = None\n",
    "        all_output_df = clean_extracted_df(all_output_df)\n",
    "        all_output_df = all_output_df[[\"id\", \"subsidiary\", \"loc\", \"own_per\"]]\n",
    "        all_output_df = all_output_df.reset_index(drop=True)\n",
    "        return extraction_metadata, all_output_df\n",
    "\n",
    "# Save model to local temp dir with artifacts, then reload for evaluation\n",
    "with TemporaryDirectory() as tmp_dir:\n",
    "    mlflow.pyfunc.save_model(\n",
    "        path=tmp_dir,\n",
    "        python_model=Ex21Extractor(),\n",
    "        artifacts={\"model_components\": model_uri},\n",
    "    )\n",
    "    ex21_extraction_model = mlflow.pyfunc.load_model(tmp_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee84b13-6c37-4afe-8faa-003ff149aa2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Evaluation\n",
    "Now the full extraction model can be evaluated using labeled validation data and logged to `mlflow`. The `mlflow` run used to evaluate and log the inference model will be created as a nested child run to the run used to train `layoutlm`. This setup allows multiple versions/configurations of inference to be associated with a single version of `layoutlm`, creating a clean organizational structure for testing the base model and inference logic separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd74bdc-bb63-4ad2-82ec-3dfcf93a6121",
   "metadata": {},
   "source": [
    "#### Load validation data\n",
    "Next, load an inference dataset containing validation data. This dataset is formatted exactly the same as those that will feed into the `Ex21Extractor` during a production run, but contain only data from the validation set. When creating inference datasets we also produce a metadata dataframe documenting any filings that couldn't be parsed/converted to a PDF. This dataframe should be empty for the validation set, but we will still load it for consistency with production runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcc912-324a-42e9-9841-3a916c6ece6b",
   "metadata": {},
   "source": [
    "Next define method method for computing validation metrics. The metrics computed above for training are looking at bounding boxes output by `layoutlm` and pertain to one word at a time. These metrics will look at an entire table produced the inference pipeline and compare to the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79bd14d-5156-4f34-9a50-e9c813b822cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "def ex21_validation_metrics(computed_df: pd.DataFrame, validation_df: pd.DataFrame):\n",
    "    \"\"\"Compute validation metrics for Ex. 21 extraction.\"\"\"\n",
    "    shared_cols = validation_df.columns.intersection(computed_df.columns)\n",
    "    validation_df = validation_df.astype(computed_df[shared_cols].dtypes)\n",
    "    n_equal = 0\n",
    "    validation_filenames = validation_df[\"id\"].unique()\n",
    "    n_files = len(validation_filenames)\n",
    "    table_metrics_dict = {}\n",
    "    jaccard_dict = {}\n",
    "    incorrect_files = []\n",
    "    # iterate through each file and check each extracted table\n",
    "    for filename in validation_filenames:\n",
    "        extracted_table_df = computed_df[computed_df[\"id\"] == filename].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        validation_table_df = validation_df[\n",
    "            validation_df[\"id\"] == filename\n",
    "        ].reset_index(drop=True)\n",
    "        # check if the tables are exactly equal\n",
    "        if extracted_table_df.equals(validation_table_df):\n",
    "            # TODO: strip llc and other company strings before comparison\n",
    "            n_equal += 1\n",
    "        else:\n",
    "            incorrect_files.append(filename)\n",
    "        # compute precision and recall for each column\n",
    "        table_metrics_dict[filename] = {}\n",
    "        jaccard_dict[filename] = {}\n",
    "        for col in [\"subsidiary\", \"loc\", \"own_per\"]:\n",
    "            table_prec_recall = validation_helpers.pandas_compute_precision_recall(\n",
    "                extracted_table_df, validation_table_df, value_col=col\n",
    "            )\n",
    "            table_metrics_dict[filename][f\"{col}_precision\"] = table_prec_recall[\n",
    "                \"precision\"\n",
    "            ]\n",
    "            table_metrics_dict[filename][f\"{col}_recall\"] = table_prec_recall[\"recall\"]\n",
    "            # get the jaccard similarity between columns\n",
    "            jaccard_dict[filename][col] = validation_helpers.jaccard_similarity(\n",
    "                computed_df=extracted_table_df,\n",
    "                validation_df=validation_table_df,\n",
    "                value_col=col,\n",
    "            )\n",
    "\n",
    "    jaccard_df = pd.DataFrame.from_dict(jaccard_dict, orient=\"index\").reset_index()\n",
    "    prec_recall_df = pd.DataFrame.from_dict(\n",
    "        table_metrics_dict, orient=\"index\"\n",
    "    ).reset_index()\n",
    "\n",
    "    return (\n",
    "        jaccard_df,\n",
    "        prec_recall_df,\n",
    "        pd.DataFrame({\"filename\": incorrect_files}),\n",
    "        {\n",
    "            \"table_accuracy\": n_equal / n_files,\n",
    "            \"avg_subsidiary_jaccard_sim\": jaccard_df[\"subsidiary\"].sum() / n_files,\n",
    "            \"avg_location_jaccard_sim\": jaccard_df[\"loc\"].sum() / n_files,\n",
    "            \"avg_own_per_jaccard_sim\": jaccard_df[\"own_per\"].sum() / n_files,\n",
    "            \"avg_subsidiary_precision\": prec_recall_df[\"subsidiary_precision\"].sum()\n",
    "            / n_files,\n",
    "            \"avg_location_precision\": prec_recall_df[\"loc_precision\"].sum() / n_files,\n",
    "            \"avg_own_per_precision\": prec_recall_df[\"own_per_precision\"].sum()\n",
    "            / n_files,\n",
    "            \"avg_subsidiary_recall\": prec_recall_df[\"subsidiary_recall\"].sum()\n",
    "            / n_files,\n",
    "            \"avg_location_recall\": prec_recall_df[\"loc_recall\"].sum() / n_files,\n",
    "            \"avg_own_per_recall\": prec_recall_df[\"own_per_recall\"].sum() / n_files,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dee550f-7b06-4091-a65e-71c6b23a5bea",
   "metadata": {},
   "source": [
    "#### Validate model\n",
    "Finally, run the full model on the validation set and log metrics to mlflow. The logged metrics/model will appear in a nested run below the training run used for the current version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb56470-8527-424c-a9e5-4135e55fde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(parent_run_id=model_info.run_id, nested=True):\n",
    "    metadata, extracted = ex21_extraction_model.predict(ex21_inference_dataset.copy())\n",
    "    metadata = pd.concat([ex21_failed_parsing_metadata, metadata])\n",
    "\n",
    "    jaccard_df, prec_recall_df, incorrect_filenames, metrics = ex21_validation_metrics(extracted, ex21_validation_set)\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"exhibit21_extractor\",\n",
    "        python_model=Ex21Extractor(),\n",
    "        artifacts={\"model_components\": model_uri},\n",
    "        signature=infer_signature(dataset, extracted), # NOTE: model returns a second dataframe with metadata, but mlflow only supports one in signature\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
